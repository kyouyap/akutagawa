{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages/ipykernel_launcher.py:138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import MeCab\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "traindf=pd.read_csv(\"data/train.csv\")\n",
    "testdf=pd.read_csv(\"data/test.csv\")\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import MeCab\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "# traindf=pd.read_csv(\"data/train.csv\")\n",
    "# testdf=pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "import mojimoji\n",
    "import re\n",
    "import jaconv\n",
    "\n",
    "\n",
    "\n",
    "def parse_text(text, debug=False):\n",
    "    '''\n",
    "    Get location\n",
    "    '''\n",
    "\n",
    "#     text = mojimoji.zen_to_han(text, kana=False)\n",
    "    text = re.sub(r'［[^］]+］', '', text)  \n",
    "#     text = re.sub(r'[\\.*[\\]]', '', text)    \n",
    "#     text = re.sub(r'[\\.*[\\]]', '', text)\n",
    "#     text = re.sub(r'', '', text)    \n",
    "#     text = re.sub(r'[\\(（[].*[）\\)]]', '', text)\n",
    "    text = re.sub(r'[\\s、]', '', text)\n",
    "    text = re.sub(r'一', '', text)\n",
    "#     text = re.sub(r'…', '', text)  \n",
    "#     text = re.sub(r'/＼', '', text)\n",
    "    text = re.sub(r'[0-9]', '0', text)\n",
    "\n",
    "\n",
    "#     text = jaconv.kata2hira(text)\n",
    "    return text\n",
    "\n",
    "# df=pd.concat([traindf,testdf])\n",
    "# df.body=df.body.map(parse_text)\n",
    "# traindf.body=traindf.body.map(parse_text)\n",
    "# testdf.body=testdf.body.map(parse_text)\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = normalize_unicode(text)\n",
    "    normalized_text = normalize_number(normalized_text)\n",
    "    normalized_text = lower_text(normalized_text)\n",
    "    return normalized_text\n",
    "\n",
    "\n",
    "def lower_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def normalize_unicode(text, form='NFKC'):\n",
    "    normalized_text = unicodedata.normalize(form, text)\n",
    "    return normalized_text\n",
    "\n",
    "\n",
    "def lemmatize_term(term, pos=None):\n",
    "    if pos is None:\n",
    "        synsets = wordnet.synsets(term)\n",
    "        if not synsets:\n",
    "            return term\n",
    "        pos = synsets[0].pos()\n",
    "        if pos == wordnet.ADJ_SAT:\n",
    "            pos = wordnet.ADJ\n",
    "    return nltk.WordNetLemmatizer().lemmatize(term, pos=pos)\n",
    "\n",
    "\n",
    "def normalize_number(text):\n",
    "    \"\"\"\n",
    "    pattern = r'\\d+'\n",
    "    replacer = re.compile(pattern)\n",
    "    result = replacer.sub('0', text)\n",
    "    \"\"\"\n",
    "    # 連続した数字を0で置換\n",
    "    replaced_text = re.sub(r'\\d+', '0', text)\n",
    "    return replaced_text\n",
    "\n",
    "\n",
    "# mecab = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd/')\n",
    "# mecab=MeCab.Tagger ('-d /usr/local/lib/mecab/dic/UniDic-kindai_1603')\n",
    "mecab = MeCab.Tagger('-Owakati')\n",
    "\n",
    "# 形態素解析をして、名詞だけ取り出す\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "#     available_norm = ['接尾', '一般', '形容動詞語幹', 'サ変接続']\n",
    "    node = mecab.parseToNode(text)\n",
    "    l = []\n",
    "    while node:\n",
    "        l.append(node.surface)\n",
    "        node = node.next\n",
    "    return l\n",
    "\n",
    "\n",
    "# 記事群のdictについて、形態素解析をしてリストに返す\n",
    "def get_words(contents):\n",
    "    available_norm = ['接尾', '一般', '形容動詞語幹', 'サ変接続']\n",
    "    node = mecab.parseToNode(contents)\n",
    "    l = []\n",
    "    while node:\n",
    "\n",
    "        l.append(node.surface)\n",
    "        node = node.next\n",
    "    return l\n",
    "\n",
    "# 一つの記事を形態素解析して返す\n",
    "\n",
    "\n",
    "def get_words_main(content):\n",
    "    return [token for token in tokenize1(content)]\n",
    "\n",
    "\n",
    "def get_words_main1(content):\n",
    "    retoken = \"\"\n",
    "    for token in tokenize(content):\n",
    "        retoken += token+\" \"\n",
    "    return retoken\n",
    "\n",
    "def tokenize1(text):\n",
    "#     available_norm = ['接尾', '一般', '形容動詞語幹', 'サ変接続']\n",
    "    node = mecab.parseToNode(text)\n",
    "    l = []\n",
    "    while node:\n",
    "        l.append(node.surface)\n",
    "        node = node.next\n",
    "    return ' '.join(l)\n",
    "df = pd.concat([traindf, testdf])\n",
    "df.body = df.body.map(parse_text).map(tokenize1)\n",
    "#     count = CountVectorizer(analyzer=tokenize)\n",
    "#     bags = count.fit_transform(df.body.values)\n",
    "#     # print(bags.toarray())\n",
    "\n",
    "#     features = count.get_feature_names()\n",
    "#     # # print(features)\n",
    "\n",
    "#     bodyvec = pd.DataFrame(bags.toarray(), columns=features)\n",
    "#     newdf = pd.concat([df.reset_index(drop=True), pd.DataFrame(bodyvec)], axis=1)\n",
    "train = df.dropna().drop([\"writing_id\", ], axis=1)\n",
    "test = df[df.author.isnull()].drop([\"writing_id\"], axis=1)\n",
    "test = test.drop([\"author\"], axis=1)\n",
    "X = train.drop([\"author\"], axis=1)\n",
    "y = train.author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# def preprocess():\n",
    "#     traindf = pd.read_csv(\"data/train.csv\")\n",
    "#     testdf = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "#     df = pd.concat([traindf, testdf])\n",
    "#     df.body = df.body.map(parse_text)\n",
    "#     count = CountVectorizer(analyzer=tokenize)\n",
    "#     bags = count.fit_transform(df.body.values)\n",
    "#     # print(bags.toarray())\n",
    "\n",
    "#     features = count.get_feature_names()\n",
    "#     # # print(features)\n",
    "\n",
    "#     bodyvec = pd.DataFrame(bags.toarray(), columns=features)\n",
    "#     newdf = pd.concat([df.reset_index(drop=True), pd.DataFrame(bodyvec)], axis=1)\n",
    "#     train = newdf.dropna().drop([\"writing_id\", \"body\", ], axis=1)\n",
    "#     test = newdf[newdf.author.isnull()].drop([\"writing_id\", \"body\"], axis=1)\n",
    "#     test = test.drop([\"author\"], axis=1)\n",
    "#     X = train.drop([\"author\"], axis=1)\n",
    "#     y = train.author\n",
    "#     return X, y, test\n",
    "\n",
    "\n",
    "# # X, y, test = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess1():\n",
    "    traindf = pd.read_csv(\"data/train.csv\")\n",
    "    testdf = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "    df = pd.concat([traindf, testdf])\n",
    "    df.body = df.body.map(parse_text).map(tokenize1)\n",
    "#     count = CountVectorizer(analyzer=tokenize)\n",
    "#     bags = count.fit_transform(df.body.values)\n",
    "#     # print(bags.toarray())\n",
    "\n",
    "#     features = count.get_feature_names()\n",
    "#     # # print(features)\n",
    "\n",
    "#     bodyvec = pd.DataFrame(bags.toarray(), columns=features)\n",
    "#     newdf = pd.concat([df.reset_index(drop=True), pd.DataFrame(bodyvec)], axis=1)\n",
    "    train = df.dropna().drop([\"writing_id\", ], axis=1)\n",
    "    test = df[df.author.isnull()].drop([\"writing_id\"], axis=1)\n",
    "    test = test.drop([\"author\"], axis=1)\n",
    "    X = train.drop([\"author\"], axis=1)\n",
    "    y = train.author\n",
    "    return X, y, test\n",
    "X, y, test=preprocess1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>先ごろ の 本欄 に 僕 の 「 風 報 」 に かい た 「 天皇陛下 に 捧ぐる 言葉...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>旅 の 眼 に 映じ た 外国 の 正月 を といふ お 需 め で 昔 前 の 記憶 か...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>或 る 心持 の よい 夕方 日比谷公園 の 樹 の 繁み の 間 で 若葉 楓 の 梢 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>島々 《 しま ／ ＼ 》 と 云 ふ 町 の 宿屋 へ 着い た の は 午 過ぎ ――...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>或 る 田舎 に 母 と 子 と が 住ん で い た 。 そして 或 る 年 の 秋 次...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>八 五 郎 の 取柄 は 誰 と で も すぐ 友達 に なれる こと でし た 。 長 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>ある 婦人 が 私 に 言 つ た 。 私 が 情痴 作家 など ゝ 言 は れる こと ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>諸君 は 東京 市 某 町 某 番地 なる 風 博士 の 邸宅 を 御存じ で あろ う ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>「 御免 」 少し 職業 的 に 落着き 拂 つた 聲錢 形 平次 は それ を 聞く と...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>ちやう ど 今日 （ 十月 三 日 ） 文部省 で 著作 家 側 を 招い て 新 カナヅ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3312 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body\n",
       "0      先ごろ の 本欄 に 僕 の 「 風 報 」 に かい た 「 天皇陛下 に 捧ぐる 言葉...\n",
       "1      旅 の 眼 に 映じ た 外国 の 正月 を といふ お 需 め で 昔 前 の 記憶 か...\n",
       "2      或 る 心持 の よい 夕方 日比谷公園 の 樹 の 繁み の 間 で 若葉 楓 の 梢 ...\n",
       "3      島々 《 しま ／ ＼ 》 と 云 ふ 町 の 宿屋 へ 着い た の は 午 過ぎ ――...\n",
       "4      或 る 田舎 に 母 と 子 と が 住ん で い た 。 そして 或 る 年 の 秋 次...\n",
       "...                                                 ...\n",
       "3307   八 五 郎 の 取柄 は 誰 と で も すぐ 友達 に なれる こと でし た 。 長 ...\n",
       "3308   ある 婦人 が 私 に 言 つ た 。 私 が 情痴 作家 など ゝ 言 は れる こと ...\n",
       "3309   諸君 は 東京 市 某 町 某 番地 なる 風 博士 の 邸宅 を 御存じ で あろ う ...\n",
       "3310   「 御免 」 少し 職業 的 に 落着き 拂 つた 聲錢 形 平次 は それ を 聞く と...\n",
       "3311   ちやう ど 今日 （ 十月 三 日 ） 文部省 で 著作 家 側 を 招い て 新 カナヅ...\n",
       "\n",
       "[3312 rows x 1 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['body']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        先ごろ の 本欄 に 僕 の 「 風 報 」 に かい た 「 天皇陛下 に 捧ぐる 言葉...\n",
       "1        旅 の 眼 に 映じ た 外国 の 正月 を といふ お 需 め で 昔 前 の 記憶 か...\n",
       "2        或 る 心持 の よい 夕方 日比谷公園 の 樹 の 繁み の 間 で 若葉 楓 の 梢 ...\n",
       "3        島々 《 しま ／ ＼ 》 と 云 ふ 町 の 宿屋 へ 着い た の は 午 過ぎ ――...\n",
       "4        或 る 田舎 に 母 と 子 と が 住ん で い た 。 そして 或 る 年 の 秋 次...\n",
       "                              ...                        \n",
       "3307     八 五 郎 の 取柄 は 誰 と で も すぐ 友達 に なれる こと でし た 。 長 ...\n",
       "3308     ある 婦人 が 私 に 言 つ た 。 私 が 情痴 作家 など ゝ 言 は れる こと ...\n",
       "3309     諸君 は 東京 市 某 町 某 番地 なる 風 博士 の 邸宅 を 御存じ で あろ う ...\n",
       "3310     「 御免 」 少し 職業 的 に 落着き 拂 つた 聲錢 形 平次 は それ を 聞く と...\n",
       "3311     ちやう ど 今日 （ 十月 三 日 ） 文部省 で 著作 家 側 を 招い て 新 カナヅ...\n",
       "Name: body, Length: 3312, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)  # 80%のデータを学習データに、20%を検証データにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[606   2]\n",
      " [  7  48]]\n",
      "accuracy =  0.9864253393665159\n",
      "precision =  0.96\n",
      "recall =  0.8727272727272727\n",
      "f1 score =  0.9142857142857144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()  # ロジスティック回帰モデルのインスタンスを作成\n",
    "lr.fit(train_X, train_y)  # ロジスティック回帰モデルの重みを学習\n",
    "pred = lr.predict(test_X)\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=test_y, y_pred=pred))\n",
    "print('accuracy = ', accuracy_score(y_true=test_y, y_pred=pred))\n",
    "print('precision = ', precision_score(y_true=test_y, y_pred=pred))\n",
    "print('recall = ', recall_score(y_true=test_y, y_pred=pred))\n",
    "print('f1 score = ', f1_score(y_true=test_y, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[605   3]\n",
      " [  3  52]]\n",
      "accuracy =  0.9909502262443439\n",
      "precision =  0.9454545454545454\n",
      "recall =  0.9454545454545454\n",
      "f1 score =  0.9454545454545454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    **{\"hidden_layer_sizes\": (128, 128, 128, 128, 128, 128), \"random_state\": 42})\n",
    "mlp.fit(train_X, train_y)\n",
    "pred = mlp.predict(test_X)\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=test_y, y_pred=pred))\n",
    "print('accuracy = ', accuracy_score(y_true=test_y, y_pred=pred))\n",
    "print('precision = ', precision_score(y_true=test_y, y_pred=pred))\n",
    "print('recall = ', recall_score(y_true=test_y, y_pred=pred))\n",
    "print('f1 score = ', f1_score(y_true=test_y, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    **{\"hidden_layer_sizes\": (128, 128, 128, 128, 128, 128), \"random_state\": 42})\n",
    "mlp.fit(X, y)\n",
    "pred = mlp.predict(test)\n",
    "# pred = model.predict(np.array(test))\n",
    "pred = np.where(pred > 0.5, 1, 0)\n",
    "sub = pd.DataFrame(pd.read_csv(\"data/test.csv\")['writing_id'])\n",
    "sub[\"author\"] = list(pred)\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached https://files.pythonhosted.org/packages/2c/72/6b3264aa2889b7dde7663464b99587d95cd6a5f3b9b30181f14d78a63e64/tensorflow-2.0.0-cp37-cp37m-macosx_10_11_x86_64.whl\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
      "  Using cached https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (1.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (1.25.0)\n",
      "Collecting tensorboard<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 1.2MB/s eta 0:00:01     |████████▋                       | 1.0MB 1.5MB/s eta 0:00:02     |█████████████▏                  | 1.6MB 1.5MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (0.1.8)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (1.17.3)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz\n",
      "Collecting wheel>=0.26\n",
      "  Using cached https://files.pythonhosted.org/packages/00/83/b4a77d044e78ad1a45610eb88f745be2fd2c6d658f9798a15e384b7d57c9/wheel-0.33.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: h5py in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow) (41.2.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.2.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.25.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.0)\n",
      "Installing collected packages: termcolor, tensorflow-estimator, wheel, tensorboard, gast, opt-einsum, tensorflow\n",
      "    Running setup.py install for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Found existing installation: gast 0.3.2\n",
      "    Uninstalling gast-0.3.2:\n",
      "      Successfully uninstalled gast-0.3.2\n",
      "    Running setup.py install for gast ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for opt-einsum ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed gast-0.2.2 opt-einsum-3.1.0 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1 termcolor-1.1.0 wheel-0.33.6\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[, 夕方, 降り出し, た, 雨, は, その, 晩, 遅く, まで, 続い, た, 。,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[, この, 「, 東北, 文学, 」, という, 雑誌, の, 貴重, な, 紙面, の,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[, 幼少, の, ころ, 高知, 《, こうち, 》, の, 城下, から, 東, に, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[, ○, 「, 三, 人, 姉妹, 」, で, マーシャ, が, どんな, 風, に, 活...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[, 眼鏡, 或, 日, 趣味, に関して, 人, に, 問, はれ, た, 。, 稍, 暫...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>[, 伝統, 的, な, 女形, と, 云う, もの, の, 型, に, 嵌っ, て, 終始...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>[, 「, あ, ツヒ, 人殺し, ツ, 」, 宵闇, を, 劈, 《, つん, ざ, 》,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>[, 本誌, （, 「, 劇作, 」, ）, 三月, 号, に, 発表, さ, れ, た, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>[, 昔, 《, むかし, 》, は, いま, より, も, もっと, 松, 《, まつ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>[, 「, 親分, 」, 「, 何, ん, だ, 八, 。, 大層, な, 意, 氣, 込み...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1420 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body\n",
       "0     [, 夕方, 降り出し, た, 雨, は, その, 晩, 遅く, まで, 続い, た, 。,...\n",
       "1     [, この, 「, 東北, 文学, 」, という, 雑誌, の, 貴重, な, 紙面, の,...\n",
       "2     [, 幼少, の, ころ, 高知, 《, こうち, 》, の, 城下, から, 東, に, ...\n",
       "3     [, ○, 「, 三, 人, 姉妹, 」, で, マーシャ, が, どんな, 風, に, 活...\n",
       "4     [, 眼鏡, 或, 日, 趣味, に関して, 人, に, 問, はれ, た, 。, 稍, 暫...\n",
       "...                                                 ...\n",
       "1415  [, 伝統, 的, な, 女形, と, 云う, もの, の, 型, に, 嵌っ, て, 終始...\n",
       "1416  [, 「, あ, ツヒ, 人殺し, ツ, 」, 宵闇, を, 劈, 《, つん, ざ, 》,...\n",
       "1417  [, 本誌, （, 「, 劇作, 」, ）, 三月, 号, に, 発表, さ, れ, た, ...\n",
       "1418  [, 昔, 《, むかし, 》, は, いま, より, も, もっと, 松, 《, まつ, ...\n",
       "1419  [, 「, 親分, 」, 「, 何, ん, だ, 八, 。, 大層, な, 意, 氣, 込み...\n",
       "\n",
       "[1420 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(pd.concat([X.body,test.body]))\n",
    "x_train = tokenizer.texts_to_sequences(X.body)\n",
    "x_test = tokenizer.texts_to_sequences(test.body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>先ごろ の 本欄 に 僕 の 「 風 報 」 に かい た 「 天皇陛下 に 捧ぐる 言葉...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>旅 の 眼 に 映じ た 外国 の 正月 を といふ お 需 め で 昔 前 の 記憶 か...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>或 る 心持 の よい 夕方 日比谷公園 の 樹 の 繁み の 間 で 若葉 楓 の 梢 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>島々 《 しま ／ ＼ 》 と 云 ふ 町 の 宿屋 へ 着い た の は 午 過ぎ ――...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>或 る 田舎 に 母 と 子 と が 住ん で い た 。 そして 或 る 年 の 秋 次...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>八 五 郎 の 取柄 は 誰 と で も すぐ 友達 に なれる こと でし た 。 長 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>ある 婦人 が 私 に 言 つ た 。 私 が 情痴 作家 など ゝ 言 は れる こと ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>諸君 は 東京 市 某 町 某 番地 なる 風 博士 の 邸宅 を 御存じ で あろ う ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>「 御免 」 少し 職業 的 に 落着き 拂 つた 聲錢 形 平次 は それ を 聞く と...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>ちやう ど 今日 （ 十月 三 日 ） 文部省 で 著作 家 側 を 招い て 新 カナヅ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3312 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body\n",
       "0      先ごろ の 本欄 に 僕 の 「 風 報 」 に かい た 「 天皇陛下 に 捧ぐる 言葉...\n",
       "1      旅 の 眼 に 映じ た 外国 の 正月 を といふ お 需 め で 昔 前 の 記憶 か...\n",
       "2      或 る 心持 の よい 夕方 日比谷公園 の 樹 の 繁み の 間 で 若葉 楓 の 梢 ...\n",
       "3      島々 《 しま ／ ＼ 》 と 云 ふ 町 の 宿屋 へ 着い た の は 午 過ぎ ――...\n",
       "4      或 る 田舎 に 母 と 子 と が 住ん で い た 。 そして 或 る 年 の 秋 次...\n",
       "...                                                 ...\n",
       "3307   八 五 郎 の 取柄 は 誰 と で も すぐ 友達 に なれる こと でし た 。 長 ...\n",
       "3308   ある 婦人 が 私 に 言 つ た 。 私 が 情痴 作家 など ゝ 言 は れる こと ...\n",
       "3309   諸君 は 東京 市 某 町 某 番地 なる 風 博士 の 邸宅 を 御存じ で あろ う ...\n",
       "3310   「 御免 」 少し 職業 的 に 落着き 拂 つた 聲錢 形 平次 は それ を 聞く と...\n",
       "3311   ちやう ど 今日 （ 十月 三 日 ） 文部省 で 著作 家 側 を 招い て 新 カナヅ...\n",
       "\n",
       "[3312 rows x 1 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>先ごろ の 本欄 に 僕 の 「 風 報 」 に かい た 「 天皇陛下 に 捧ぐる 言葉...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>旅 の 眼 に 映じ た 外国 の 正月 を といふ お 需 め で 昔 前 の 記憶 か...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>或 る 心持 の よい 夕方 日比谷公園 の 樹 の 繁み の 間 で 若葉 楓 の 梢 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body\n",
       "0   先ごろ の 本欄 に 僕 の 「 風 報 」 に かい た 「 天皇陛下 に 捧ぐる 言葉...\n",
       "1   旅 の 眼 に 映じ た 外国 の 正月 を といふ お 需 め で 昔 前 の 記憶 か...\n",
       "2   或 る 心持 の よい 夕方 日比谷公園 の 樹 の 繁み の 間 で 若葉 楓 の 梢 ..."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 先ごろ の 本欄 に 僕 の 「 風 報 」 に かい た 「 天皇陛下 に 捧ぐる 言葉 」 を 評し て 俗 うけ を 狙っ た 媚態 露出 だ と の こと で ある が 白井 明 先生 の 鑑賞 眼 は 浅薄 低俗 と 申さ なけれ ば なら ない 。 あの 文章 に こもる 祖国 へ よせる 僕 の 愛情 や あれ を 書か ず に い られ なかっ た 情熱 を 読みとる こと が でき ない と は 白井 先生 が 頃日 書く 意味 も ない 駄文 ばかり 書い てる せい な の で ある 。 いったい に 文学 の 反語 性 に 味読 の 及ば ぬ 識見 低俗 な ヤカラ が 文学 を 批評 する という の が 間違っ て いる 。 僕 の 「 堕落 論 」 その他 の エッセイ に し て も 小説 に し て も その 反語 に こもる 正しい 意味 を 理解 し 得 ず に 軽率 な 判読 断定 を 下す から 読者 に 誤読 の お手本 を 与え て いる よう な もの で ある 。 本名 で は 愚かしい ソラゴト しか 書け ず 匿名 で しか 本音 の 吐け ぬ 文学 者 など という もの は ない 。 僕 に は 匿名 の 必要 は ない 。 いつ でも 本音 を 吐き ギリギリ の こと を 言っ てる から だ 。 だから また ぼく の 本音 は 文学 の 本質 的 な もの で あり 単なる 中傷 の ケチ くさい 汚らし さ は ない の で ある 。 白井 明 先生 も 本名 で 本音 を 吐く こと を 学び た ま え 。 本名 で 君 の ケチアサマシサ を さらけだす こと の 苦痛 に 堪え て その 争い の 嵐 の 中 で 自分 を 育て た ま え 。 さ すれ ば 文学 の 本質 に も やがて 近づき うる で あろ う 。 「 天皇陛下 に さ ゝ ぐる 言葉 」 に こもる 大いなる 愛情 も やみ がたい 情熱 も 君 の 目 に は 逆 の 意味 に うつる の も きわめて 当然 な こと で ある 。 しかし こういう 愚 に も つか ない 批評 で も それ が 君 の 本音 なら 仕方 が ない から せめて 本名 で 書か れん こと を 。 さ すれ ば 進歩 は あり うる で あろ う 。 \n",
      "[39426, 1, 71228, 3, 100, 1, 15, 211, 6868, 16, 3, 328, 5, 15, 33066, 3, 60194, 182, 16, 7, 12213, 6, 4654, 1242, 7, 8387, 5, 13448, 8646, 19, 10, 1, 23, 9, 22, 8, 27058, 1434, 259, 1, 5210, 136, 4, 9319, 11963, 10, 6102, 149, 50, 66, 18, 2, 105, 1004, 3, 17101, 6550, 26, 15300, 100, 1, 1627, 45, 254, 7, 919, 70, 3, 25, 88, 93, 5, 1732, 7, 29393, 23, 8, 271, 18, 10, 4, 27058, 259, 8, 40868, 761, 289, 13, 18, 45979, 123, 283, 165, 602, 17, 1, 9, 22, 2, 2818, 3, 172, 1, 30729, 332, 3, 71229, 1, 3000, 103, 18777, 11963, 17, 48076, 8, 172, 7, 875, 44, 49, 1, 8, 4983, 6, 30, 2, 100, 1, 15, 5285, 1311, 16, 1045, 1, 38139, 3, 14, 6, 13, 282, 3, 14, 6, 13, 27, 30729, 3, 17101, 2025, 289, 7, 712, 14, 319, 70, 3, 10282, 17, 26542, 6888, 7, 4741, 20, 1029, 3, 44052, 1, 15301, 7, 821, 6, 30, 38, 17, 31, 9, 22, 2, 9730, 9, 4, 25078, 80117, 380, 2328, 70, 25079, 9, 380, 13234, 1, 35847, 103, 172, 95, 104, 49, 31, 4, 18, 2, 100, 3, 4, 25079, 1, 364, 4, 18, 2, 532, 96, 13234, 7, 6135, 13758, 1, 23, 7, 309, 165, 20, 19, 2, 550, 113, 1118, 1, 13234, 4, 172, 1, 1080, 54, 17, 31, 9, 53, 3172, 28151, 1, 5379, 4735, 40869, 29, 4, 18, 1, 9, 22, 2, 27058, 1434, 259, 13, 9730, 9, 13234, 7, 7984, 23, 7, 9320, 5, 199, 162, 2, 9730, 9, 164, 1, 95718, 7, 42403, 23, 1, 1886, 3, 2495, 6, 27, 7148, 1, 4179, 1, 62, 9, 59, 7, 3153, 5, 199, 162, 2, 29, 378, 50, 172, 1, 1080, 3, 13, 471, 5892, 1289, 9, 232, 24, 2, 15, 33066, 3, 29, 110, 5812, 182, 16, 3, 17101, 8931, 1627, 13, 4623, 2751, 1732, 13, 164, 1, 168, 3, 4, 1605, 1, 289, 3, 8545, 1, 13, 3200, 918, 17, 23, 9, 22, 2, 174, 526, 4935, 3, 13, 520, 18, 875, 9, 13, 36, 8, 164, 1, 13234, 66, 911, 8, 18, 20, 1863, 9730, 9, 919, 4769, 23, 7, 2, 29, 378, 50, 1986, 4, 53, 1289, 9, 232, 24, 2]\n",
      " 旅 の 眼 に 映じ た 外国 の 正月 を といふ お 需 め で 昔 前 の 記憶 から 探し て み た が 其処 に は ほとんど 「 お正月 」 と いふ もの が ない 。 我々 の 頭 に 幼少 の 頃 から 浸 み 込ん で ゐる お正月 新年 といふ もの と は およそ かけ離れ た もの で あつ た 。 古い 年 が 逝き 新しい 年 が 来る と いふ 事 を 我々 の 祖先 が 何故 こんなに 重 大事 と し 華やか な 儀式 を 以 つて 迎 へる 様 に なつ た か その 穿鑿 は 別 として 欧米 人 は 実に あつ さ り と これ を 扱 つて ゐる 。 私 は 丁度 四 回 の 新年 を 巴里 で 迎 へ たわけ で ある が 仏蘭西 人 の 下宿 に 住み 故国 から の 留学生 とか 大使館 関係 の 人達 と の 交際 など も 少 なかつ た ので 猶 更 その 正月 は ひつ そり し た もの で あつ た 。 最初 の 年 は それでも 「 あ 今時分 は 弟妹 達 雑煮 で も 祝 つて ゐる か な 」 とか 母 の 得意 の 煎 田作 で 飯 を 食べ て みたい とか 思 つ たり し た もの で ある が 次 の 年 から は そんな 感傷 も 薄らぎ 結句 煩雑 な 儀礼 に 縛ら れ ない で 済む 身軽 さ の 気持 に のびのび と 己 れ を 浸し て ゐ た 。 それでも 大晦日 の 晩 は レヴエイヨン と い つ て みんな 大概 レストラン か 何 か に 出かけ 知人 等 と 食事 を 供 に し 踊 つ たり 唄 つ たり で 夜 を 更かす つまり それ が 外国 で は 新年 を 迎 へる 気持 の 唯 の 現 はれ と 云 へ よう 。 その 騒ぎ も 夜 が 明ける 頃 に は 何処 も す つかり 静ま つて 街 上 に も 屋内 に も 平常 と 何 の 変り も ない 日 が 来る 。 起き て 食堂 に でも 出 て 来る と 流石 下宿 の 女 主人 が 「 お早う 」 の 代り に 「 お 目出度う 」 と 云 つ て くれる 。 しかし それ も ほんの 軽い 挨拶 で 別に その 言葉 から 正月 を 感じ させ て くれる やう な もの で は ない 。 カトリック教 の 国 に 「 王様 の 日 」 と いふ の が ある 。 これ は 偶然 日本 の 「 松の内 」 に ある お祭り 日 で あ つて 向 ふ の 人達 に は 新年 と は 関 り の ない もの で ある が 日本人 で ある 私 など に は 時 が 時 な ので ちよ つと その 日 は お正月 らしい 気分 を 味 は へる もの だ つ た 。 それ は 聖書 に ある 通り 基督 が 生れ た 時 東方 の 国 の 博士 達 が 星 の 占 ひで ベツレヘム に 偉い 人 が 生れ た と 云 つたの により 東方 の 国 の 王 が その 誕生 を 祝ひ に 来 た といふ その 日 を 祝 ふ の で ある 。 この 日 各 家庭 で は 独身 だ つ たり 遠く から 学校 の 寄宿舎 に 来 て ゐる 人 など 家 を 持た ない 人達 を 招き 煖炉 を 前 に し て カルタ や 唄 や 隠し芸 の 披露 や 極 く 呑気 に 家庭 的 な 娯楽 に うち 興じる 。 そして この 日 に は 食後 に 必ず 特別 の 菓子 が 出る 。 丁度 誕生 日 や クリスマス の 時 の 様 な 大きい カステラ 風 の 菓子 だ が 大抵 は その 家 の 主婦 の 手製 といふ 事 に なつ て ゐる 。 これ を 主婦 が 人数 だけ に 分け て 各自 に 配る の で ある が この 中 に は たつ たつ 王様 の 人形 が 入れ て あり それ に ぶつ か つた 人 は 男 なら 王様 に なり 相手 の 女王 を 選ぶ し 女 なら 王様 を 選ぶ 権利 が ある 。 女王 なり 王様 なり が 決まる と みんな は 二 人 を ならべ て 口々 に 「 王様 お 目出度う 女 王様 お 目 出 た う 」 と 祝詞 を 述べ て 囃す の で ある 。 この 人形 を なるたけ その 日 の 人気 者 例へば その 中 に 恋人 同志 が ゐる と すれ ば その 方 に といふ 工合 に うまく 当る 様 に する の も 主婦 の 腕前 な の ださ う で ある 。 しかし 当 つた 当人 は てれくさい ので わざと みんな の 期待 通り に は 選ば なかつ たり する 。 かう し て 老若男女 無邪気 に 日 を 楽しむ この 日 だけ が ちよ つと 日本 の 正月 の カルタ 会 の 空気 など を 思は せ られる 唯 の もの だ つ た 。 \n",
      "[1361, 1, 136, 3, 12571, 5, 1113, 1, 2826, 7, 107, 39, 33067, 206, 9, 423, 118, 1, 1088, 20, 1819, 6, 126, 5, 8, 1334, 3, 4, 1253, 15, 7315, 16, 10, 166, 31, 8, 18, 2, 869, 1, 214, 3, 12776, 1, 260, 20, 9232, 126, 856, 9, 67, 7315, 8316, 107, 31, 10, 4, 5008, 20240, 5, 31, 9, 151, 5, 2, 1066, 114, 8, 60195, 511, 114, 8, 193, 10, 166, 90, 7, 869, 1, 4655, 8, 1147, 1189, 992, 893, 10, 14, 6947, 17, 9457, 7, 4778, 42, 2094, 633, 265, 3, 253, 5, 21, 27, 15889, 4, 578, 82, 8580, 37, 4, 518, 151, 29, 138, 10, 71, 7, 4801, 42, 67, 2, 34, 4, 837, 152, 1369, 1, 8316, 7, 5893, 9, 2094, 26, 17699, 9, 22, 8, 3473, 37, 1, 2452, 3, 4820, 10912, 20, 1, 12875, 257, 10520, 453, 1, 1673, 10, 1, 4626, 104, 13, 5632, 171, 5, 119, 3571, 2716, 27, 2826, 4, 1676, 2294, 14, 5, 31, 9, 151, 5, 2, 1078, 1, 114, 4, 624, 15, 142, 16885, 4, 13898, 285, 18778, 9, 13, 7014, 42, 67, 21, 17, 16, 257, 226, 1, 2684, 1, 19964, 71230, 9, 2603, 7, 1384, 6, 696, 257, 229, 41, 127, 14, 5, 31, 9, 22, 8, 375, 1, 114, 20, 4, 134, 3010, 13, 40870, 56393, 26543, 17, 13235, 3, 3190, 35, 18, 9, 6334, 13236, 29, 1, 311, 3, 11136, 10, 1770, 35, 7, 16886, 6, 80, 5, 2, 624, 11289, 1, 434, 4, 95719, 10, 25, 41, 6, 344, 3610, 14877, 21, 52, 21, 3, 900, 4656, 363, 10, 1535, 7, 4922, 3, 14, 4274, 41, 127, 3135, 41, 127, 9, 248, 7, 60196, 552, 36, 8, 1113, 9, 4, 8316, 7, 2094, 633, 311, 1, 940, 1, 1471, 580, 10, 129, 26, 38, 2, 27, 1993, 13, 248, 8, 9414, 260, 3, 4, 719, 13, 218, 1181, 60197, 42, 1055, 91, 3, 13, 17290, 3, 13, 4275, 10, 52, 1, 2062, 13, 18, 73, 8, 193, 2, 1044, 6, 1971, 3, 96, 108, 6, 193, 10, 5078, 2452, 1, 94, 317, 8, 15, 11137, 16, 1, 807, 3, 15, 39, 36927, 16, 10, 129, 41, 6, 655, 2, 174, 36, 13, 1827, 2877, 1226, 9, 1421, 27, 182, 20, 2826, 7, 175, 935, 6, 655, 79, 17, 31, 9, 4, 18, 2, 53221, 1, 473, 3, 15, 5510, 1, 73, 16, 10, 166, 1, 8, 22, 2, 71, 4, 1438, 141, 1, 15, 40871, 16, 3, 22, 38140, 73, 9, 142, 42, 495, 139, 1, 1673, 3, 4, 8316, 10, 4, 2111, 138, 1, 18, 31, 9, 22, 8, 715, 9, 22, 34, 104, 3, 4, 75, 8, 75, 17, 119, 1174, 506, 27, 73, 4, 7315, 187, 902, 7, 883, 4, 633, 31, 19, 41, 5, 2, 36, 4, 9567, 3, 22, 295, 14478, 8, 617, 5, 75, 17700, 1, 473, 1, 1166, 285, 8, 1547, 1, 8388, 1855, 65041, 3, 3923, 37, 8, 617, 5, 10, 129, 388, 3358, 17700, 1, 473, 1, 1628, 8, 27, 4264, 7, 18336, 3, 74, 5, 107, 27, 73, 7, 7014, 139, 1, 9, 22, 2, 46, 73, 1795, 836, 9, 4, 6451, 19, 41, 127, 972, 20, 486, 1, 8686, 3, 74, 6, 67, 37, 104, 85, 7, 1591, 18, 1673, 7, 11964, 10166, 7, 118, 3, 14, 6, 17701, 45, 3135, 45, 36928, 1, 7811, 45, 2808, 207, 5079, 3, 836, 54, 17, 4898, 3, 135, 42404, 2, 102, 46, 73, 3, 4, 12490, 3, 1130, 1041, 1, 3513, 8, 553, 2, 837, 4264, 73, 45, 11965, 1, 75, 1, 265, 17, 606, 23460, 211, 1, 3513, 19, 8, 2004, 4, 27, 85, 1, 3298, 1, 17291, 107, 90, 3, 253, 6, 67, 2, 71, 7, 3298, 8, 7519, 81, 3, 3762, 6, 5577, 3, 23841, 1, 9, 22, 8, 46, 62, 3, 4, 540, 540, 5510, 1, 1209, 8, 420, 6, 53, 36, 3, 2720, 21, 148, 37, 4, 121, 66, 5510, 3, 109, 426, 1, 7261, 7, 5633, 14, 94, 66, 5510, 7, 5633, 3343, 8, 22, 2, 7261, 109, 5510, 109, 8, 48077, 10, 344, 4, 51, 37, 7, 7520, 6, 8061, 3, 15, 5510, 39, 36927, 94, 5510, 39, 168, 108, 5, 24, 16, 10, 48078, 7, 1957, 6, 80118, 1, 9, 22, 2, 46, 1209, 7, 9371, 27, 73, 1, 3036, 95, 3154, 27, 62, 3, 2733, 2414, 8, 67, 10, 378, 50, 27, 83, 3, 107, 1250, 3, 1850, 2997, 265, 3, 44, 1, 13, 3298, 1, 9962, 17, 1, 2311, 24, 9, 22, 2, 174, 1639, 148, 3882, 4, 25536, 119, 2238, 344, 1, 1773, 295, 3, 4, 6154, 171, 127, 44, 2, 466, 14, 6, 18337, 3681, 3, 73, 7, 8966, 46, 73, 81, 8, 1174, 506, 141, 1, 2826, 1, 17701, 437, 1, 993, 104, 7, 556, 77, 269, 940, 1, 31, 19, 41, 5, 2]\n",
      " 或 る 心持 の よい 夕方 日比谷公園 の 樹 の 繁み の 間 で 若葉 楓 の 梢 を 眺め て い たら どこ から とも なく ラジオ の 声 が 流れ て 来 た 。 職業 紹介 で あっ た 。 ずっと 歩い て 行っ て 見 たら 空地 に 向っ た 高い ところ に 満州 国 から の 貴賓 を 迎える ため 赤 や 緑 で 装飾 さ れ た 拡声 機 が 据えつけ て あっ て そこ から 「 年齢 十 六 歳 前後 住む 込み で 月給 七 円 住み こみ で 月給 七 円 」 と 夕空 に 響い て いる の で あっ た 。 私 は パパ ママ は いけ ない という 松田 文相 の 小学 放送 の 試み や ラジオ に でる に は なかなか お金 が かかる ん で ねえ と 打ち かこっ た 或 る 長唄 の 師匠 の 言葉 など を 思い出し ながら その 声 を きい て いる の で あっ た 。 聴取 料 が 五 十 銭 に なっ た こと は ラジオ に対する 大衆 の 親しみ を 増し 何より の こと と 思う 。 ところで この間 馬場 先 を 通っ て い たら かね て 新聞 で 披露 さ れ て い た 犯人 逮捕 用 ラジオ 自動車 が 消防 自動車 の よう な 勢 で むこ う から 疾走 し て 来 た 。 通行人 も 珍し げに それ を よ け て 見送っ て い た 。 ふと 私 は 民間 自動車 の ラジオ は 許さ れ て い ず その 設備 の ある 新 車体 は セット を はずし て 車体 検査 を 受け ね ば なら ぬ という 事実 を 想い 起し 改めて 悠々 と 走り去る ラジオ 自動車 を 眺め た 。 \n",
      "[320, 131, 366, 1, 338, 1552, 23842, 1, 1996, 1, 17489, 1, 146, 9, 6837, 7953, 1, 3691, 7, 395, 6, 25, 115, 219, 20, 297, 69, 2598, 1, 180, 8, 790, 6, 74, 5, 2, 1450, 1606, 9, 58, 5, 2, 1356, 421, 6, 196, 6, 63, 115, 5496, 3, 813, 5, 788, 86, 3, 8062, 473, 20, 1, 60198, 7, 9731, 122, 757, 45, 2743, 9, 5999, 29, 35, 5, 22035, 2366, 8, 39427, 6, 58, 6, 169, 20, 15, 3645, 55, 178, 586, 1475, 3338, 2834, 9, 4914, 210, 449, 4820, 4481, 9, 4914, 210, 449, 16, 10, 22378, 3, 3028, 6, 30, 1, 9, 58, 5, 2, 34, 4, 7373, 7723, 4, 367, 18, 49, 22717, 21090, 1, 6494, 4120, 1, 1958, 45, 2598, 3, 1346, 3, 4, 615, 1190, 8, 1319, 28, 9, 261, 10, 1440, 31478, 5, 320, 131, 21091, 1, 2427, 1, 182, 104, 7, 1468, 92, 27, 180, 7, 582, 6, 30, 1, 9, 58, 5, 2, 16887, 3486, 8, 89, 55, 1083, 3, 64, 5, 23, 4, 2598, 525, 1197, 1, 4374, 7, 4072, 3063, 1, 23, 10, 208, 2, 938, 1595, 7262, 288, 7, 1727, 6, 25, 115, 961, 6, 515, 9, 7811, 29, 35, 6, 25, 5, 1811, 14621, 840, 2598, 1350, 8, 10843, 1350, 1, 38, 17, 1416, 9, 3799, 24, 20, 15302, 14, 6, 74, 5, 2, 9458, 13, 10521, 8099, 36, 7, 48, 303, 6, 6748, 6, 25, 5, 2, 542, 34, 4, 9513, 1350, 1, 2598, 4, 1889, 35, 6, 25, 70, 27, 6335, 1, 22, 444, 26544, 4, 20241, 7, 8615, 6, 26544, 8100, 7, 669, 56, 50, 66, 103, 49, 501, 7, 8193, 1305, 2657, 4037, 10, 30730, 2598, 1350, 7, 395, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "for text, vector in zip(X.body.head(3), x_train[0:3]):\n",
    "    print(text)\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=1000\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 32)          4722304   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 4,725,457\n",
      "Trainable params: 4,725,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class F1Callback(Callback):\n",
    "    def __init__(self, model, X_val, y_val):\n",
    "        self.model = model\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        pred = self.model.predict(self.X_val)\n",
    "        f1_val = f1_score(self.y_val, np.round(pred))\n",
    "        print(\"f1_val =\", f1_val)\n",
    "        # 以下チェックポイントなど必要なら書く\n",
    "\n",
    "vocabulary_size = len(tokenizer.word_index) + 1  # 学習データの語彙数+1\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=32))\n",
    "model.add(LSTM(16, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import warnings\n",
    "# display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "warnings.filterwarnings('ignore')\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(x_train, y, test_size = 0.2, random_state = 0) # 80%のデータを学習データに、20%を検証データにする\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2649 samples, validate on 663 samples\n",
      "Epoch 1/10\n",
      "2649/2649 [==============================] - 47s 18ms/step - loss: 0.3708 - accuracy: 0.9226 - val_loss: 0.2541 - val_accuracy: 0.9170\n",
      "f1_val = 0.0\n",
      "Epoch 2/10\n",
      "2649/2649 [==============================] - 45s 17ms/step - loss: 0.1798 - accuracy: 0.9320 - val_loss: 0.1833 - val_accuracy: 0.9231\n",
      "f1_val = 0.13559322033898305\n",
      "Epoch 3/10\n",
      "2649/2649 [==============================] - 50s 19ms/step - loss: 0.1346 - accuracy: 0.9577 - val_loss: 0.1347 - val_accuracy: 0.9608\n",
      "f1_val = 0.723404255319149\n",
      "Epoch 4/10\n",
      "2649/2649 [==============================] - 57s 22ms/step - loss: 0.0772 - accuracy: 0.9758 - val_loss: 0.1128 - val_accuracy: 0.9593\n",
      "f1_val = 0.7216494845360825\n",
      "Epoch 5/10\n",
      "2649/2649 [==============================] - 57s 21ms/step - loss: 0.0490 - accuracy: 0.9875 - val_loss: 0.1028 - val_accuracy: 0.9638\n",
      "f1_val = 0.7551020408163266\n",
      "Epoch 6/10\n",
      "2649/2649 [==============================] - 61s 23ms/step - loss: 0.0329 - accuracy: 0.9955 - val_loss: 0.0993 - val_accuracy: 0.9623\n",
      "f1_val = 0.7474747474747475\n",
      "Epoch 7/10\n",
      "2649/2649 [==============================] - 54s 21ms/step - loss: 0.0355 - accuracy: 0.9936 - val_loss: 0.1589 - val_accuracy: 0.9472\n",
      "f1_val = 0.5783132530120482\n",
      "Epoch 8/10\n",
      "2649/2649 [==============================] - 53s 20ms/step - loss: 0.0193 - accuracy: 0.9981 - val_loss: 0.1206 - val_accuracy: 0.9578\n",
      "f1_val = 0.7083333333333334\n",
      "Epoch 9/10\n",
      " 384/2649 [===>..........................] - ETA: 40s - loss: 0.0116 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c2c994a90297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF1Callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_X, train_y, batch_size=32, epochs=10,\n",
    "    validation_data=(valid_X, valid_y),callbacks=[F1Callback(model, valid_X, valid_y)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[604   4]\n",
      " [ 18  37]]\n",
      "accuracy =  0.9668174962292609\n",
      "precision =  0.9024390243902439\n",
      "recall =  0.6727272727272727\n",
      "f1 score =  0.7708333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "pred = model.predict_classes(valid_X)\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=valid_y, y_pred=pred))\n",
    "print('accuracy = ', accuracy_score(y_true=valid_y, y_pred=pred))\n",
    "print('precision = ', precision_score(y_true=valid_y, y_pred=pred))\n",
    "print('recall = ', recall_score(y_true=valid_y, y_pred=pred))\n",
    "print('f1 score = ', f1_score(y_true=valid_y, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
