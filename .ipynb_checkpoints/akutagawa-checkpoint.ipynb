{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf=pd.read_csv(\"data/train.csv\")\n",
    "testdf=pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mojimoji\n",
    "import re\n",
    "import jaconv\n",
    "def parse_text(text, debug=False):\n",
    "    '''\n",
    "    Get location\n",
    "    '''\n",
    "\n",
    "\n",
    "    text = re.sub(r'［[^］]+］', ' ', text)  \n",
    "#     text = re.sub(r'（[^）]+）', ' ', text)  \n",
    "    text = re.sub(r'○', ' ', text)    \n",
    "    text = re.sub(r'×', '', text)\n",
    "    text = re.sub(r'※', '', text)    \n",
    "#     text = re.sub(r'｜', ' ', text)\n",
    "    text = re.sub(r'[\\s、]', '' , text)\n",
    "#     text = re.sub(r'一', ' ', text)\n",
    "#     text = re.sub(r'…', ' ', text)  \n",
    "#     text = re.sub(r'―', ' ', text)\n",
    "    text = re.sub(r'[0-9]', '0', text)\n",
    "\n",
    "#     text = mojimoji.zen_to_han(text, kana=False)\n",
    "#     text = jaconv.kata2hira(text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\n\\r\\n○僕はこれからも今月のと同じような材料を使って創作するつもりである。あれを単なる歴史小説の仲間入をさせられてはたまらない。もちろん今のがたいしたものだとは思わないが。そのうちにもう少しどうにかできるだろう。（新思潮創刊号）\\r\\n○酒虫《しゅちゅう》は材料を聊斎志異《りょうさいしい》からとった。原《もと》の話とほとんど変わったところはない。（新思潮第四号）\\r\\n○酒虫は「しゅちゅう」で「さかむし」ではない。気になるから、書き加える。（新思潮第六号）\\r\\n○僕は新小説の九月号に「芋粥《いもがゆ》」という小説を書いた。\\r\\n○まだあき地があるそうだから、もう少し書く。松岡の手紙によると、新思潮は新潟《にいがた》県にまじめな読者をかなり持っているそうだ。そうしてその人たちの中には、創作に志している青年も多いそうだ。ひとり新思潮のためのみならず、日本のためにも、そういう人たちの多くなることを祈りたい。もし同人のうぬぼれが、単にうぬぼれにとどまらない以上は。\\r\\n○僕の書くものを、小さくまとまりすぎていると言うて非難する人がある。しかし僕は、小さくとも完成品を作りたいと思っている。芸術の境に未成品はない。大いなる完成品に至る途《みち》は、小なる完成品あるのみである。流行の大なる未成品のごときは、僕にとって、なんらの意味もない。（以上新思潮第七号）\\r\\n○「煙草《たばこ》」の材料は、昔、高木さんの比較神話学を読んだ時に見た話を少し変えて使った。どこの伝説だか、その本にも書いてなかったように思う。\\r\\n○新小説へ書いた「煙管《きせる》」の材料も、加州藩の古老に聞いた話を、やはり少し変えて使った。前に出した「虱《しらみ》」とこれと、来月出す「明君」とは皆、同じ人の集めてくれた材料である。\\r\\n○同人は皆、非常に自信家のように思う人があるが、それは大ちがいだ。ほかの作家の書いたものに、帽子をとることも、ずいぶんある。なんでもしっかりつかまえて、書いてある人を見ると、書いていることはしばらく問題外に置いて、つかまえ方、書き方のうまいのには、敬意を表せずにはいられないことが多い。（そういう人は、自然派の作家の中にもいる）傾向ばかり見て感心するより、こういう感心のしかたのほうが、より合理的だと思っているから。\\r\\n○ほめられれば作家が必ずよろこぶと思うのは少し虫がいい。\\r\\n○批評家が作家に折紙をつけるばかりではない。作家も批評家へ折紙をつける。しかも作家のつける折紙のほうが、論理的な部分は、客観的にも、正否がきめられうるから。（以上新思潮第九号）\\r\\n○夏目先生の逝去《せいきょ》ほど惜しいものはない。先生は過去において、十二分に仕事をされた人である。が、先生の逝去ほど惜しいものはない。先生は、このごろある転機の上に立っていられたようだから。すべての偉大な人のように、五十歳を期として、さらに大踏歩《だいとうほ》を進められようとしていたから。\\r\\n○僕一身から言うと、ほかの人にどんな悪口を言われても先生にほめられれば、それで満足だった。同時に先生を唯一の標準にすることの危険を、時々は怖《おそ》れもした。\\r\\n○それから僕はいろんな事情に妨げられて、この正月にはちっとも働けなかった。働いた範囲においても時間が足りないので、無理をしたのが多い。これは今考えても不快である。自分の良心の上からばかりでなく、ほかの雑誌の編輯者《へんしゅうしゃ》に、さぞ迷惑をかけたろうと思うと、実際いい気はしない。\\r\\n○これからは、作ができてから、遣《つか》うものなら遣ってもらうようにしたいと思う。とうからもそう思っていたが、このごろは特にその感が深い。\\r\\n○そうして、ゆっくり腰をすえて、自分の力の許す範囲で、少しは大きなものにぶつかりたい。計画がないでもないが、どうも失敗しそうで、逡巡《しゅんじゅん》したくなる。アミエルの言ったように、腕だめしに剣を揮《ふ》ってみるばかりで、一度もそれを実際に使わないようなことになっては、たいへんだと思う。\\r\\n○絶えず必然に、底力強く進歩していかれた夏目先生を思うと、自分のいくじないのが恥かしい。心から恥かしい。\\r\\n○文壇は来るべきなにものかに向かって動きつつある。亡《ほろ》ぶべき者が亡びるとともに、生まるべき者は必ず生まれそうに思われる。今年は必ず何かある。何かあらずにはいられない、僕らは皆小手しらべはすんだという気がしている。（以上新思潮第二年第一号）\\r\\n［＃地から２字上げ］（大正五年三月―大正六年一月）\\r\\n\\r\\n\\r\\n\\r\\n'"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf.body[815]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'僕はこれからも今月のと同じような材料を使って創作するつもりである。あれを単なる歴史小説の仲間入をさせられてはたまらない。もちろん今のがたいしたものだとは思わないが。そのうちにもう少しどうにかできるだろう。（新思潮創刊号）酒虫《しゅちゅう》は材料を聊斎志異《りょうさいしい》からとった。原《もと》の話とほとんど変わったところはない。（新思潮第四号）酒虫は「しゅちゅう」で「さかむし」ではない。気になるから書き加える。（新思潮第六号）僕は新小説の九月号に「芋粥《いもがゆ》」という小説を書いた。まだあき地があるそうだからもう少し書く。松岡の手紙によると新思潮は新潟《にいがた》県にまじめな読者をかなり持っているそうだ。そうしてその人たちの中には創作に志している青年も多いそうだ。ひとり新思潮のためのみならず日本のためにもそういう人たちの多くなることを祈りたい。もし同人のうぬぼれが単にうぬぼれにとどまらない以上は。僕の書くものを小さくまとまりすぎていると言うて非難する人がある。しかし僕は小さくとも完成品を作りたいと思っている。芸術の境に未成品はない。大いなる完成品に至る途《みち》は小なる完成品あるのみである。流行の大なる未成品のごときは僕にとってなんらの意味もない。（以上新思潮第七号）「煙草《たばこ》」の材料は昔高木さんの比較神話学を読んだ時に見た話を少し変えて使った。どこの伝説だかその本にも書いてなかったように思う。新小説へ書いた「煙管《きせる》」の材料も加州藩の古老に聞いた話をやはり少し変えて使った。前に出した「虱《しらみ》」とこれと来月出す「明君」とは皆同じ人の集めてくれた材料である。同人は皆非常に自信家のように思う人があるがそれは大ちがいだ。ほかの作家の書いたものに帽子をとることもずいぶんある。なんでもしっかりつかまえて書いてある人を見ると書いていることはしばらく問題外に置いてつかまえ方書き方のうまいのには敬意を表せずにはいられないことが多い。（そういう人は自然派の作家の中にもいる）傾向ばかり見て感心するよりこういう感心のしかたのほうがより合理的だと思っているから。ほめられれば作家が必ずよろこぶと思うのは少し虫がいい。批評家が作家に折紙をつけるばかりではない。作家も批評家へ折紙をつける。しかも作家のつける折紙のほうが論理的な部分は客観的にも正否がきめられうるから。（以上新思潮第九号）夏目先生の逝去《せいきょ》ほど惜しいものはない。先生は過去において十二分に仕事をされた人である。が先生の逝去ほど惜しいものはない。先生はこのごろある転機の上に立っていられたようだから。すべての偉大な人のように五十歳を期としてさらに大踏歩《だいとうほ》を進められようとしていたから。僕一身から言うとほかの人にどんな悪口を言われても先生にほめられればそれで満足だった。同時に先生を唯一の標準にすることの危険を時々は怖《おそ》れもした。それから僕はいろんな事情に妨げられてこの正月にはちっとも働けなかった。働いた範囲においても時間が足りないので無理をしたのが多い。これは今考えても不快である。自分の良心の上からばかりでなくほかの雑誌の編輯者《へんしゅうしゃ》にさぞ迷惑をかけたろうと思うと実際いい気はしない。これからは作ができてから遣《つか》うものなら遣ってもらうようにしたいと思う。とうからもそう思っていたがこのごろは特にその感が深い。そうしてゆっくり腰をすえて自分の力の許す範囲で少しは大きなものにぶつかりたい。計画がないでもないがどうも失敗しそうで逡巡《しゅんじゅん》したくなる。アミエルの言ったように腕だめしに剣を揮《ふ》ってみるばかりで一度もそれを実際に使わないようなことになってはたいへんだと思う。絶えず必然に底力強く進歩していかれた夏目先生を思うと自分のいくじないのが恥かしい。心から恥かしい。文壇は来るべきなにものかに向かって動きつつある。亡《ほろ》ぶべき者が亡びるとともに生まるべき者は必ず生まれそうに思われる。今年は必ず何かある。何かあらずにはいられない僕らは皆小手しらべはすんだという気がしている。（以上新思潮第二年第一号）（大正五年三月―大正六年一月）'"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_text(testdf.body[815])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([traindf,testdf])\n",
    "df.body=df.body.map(parse_text)\n",
    "traindf.body=traindf.body.map(parse_text)\n",
    "testdf.body=testdf.body.map(parse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# def doc_to_corpus(doc_list):\n",
    "#     for i, line in enumerate(doc_list):\n",
    "#         yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n",
    "# # doc2vecの学習\n",
    "# all_corpus = list(doc_to_corpus(df.body))\n",
    "# model = gensim.models.doc2vec.Doc2Vec(size=1000, min_count=2, iter=55)\n",
    "# model.build_vocab(all_corpus)\n",
    "# %time model.train(all_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "# bodyvec = np.array([model.infer_vector(x.words) for x in all_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bodyvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import MeCab\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = normalize_unicode(text)\n",
    "    normalized_text = normalize_number(normalized_text)\n",
    "    normalized_text = lower_text(normalized_text)\n",
    "    return normalized_text\n",
    "\n",
    "\n",
    "def lower_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def normalize_unicode(text, form='NFKC'):\n",
    "    normalized_text = unicodedata.normalize(form, text)\n",
    "    return normalized_text\n",
    "\n",
    "\n",
    "def lemmatize_term(term, pos=None):\n",
    "    if pos is None:\n",
    "        synsets = wordnet.synsets(term)\n",
    "        if not synsets:\n",
    "            return term\n",
    "        pos = synsets[0].pos()\n",
    "        if pos == wordnet.ADJ_SAT:\n",
    "            pos = wordnet.ADJ\n",
    "    return nltk.WordNetLemmatizer().lemmatize(term, pos=pos)\n",
    "\n",
    "\n",
    "def normalize_number(text):\n",
    "    \"\"\"\n",
    "    pattern = r'\\d+'\n",
    "    replacer = re.compile(pattern)\n",
    "    result = replacer.sub('0', text)\n",
    "    \"\"\"\n",
    "    # 連続した数字を0で置換\n",
    "    replaced_text = re.sub(r'\\d+', '0', text)\n",
    "    return replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "mecab = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd/')\n",
    "# mecab=MeCab.Tagger ('-d /usr/local/lib/mecab/dic/UniDic-kindai_1603')\n",
    "# mecab = MeCab.Tagger('-Owakati')\n",
    "\n",
    "# 形態素解析をして、名詞だけ取り出す\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "#     available_norm = ['接尾', '一般', '形容動詞語幹', 'サ変接続']\n",
    "    node = mecab.parseToNode(text)\n",
    "    l = []\n",
    "    while node:\n",
    "\n",
    "        l.append(node.surface)\n",
    "        node = node.next\n",
    "    return ' '.join(l)\n",
    "\n",
    "\n",
    "# 記事群のdictについて、形態素解析をしてリストに返す\n",
    "def get_words(contents):\n",
    "    ret = []\n",
    "    for content in contents:\n",
    "        ret.append(get_words_main(content))\n",
    "    return ret\n",
    "\n",
    "# 一つの記事を形態素解析して返す\n",
    "\n",
    "\n",
    "def get_words_main(content):\n",
    "    return [token for token in tokenize1(content)]\n",
    "\n",
    "\n",
    "def get_words_main1(content):\n",
    "    retoken = \"\"\n",
    "    for token in tokenize(content):\n",
    "        retoken += token+\" \"\n",
    "    return retoken\n",
    "\n",
    "\n",
    "def tokenize1(text):\n",
    "    available_norm = ['接尾', '一般', '形容動詞語幹', 'サ変接続']\n",
    "    node = mecab.parseToNode(text)\n",
    "    l = []\n",
    "    while node:\n",
    "        l.append(node.surface)\n",
    "        node = node.next\n",
    "    return l\n",
    "def tokenize2(text):\n",
    "    available_norm = ['接尾', '一般', '形容動詞語幹', 'サ変接続']\n",
    "    node = mecab.parseToNode(text)\n",
    "    l = []\n",
    "    while node:\n",
    "        l.append(node.surface)\n",
    "        node = node.next\n",
    "    bigram_list = []\n",
    "    for i in range(len(l)-1):\n",
    "        bigram_list.append((l[i], l[i+1]))\n",
    "    return bigram_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": BOS/EOS * \n",
      "宮: 名詞 固有名詞 宮\n",
      "根: 名詞 一般 根\n",
      "誠司: 名詞 固有名詞 誠司\n",
      "は: 助詞 係助詞 は\n",
      "すごく: 形容詞 自立 すごく\n",
      "ない: 助動詞 * ない\n",
      "。: 記号 句点 。\n",
      "自然: 名詞 形容動詞語幹 自然\n",
      "言語: 名詞 一般 言語\n",
      "処理: 名詞 サ変接続 処理\n",
      "は: 助詞 係助詞 は\n",
      "楽しい: 形容詞 自立 楽しい\n",
      "。: 記号 句点 。\n",
      "みんな: 名詞 代名詞 みんな\n",
      "、: 記号 読点 、\n",
      "自然: 名詞 形容動詞語幹 自然\n",
      "言語: 名詞 一般 言語\n",
      "処理: 名詞 サ変接続 処理\n",
      "に: 助詞 格助詞 に\n",
      "取り掛かっ: 動詞 自立 取り掛かっ\n",
      "て: 助詞 接続助詞 て\n",
      "みよ: 動詞 非自立 みよ\n",
      "う: 助動詞 * う\n",
      "。: 記号 句点 。\n",
      ": BOS/EOS * \n"
     ]
    }
   ],
   "source": [
    "# mecab=MeCab.Tagger ('-d /usr/local/lib/mecab/dic/UniDic-qkana_1603')\n",
    "mecab = MeCab.Tagger('-Owakati')\n",
    "# mecab = MeCab.Tagger('-d /usr/local/lib/mecab/dic/UniDic-kindai_1603')\n",
    "node = mecab.parseToNode(\"宮根誠司はすごくない。自然言語処理は楽しい。みんな、自然言語処理に取り掛かってみよう。\")\n",
    "while node:\n",
    "    word = node.surface\n",
    "    hinshi = node.feature.split(\",\")[0]\n",
    "    print(word+\": \"+hinshi,node.feature.split(',')[1],node.surface.lower())\n",
    "#     print(node)\n",
    "    node = node.next\n",
    "# get_words(traindf.body[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import MeCab\n",
    "# tagger = MeCab.Tagger(\"\")\n",
    "\n",
    "\n",
    "# def japanese_analyzer(string):\n",
    "#     result_list = []\n",
    "#     for line in tagger.parse(string).split(\"\\n\"):\n",
    "#         splited_line = line.split(\"\\t\")\n",
    "#         if len(splited_line) >= 2 and \"名詞\" in splited_line[1]:\n",
    "#             result_list.append(splited_line[0])\n",
    "#     return result_list\n",
    "\n",
    "\n",
    "# X_text = list(df.body.map(tokenize).values)\n",
    "y = df['author'].values\n",
    "\n",
    "count = CountVectorizer(analyzer=tokenize1)\n",
    "bags = count.fit_transform(df.body.values)\n",
    "# print(bags.toarray())\n",
    "\n",
    "features = count.get_feature_names()\n",
    "# print(features)\n",
    "\n",
    "bodyvec = pd.DataFrame(bags.toarray(), columns=features)\n",
    "# newdf = pd.concat([df.reset_index(drop=True), pd.DataFrame(bodyvec)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordbody=bodyvec.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "の             1371701\n",
       "。              922501\n",
       "に              825702\n",
       "は              788479\n",
       "た              728677\n",
       "               ...   \n",
       "ジャアナリスチック           1\n",
       "畳み込み                1\n",
       "ジメ                  1\n",
       "ジムバリストパアロー          1\n",
       "クラガヘ                1\n",
       "Length: 148113, dtype: int64"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordbody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bodyvec1 = bodyvec[list(wordbody[wordbody > 100].index)]\n",
    "xsum = bodyvec1.sum(axis=1)\n",
    "\n",
    "xsum = np.array(xsum).reshape(len(xsum), 1)\n",
    "newdf = pd.concat([df.reset_index(drop=True), pd.DataFrame(bodyvec1/xsum*100)], axis=1)\n",
    "# newdf = pd.concat([df.reset_index(drop=True), pd.DataFrame(bodyvec1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newdf=pd.concat([newdf.author,newdf.writing_id,newdf.body,newdf.iloc[:,list(newdf[newdf[\"author\"]==1].sum()==0)]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=newdf.dropna().drop([\"writing_id\",\"body\",],axis=1)\n",
    "test=newdf[newdf.author.isnull()].drop([\"writing_id\",\"body\"],axis=1)\n",
    "test=test.drop([\"author\"],axis=1)\n",
    "X = train.drop([\"author\"],axis=1)\n",
    "y = train.author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn import preprocessing\n",
    "mmscaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "mmscaler.fit(pd.concat([X,test]))\n",
    "X = mmscaler.transform(X)\n",
    "test= mmscaler.transform(test)\n",
    "# display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "warnings.filterwarnings('ignore')\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.3, random_state = 0) # 80%のデータを学習データに、20%を検証データにする\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3312, 12283), (3312,))"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[919   0]\n",
      " [ 11  64]]\n",
      "accuracy =  0.9889336016096579\n",
      "precision =  1.0\n",
      "recall =  0.8533333333333334\n",
      "f1 score =  0.9208633093525179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "lr = LogisticRegression() # ロジスティック回帰モデルのインスタンスを作成\n",
    "lr.fit(train_X, train_y) # ロジスティック回帰モデルの重みを学習\n",
    "pred = lr.predict(test_X)\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=test_y, y_pred=pred))\n",
    "print('accuracy = ', accuracy_score(y_true=test_y, y_pred=pred))\n",
    "print('precision = ', precision_score(y_true=test_y, y_pred=pred))\n",
    "print('recall = ', recall_score(y_true=test_y, y_pred=pred))\n",
    "print('f1 score = ', f1_score(y_true=test_y, y_pred=pred))\n",
    "# confusion matrix = 100\n",
    "#  [[919   0]\n",
    "#  [ 11  64]]\n",
    "# accuracy =  0.9889336016096579\n",
    "# precision =  1.0\n",
    "# recall =  0.8533333333333334\n",
    "# f1 score =  0.9208633093525179"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# model = CatBoostClassifier(**{\"depth\":6})\n",
    "# model.fit(train_X, train_y)\n",
    "# pred = model.predict(test_X)\n",
    "# print('confusion matrix = \\n', confusion_matrix(y_true=test_y, y_pred=pred))\n",
    "# print('accuracy = ', accuracy_score(y_true=test_y, y_pred=pred))\n",
    "# print('precision = ', precision_score(y_true=test_y, y_pred=pred))\n",
    "# print('recall = ', recall_score(y_true=test_y, y_pred=pred))\n",
    "# print('f1 score = ', f1_score(y_true=test_y, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[919   0]\n",
      " [  1  74]]\n",
      "accuracy =  0.9989939637826962\n",
      "precision =  1.0\n",
      "recall =  0.9866666666666667\n",
      "f1 score =  0.9932885906040269\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "mlp=MLPClassifier(**{\"hidden_layer_sizes\":(128,128,128),\"random_state\":42})\n",
    "mlp.fit(train_X,train_y)\n",
    "pred = mlp.predict(test_X)\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=test_y, y_pred=pred))\n",
    "print('accuracy = ', accuracy_score(y_true=test_y, y_pred=pred))\n",
    "print('precision = ', precision_score(y_true=test_y, y_pred=pred))\n",
    "print('recall = ', recall_score(y_true=test_y, y_pred=pred))\n",
    "print('f1 score = ', f1_score(y_true=test_y, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[919   0]\n",
      " [  1  74]]\n",
      "accuracy =  0.9989939637826962\n",
      "precision =  1.0\n",
      "recall =  0.9866666666666667\n",
      "f1 score =  0.9932885906040269\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "mlpr=MLPRegressor(**{\"hidden_layer_sizes\":(128,128,128),\"random_state\":42})\n",
    "mlpr.fit(train_X,train_y)\n",
    "pred = np.where(mlpr.predict(test_X)>0.38,1,0)\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=test_y, y_pred=pred))\n",
    "print('accuracy = ', accuracy_score(y_true=test_y, y_pred=pred))\n",
    "print('precision = ', precision_score(y_true=test_y, y_pred=pred))\n",
    "print('recall = ', recall_score(y_true=test_y, y_pred=pred))\n",
    "print('f1 score = ', f1_score(y_true=test_y, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[916   3]\n",
      " [  0  75]]\n",
      "accuracy =  0.9969818913480886\n",
      "precision =  0.9615384615384616\n",
      "recall =  1.0\n",
      "f1 score =  0.9803921568627451\n"
     ]
    }
   ],
   "source": [
    "pred = np.where(mlpr.predict(test_X)>0.29,1,0)\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=test_y, y_pred=pred))\n",
    "print('accuracy = ', accuracy_score(y_true=test_y, y_pred=pred))\n",
    "print('precision = ', precision_score(y_true=test_y, y_pred=pred))\n",
    "print('recall = ', recall_score(y_true=test_y, y_pred=pred))\n",
    "print('f1 score = ', f1_score(y_true=test_y, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import RidgeClassifier\n",
    "# from sklearn.model_selection import KFold\n",
    "# import optuna\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.metrics import r2_score\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# class RidgeCV():\n",
    "#     model_cls = RidgeClassifier\n",
    "\n",
    "#     def __init__(self, n_trials=100):\n",
    "#         self.n_trials = n_trials\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         if isinstance(X, np.ndarray):\n",
    "#             X = pd.DataFrame(X)\n",
    "#             y = pd.DataFrame(y)\n",
    "#         elif isinstance(X, pd.DataFrame):\n",
    "#             X = X.reset_index(drop=True)\n",
    "#             y = y.reset_index(drop=True)\n",
    "\n",
    "#         self.X = X\n",
    "#         self.y = y\n",
    "\n",
    "#         study = optuna.create_study(direction='maximize')\n",
    "# #         study = optuna.create_study(direction='minimize')\n",
    "#         study.optimize(self, n_trials=self.n_trials)\n",
    "#         self.best_trial = study.best_trial\n",
    "\n",
    "#         print()\n",
    "#         print(\"Best score:\", round(self.best_trial.value, 2))\n",
    "#         print(\"Best params:\", self.best_trial.params)\n",
    "#         print()\n",
    "\n",
    "#         self.best_model = self.model_cls(**self.best_trial.params)\n",
    "#         self.best_model.fit(self.X, self.y)\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         if isinstance(X, pd.Series):\n",
    "#             X = pd.DataFrame(X.values.reshape(1, -1))\n",
    "#         elif isinstance(X, np.ndarray):\n",
    "#             X = pd.DataFrame(X)\n",
    "\n",
    "#         return self.best_model.predict(X)\n",
    "\n",
    "#     def score(self, X, y):\n",
    "#         if isinstance(X, np.ndarray):\n",
    "#             X = pd.DataFrame(X)\n",
    "#             y = pd.DataFrame(y)\n",
    "\n",
    "#         return self.best_model.score(X, y)\n",
    "\n",
    "#     def kfold_cv(self, model, splits=5):\n",
    "#         scores = []\n",
    "\n",
    "#         kf = KFold(n_splits=splits, shuffle=True)\n",
    "#         for train_index, test_index in kf.split(self.X):\n",
    "#             X_train, X_test = self.X.iloc[train_index], self.X.iloc[test_index]\n",
    "#             y_train, y_test = self.y.iloc[train_index], self.y.iloc[test_index]\n",
    "#             model.fit(X_train, y_train)\n",
    "#             scores.append(f1_score(model.predict(X_test),y_test))\n",
    "\n",
    "#         score = np.array(scores).mean()\n",
    "#         return score\n",
    "# class LGBCV(RidgeCV):\n",
    "#     model_cls = lgb.LGBMClassifier\n",
    "#     def __call__(self, trial):\n",
    "#         params = {\n",
    "# #             'eval_metric':'mae',\n",
    "# #             'booster':trial.suggest_categorical('booster',['gbtree','gblinear']),\n",
    "# #             'loss_function': 'fair',\n",
    "# #             'iterations' : trial.suggest_int('iterations', 50, 400),                      \n",
    "#             'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf' , 0, 300),  \n",
    "# #             'depth':16,\n",
    "# #             'regression_l1':'mean_absolute_error',\n",
    "#             'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),               \n",
    "# #             'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n",
    "# #             'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00), \n",
    "# #             'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "# #             'od_wait' :trial.suggest_int('od_wait', 10, 50),\n",
    "# #             'metric': 'auc',\n",
    "#             'verbosity': -1,\n",
    "#             'boosting_type': 'gbdt',\n",
    "# #             'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "# #             'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "#             'num_leaves': trial.suggest_int('num_leaves', 2, 4000),\n",
    "#             'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "#             'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "#             'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "#             'min_child_samples': trial.suggest_int('min_child_samples', 50, 5000),\n",
    "#     }\n",
    "#         model=self.model_cls(**params)\n",
    "#         score = self.kfold_cv(model)\n",
    "#         return score\n",
    "# model=LGBCV(n_trials=100)\n",
    "\n",
    "# # \n",
    "# model.fit(np.array(X),np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[917   2]\n",
      " [  7  68]]\n",
      "accuracy =  0.9909456740442656\n",
      "precision =  0.9714285714285714\n",
      "recall =  0.9066666666666666\n",
      "f1 score =  0.9379310344827586\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# lr.fit(X,y)\n",
    "# rf.fit(X,y)\n",
    "lgbmodel=lgb.LGBMClassifier(** {'min_data_in_leaf': 275, 'learning_rate': 0.46519050036541065, 'num_leaves': 820, 'feature_fraction': 0.5793467152844205, 'bagging_fraction': 0.7388500720094163, 'bagging_freq': 2, 'min_child_samples': 263})\n",
    "lgbmodel.fit(np.array(train_X),np.array(train_y))\n",
    "pred = lgbmodel.predict(np.array(test_X))\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=test_y, y_pred=pred))\n",
    "print('accuracy = ', accuracy_score(y_true=test_y, y_pred=pred))\n",
    "print('precision = ', precision_score(y_true=test_y, y_pred=pred))\n",
    "print('recall = ', recall_score(y_true=test_y, y_pred=pred))\n",
    "print('f1 score = ', f1_score(y_true=test_y, y_pred=pred))\n",
    "# print('Feature Importances:')\n",
    "# # for i,feat in enumerate(X.columns):\n",
    "# #     print('\\t{0:10s} : {1:>.6f}'.format(feat, fti[i]))\n",
    "# col_names = test.columns.values\n",
    "# col_names_ = col_names[np.argsort(lgbmodel.feature_importances_)[::-1]]\n",
    "# col_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[918   1]\n",
      " [  7  68]]\n",
      "accuracy =  0.9919517102615694\n",
      "precision =  0.9855072463768116\n",
      "recall =  0.9066666666666666\n",
      "f1 score =  0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# lr.fit(X,y)\n",
    "# rf.fit(X,y)\n",
    "xgbmodel=XGBClassifier()\n",
    "xgbmodel.fit(np.array(train_X),np.array(train_y))\n",
    "pred = xgbmodel.predict(np.array(test_X))\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=test_y, y_pred=pred))\n",
    "print('accuracy = ', accuracy_score(y_true=test_y, y_pred=pred))\n",
    "print('precision = ', precision_score(y_true=test_y, y_pred=pred))\n",
    "print('recall = ', recall_score(y_true=test_y, y_pred=pred))\n",
    "print('f1 score = ', f1_score(y_true=test_y, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=lgb.LGBMClassifier(**lgbm_params)\n",
    "# model=LogisticRegression() \n",
    "# model.fit(np.array(X),np.array(y))\n",
    "mlp=MLPClassifier(**{\"hidden_layer_sizes\":(128,128,128),\"random_state\":42})\n",
    "mlp.fit(X,y)\n",
    "pred = mlp.predict(test)\n",
    "# pred = model.predict(np.array(test))\n",
    "pred=np.where(pred>0.5,1,0)\n",
    "sub = pd.DataFrame(pd.read_csv(\"data/test.csv\")['writing_id'])\n",
    "sub[\"author\"] = list(pred)\n",
    "sub.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "mlpr=MLPRegressor(**{\"hidden_layer_sizes\":(128,128,128),\"random_state\":42})\n",
    "mlpr.fit(X,y)\n",
    "pred = np.where(mlpr.predict(test)>0.3,1,0)\n",
    "sub = pd.DataFrame(pd.read_csv(\"data/test.csv\")['writing_id'])\n",
    "sub[\"author\"] = list(pred)\n",
    "sub.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=mlpr.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['の',\n",
       " '。',\n",
       " 'に',\n",
       " 'は',\n",
       " 'た',\n",
       " 'て',\n",
       " 'を',\n",
       " 'が',\n",
       " 'で',\n",
       " 'と',\n",
       " '《',\n",
       " '》',\n",
       " 'も',\n",
       " 'し',\n",
       " '「',\n",
       " '」',\n",
       " 'な',\n",
       " 'ない',\n",
       " 'だ',\n",
       " 'から',\n",
       " 'か',\n",
       " 'ある',\n",
       " 'こと',\n",
       " 'う',\n",
       " 'い',\n",
       " 'へ',\n",
       " 'その',\n",
       " 'ん',\n",
       " 'さ',\n",
       " 'いる',\n",
       " 'もの',\n",
       " 'です',\n",
       " 'まし',\n",
       " '私',\n",
       " 'れ',\n",
       " '一',\n",
       " 'それ',\n",
       " '人',\n",
       " 'よう',\n",
       " 'お',\n",
       " '…',\n",
       " 'つて',\n",
       " 'ます',\n",
       " 'する',\n",
       " 'や',\n",
       " 'この',\n",
       " '――',\n",
       " 'よ',\n",
       " 'つ',\n",
       " 'という',\n",
       " 'ば',\n",
       " '二',\n",
       " '何',\n",
       " 'あり',\n",
       " '的',\n",
       " '十',\n",
       " 'ね',\n",
       " 'ませ',\n",
       " 'あっ',\n",
       " '自分',\n",
       " '彼',\n",
       " 'さん',\n",
       " '中',\n",
       " 'なっ',\n",
       " '見',\n",
       " '三',\n",
       " 'なら',\n",
       " 'ゐる',\n",
       " 'まで',\n",
       " 'なく',\n",
       " 'ず',\n",
       " 'これ',\n",
       " 'そう',\n",
       " '日',\n",
       " '来',\n",
       " 'ひ',\n",
       " 'せ',\n",
       " '時',\n",
       " '？',\n",
       " 'やう',\n",
       " 'ゐ',\n",
       " 'として',\n",
       " 'だけ',\n",
       " 'なる',\n",
       " 'ところ',\n",
       " 'き',\n",
       " '家',\n",
       " 'られ',\n",
       " '五',\n",
       " '方',\n",
       " '上',\n",
       " '事',\n",
       " 'ながら',\n",
       " 'なかっ',\n",
       " '女',\n",
       " '者',\n",
       " 'でも',\n",
       " '八',\n",
       " '（',\n",
       " '）',\n",
       " '僕',\n",
       " 'そして',\n",
       " 'もう',\n",
       " 'ぬ',\n",
       " 'など',\n",
       " 'あの',\n",
       " 'といふ',\n",
       " 'たち',\n",
       " '出',\n",
       " 'なり',\n",
       " 'ゝ',\n",
       " '年',\n",
       " 'また',\n",
       " '顔',\n",
       " 'れる',\n",
       " 'たら',\n",
       " 'いい',\n",
       " 'でし',\n",
       " '前',\n",
       " 'ので',\n",
       " 'より',\n",
       " '男',\n",
       " 'ため',\n",
       " 'ばかり',\n",
       " 'み',\n",
       " 'たり',\n",
       " '気',\n",
       " '手',\n",
       " '云',\n",
       " 'どう',\n",
       " 'る',\n",
       " '心',\n",
       " '言',\n",
       " 'ぢ',\n",
       " 'そんな',\n",
       " 'うち',\n",
       " '眼',\n",
       " '生活',\n",
       " '一つ',\n",
       " 'り',\n",
       " 'ふ',\n",
       " 'だっ',\n",
       " 'あ',\n",
       " '平次',\n",
       " '日本',\n",
       " 'わ',\n",
       " '人間',\n",
       " 'だろ',\n",
       " 'つた',\n",
       " '間',\n",
       " 'なけれ',\n",
       " 'たい',\n",
       " 'あつ',\n",
       " '居る',\n",
       " '四',\n",
       " '話',\n",
       " 'でしょ',\n",
       " '今',\n",
       " '彼女',\n",
       " 'よく',\n",
       " 'って',\n",
       " '御',\n",
       " 'ら',\n",
       " 'え',\n",
       " '！',\n",
       " '君',\n",
       " 'てる',\n",
       " 'いふ',\n",
       " 'ほど',\n",
       " 'そこ',\n",
       " 'なかつ',\n",
       " '大',\n",
       " '文学',\n",
       " '目',\n",
       " '云っ',\n",
       " 'しかし',\n",
       " '感じ',\n",
       " 'とき',\n",
       " 'あなた',\n",
       " '六',\n",
       " '見る',\n",
       " 'じゃ',\n",
       " '言葉',\n",
       " '声',\n",
       " 'わけ',\n",
       " '娘',\n",
       " '居',\n",
       " 'らしい',\n",
       " 'こ',\n",
       " '郎',\n",
       " '誰',\n",
       " '出し',\n",
       " '少し',\n",
       " '来る',\n",
       " 'お前',\n",
       " 'まだ',\n",
       " '行っ',\n",
       " '考え',\n",
       " '子供',\n",
       " 'くれ',\n",
       " 'ま',\n",
       " '｜',\n",
       " '思っ',\n",
       " '行く',\n",
       " '云う',\n",
       " 'かも',\n",
       " 'め',\n",
       " '思う',\n",
       " 'く',\n",
       " '下',\n",
       " '七',\n",
       " '風',\n",
       " 'かけ',\n",
       " '今日',\n",
       " '頭',\n",
       " '外',\n",
       " '伸子',\n",
       " 'す',\n",
       " 'どこ',\n",
       " '本',\n",
       " 'ござい',\n",
       " '後',\n",
       " 'けれども',\n",
       " '親分',\n",
       " '々',\n",
       " '口',\n",
       " '母',\n",
       " '九',\n",
       " 'ただ',\n",
       " '',\n",
       " '思',\n",
       " '出来',\n",
       " 'あろ',\n",
       " '知れ',\n",
       " 'ち',\n",
       " 'しまっ',\n",
       " '度',\n",
       " 'について',\n",
       " 'つけ',\n",
       " '力',\n",
       " '姿',\n",
       " '同じ',\n",
       " 'じ',\n",
       " 'いや',\n",
       " 'いっ',\n",
       " 'いう',\n",
       " 'べき',\n",
       " 'こんな',\n",
       " '知ら',\n",
       " '思い',\n",
       " '屋',\n",
       " '町',\n",
       " 'あれ',\n",
       " 'わたし',\n",
       " 'ひと',\n",
       " 'とか',\n",
       " 'なつ',\n",
       " '第',\n",
       " '物',\n",
       " '先生',\n",
       " '頃',\n",
       " 'そういう',\n",
       " 'ねえ',\n",
       " 'ここ',\n",
       " '作家',\n",
       " '夜',\n",
       " '金',\n",
       " '所',\n",
       " 'つい',\n",
       " 'られる',\n",
       " '子',\n",
       " 'でき',\n",
       " '又',\n",
       " '百',\n",
       " '様',\n",
       " '仕事',\n",
       " '行',\n",
       " '問題',\n",
       " 'まい',\n",
       " '見え',\n",
       " 'だら',\n",
       " '來',\n",
       " '社会',\n",
       " '部屋',\n",
       " '小説',\n",
       " '書い',\n",
       " '俺',\n",
       " '達',\n",
       " '居り',\n",
       " '思ひ',\n",
       " '先',\n",
       " '意味',\n",
       " '時代',\n",
       " '・',\n",
       " '／',\n",
       " '氏',\n",
       " 'やっ',\n",
       " '通り',\n",
       " '身',\n",
       " 'こう',\n",
       " '若い',\n",
       " 'もっ',\n",
       " 'なか',\n",
       " 'とも',\n",
       " '＼',\n",
       " 'け',\n",
       " '』',\n",
       " '『',\n",
       " '芸術',\n",
       " '自身',\n",
       " '言っ',\n",
       " '作品',\n",
       " '気持',\n",
       " '人々',\n",
       " 'さえ',\n",
       " 'のに',\n",
       " '父',\n",
       " '思わ',\n",
       " 'っ',\n",
       " '主人',\n",
       " '音',\n",
       " '得',\n",
       " '或',\n",
       " 'どんな',\n",
       " 'によって',\n",
       " '手紙',\n",
       " '場合',\n",
       " 'まま',\n",
       " 'いろいろ',\n",
       " 'けれど',\n",
       " 'かい',\n",
       " '持っ',\n",
       " '性',\n",
       " 'すぐ',\n",
       " '筈',\n",
       " '下さい',\n",
       " 'まつ',\n",
       " 'せる',\n",
       " 'なんか',\n",
       " '酒',\n",
       " 'よい',\n",
       " 'ゞ',\n",
       " '婦人',\n",
       " 'やつ',\n",
       " '然し',\n",
       " '時間',\n",
       " 'みんな',\n",
       " '氣',\n",
       " 'あと',\n",
       " 'やる',\n",
       " '胸',\n",
       " '道',\n",
       " '花',\n",
       " '全く',\n",
       " '色',\n",
       " '水',\n",
       " '違',\n",
       " '名',\n",
       " 'ど',\n",
       " 'ゆく',\n",
       " 'いつも',\n",
       " '自然',\n",
       " '主義',\n",
       " '千',\n",
       " '世界',\n",
       " '必要',\n",
       " 'やはり',\n",
       " 'しよ',\n",
       " '立っ',\n",
       " '心持',\n",
       " 'いけ',\n",
       " 'なん',\n",
       " 'たる',\n",
       " 'たく',\n",
       " '次',\n",
       " '思ふ',\n",
       " '小',\n",
       " 'すれ',\n",
       " 'ご',\n",
       " '等',\n",
       " 'しか',\n",
       " '東京',\n",
       " 'ツ',\n",
       " '空',\n",
       " 'はじめ',\n",
       " 'やら',\n",
       " '点',\n",
       " 'つたの',\n",
       " 'なし',\n",
       " '文化',\n",
       " '大きな',\n",
       " '決して',\n",
       " '様子',\n",
       " '眺め',\n",
       " 'ちゃん',\n",
       " 'それから',\n",
       " '数',\n",
       " '妙',\n",
       " 'ましょ',\n",
       " '生き',\n",
       " '他',\n",
       " '無い',\n",
       " 'くる',\n",
       " '窓',\n",
       " '死ん',\n",
       " '足',\n",
       " 'ア',\n",
       " '朝',\n",
       " 'ッ',\n",
       " '知っ',\n",
       " 'だって',\n",
       " 'もつ',\n",
       " 'ほか',\n",
       " '笑',\n",
       " 'みる',\n",
       " 'あげ',\n",
       " '面',\n",
       " '一緒',\n",
       " '入れ',\n",
       " '歩い',\n",
       " 'ああ',\n",
       " '相手',\n",
       " '昔',\n",
       " '行き',\n",
       " '山',\n",
       " 'かつ',\n",
       " '精神',\n",
       " '現実',\n",
       " '忘れ',\n",
       " 'ぜ',\n",
       " '店',\n",
       " '持つ',\n",
       " '晩',\n",
       " 'に対して',\n",
       " '村',\n",
       " '会',\n",
       " 'のみ',\n",
       " '不思議',\n",
       " 'らしく',\n",
       " 'げ',\n",
       " '多く',\n",
       " 'つき',\n",
       " 'ろ',\n",
       " 'ぞ',\n",
       " '万',\n",
       " '新',\n",
       " 'きり',\n",
       " '分',\n",
       " '彼等',\n",
       " '関係',\n",
       " '円',\n",
       " '最も',\n",
       " 'こそ',\n",
       " '合',\n",
       " '室',\n",
       " 'んで',\n",
       " 'ちや',\n",
       " '光',\n",
       " 'あたし',\n",
       " 'いま',\n",
       " '急',\n",
       " 'すぎ',\n",
       " '変',\n",
       " '夢',\n",
       " 'かう',\n",
       " '耳',\n",
       " 'さま',\n",
       " 'やがて',\n",
       " 'なんて',\n",
       " 'つもり',\n",
       " '国',\n",
       " '考へ',\n",
       " '枚',\n",
       " 'まあ',\n",
       " 'もん',\n",
       " 'うし',\n",
       " 'おり',\n",
       " '以上',\n",
       " 'おれ',\n",
       " 'わから',\n",
       " 'おい',\n",
       " '階',\n",
       " '学校',\n",
       " 'ほう',\n",
       " '話し',\n",
       " '聞い',\n",
       " '妻',\n",
       " '入',\n",
       " 'かた',\n",
       " 'すると',\n",
       " 'あまり',\n",
       " 'あたり',\n",
       " '感情',\n",
       " '見える',\n",
       " 'どうして',\n",
       " '事実',\n",
       " '見せ',\n",
       " '言う',\n",
       " '二つ',\n",
       " 'すべて',\n",
       " 'あら',\n",
       " 'やり',\n",
       " '歴史',\n",
       " '新しい',\n",
       " 'どうも',\n",
       " '〔',\n",
       " '云い',\n",
       " '皆',\n",
       " '新聞',\n",
       " '実に',\n",
       " '知',\n",
       " '〕',\n",
       " '小さい',\n",
       " '涙',\n",
       " 'なさい',\n",
       " 'にとって',\n",
       " 'に対する',\n",
       " 'こういう',\n",
       " '非常',\n",
       " 'つか',\n",
       " '入っ',\n",
       " '程',\n",
       " '始め',\n",
       " '首',\n",
       " '着',\n",
       " '早く',\n",
       " '形',\n",
       " '体',\n",
       " 'いつ',\n",
       " '顏',\n",
       " '門',\n",
       " '悪い',\n",
       " '木',\n",
       " 'ひとり',\n",
       " '殺し',\n",
       " 'ふと',\n",
       " 'しまう',\n",
       " '戦争',\n",
       " '同時に',\n",
       " 'つと',\n",
       " 'なれ',\n",
       " 'きょう',\n",
       " 'なに',\n",
       " '調子',\n",
       " 'だから',\n",
       " '美しい',\n",
       " 'つまり',\n",
       " '一番',\n",
       " '出る',\n",
       " 'おも',\n",
       " '兄',\n",
       " '思は',\n",
       " '分ら',\n",
       " '位',\n",
       " 'かしら',\n",
       " 'まるで',\n",
       " '今度',\n",
       " '立て',\n",
       " '之',\n",
       " '於',\n",
       " 'つく',\n",
       " '青年',\n",
       " '結婚',\n",
       " '内',\n",
       " '面白い',\n",
       " 'だい',\n",
       " '奴',\n",
       " '長い',\n",
       " '自由',\n",
       " '実際',\n",
       " '庭',\n",
       " '別',\n",
       " '帰っ',\n",
       " '馬鹿',\n",
       " 'はれ',\n",
       " 'はっきり',\n",
       " 'きい',\n",
       " '左',\n",
       " 'はず',\n",
       " '覚え',\n",
       " '歳',\n",
       " '出来る',\n",
       " '似',\n",
       " '云わ',\n",
       " 'いくら',\n",
       " 'しろ',\n",
       " '作者',\n",
       " '海',\n",
       " '深い',\n",
       " '行か',\n",
       " 'ぐらい',\n",
       " '無',\n",
       " '月',\n",
       " '石',\n",
       " '夫',\n",
       " '読ん',\n",
       " 'せい',\n",
       " '斯',\n",
       " '樣',\n",
       " '大きい',\n",
       " 'ところが',\n",
       " '立つ',\n",
       " '例',\n",
       " '相',\n",
       " '女房',\n",
       " 'いえ',\n",
       " '火',\n",
       " 'もっと',\n",
       " 'なかなか',\n",
       " 'たつ',\n",
       " '夫人',\n",
       " '衞',\n",
       " '生れ',\n",
       " '置い',\n",
       " '一寸',\n",
       " '落ち',\n",
       " '最後',\n",
       " 'ひどく',\n",
       " 'それでも',\n",
       " 'しまい',\n",
       " 'かん',\n",
       " '好き',\n",
       " '馬',\n",
       " '變',\n",
       " '吉',\n",
       " '過ぎ',\n",
       " 'へる',\n",
       " '向',\n",
       " '事件',\n",
       " 'ざる',\n",
       " '腹',\n",
       " '助',\n",
       " '底',\n",
       " '神',\n",
       " '雨',\n",
       " 'しれ',\n",
       " '右',\n",
       " 'そ',\n",
       " 'できる',\n",
       " '雪',\n",
       " '当時',\n",
       " '太郎',\n",
       " 'とり',\n",
       " '人物',\n",
       " '得る',\n",
       " '肩',\n",
       " '心配',\n",
       " '人生',\n",
       " 'もと',\n",
       " '場',\n",
       " '大変',\n",
       " 'くれる',\n",
       " 'なぜ',\n",
       " '科学',\n",
       " '静か',\n",
       " '飛ん',\n",
       " '死',\n",
       " 'これから',\n",
       " 'わかり',\n",
       " 'われわれ',\n",
       " '書き',\n",
       " '受け',\n",
       " '研究',\n",
       " 'つつ',\n",
       " '運動',\n",
       " 'かかっ',\n",
       " '裏',\n",
       " '代',\n",
       " '腕',\n",
       " '態度',\n",
       " 'だし',\n",
       " '芝居',\n",
       " '時々',\n",
       " '言い',\n",
       " '出す',\n",
       " 'くらい',\n",
       " '側',\n",
       " '角',\n",
       " '美',\n",
       " 'エ',\n",
       " 'どんなに',\n",
       " 'ちゃ',\n",
       " '奥',\n",
       " '舞台',\n",
       " 'しかも',\n",
       " '多い',\n",
       " '犬',\n",
       " 'ほ',\n",
       " 'において',\n",
       " '葉',\n",
       " 'あらう',\n",
       " '半',\n",
       " 'みたい',\n",
       " '夏',\n",
       " '存在',\n",
       " '素子',\n",
       " '當',\n",
       " '或は',\n",
       " '鼻',\n",
       " '映画',\n",
       " '血',\n",
       " '笑い',\n",
       " '労働',\n",
       " 'あらゆる',\n",
       " '取',\n",
       " 'あんな',\n",
       " '白い',\n",
       " '船',\n",
       " '理解',\n",
       " '毎日',\n",
       " '本当に',\n",
       " '日本人',\n",
       " '雑誌',\n",
       " 'それで',\n",
       " 'こん',\n",
       " '演劇',\n",
       " '何処',\n",
       " '女中',\n",
       " '興味',\n",
       " '一方',\n",
       " '申し',\n",
       " 'すっかり',\n",
       " '歌',\n",
       " '少年',\n",
       " '種',\n",
       " '腰',\n",
       " '如く',\n",
       " '勿論',\n",
       " 'もし',\n",
       " '現代',\n",
       " '暮し',\n",
       " '白',\n",
       " '為',\n",
       " '笑っ',\n",
       " '帰',\n",
       " 'どの',\n",
       " '春',\n",
       " '表現',\n",
       " 'そうして',\n",
       " '共',\n",
       " '無理',\n",
       " 'べ',\n",
       " 'よし',\n",
       " '〇',\n",
       " '横',\n",
       " '不',\n",
       " '結果',\n",
       " '影',\n",
       " '好い',\n",
       " '俳優',\n",
       " 'むしろ',\n",
       " '赤',\n",
       " '女性',\n",
       " '妹',\n",
       " 'それら',\n",
       " '書く',\n",
       " '殺さ',\n",
       " '品',\n",
       " '場所',\n",
       " 'そこで',\n",
       " 'はい',\n",
       " '想像',\n",
       " '紙',\n",
       " 'しょ',\n",
       " 'たろ',\n",
       " '昨夜',\n",
       " '写真',\n",
       " 'いたし',\n",
       " '信じ',\n",
       " '立派',\n",
       " 'そんなに',\n",
       " '答え',\n",
       " '返事',\n",
       " '現在',\n",
       " '政治',\n",
       " '歩',\n",
       " 'たま',\n",
       " '明日',\n",
       " '更に',\n",
       " '愛',\n",
       " '高い',\n",
       " '着物',\n",
       " '流れ',\n",
       " '兵',\n",
       " '黙っ',\n",
       " '劇',\n",
       " '此',\n",
       " '良い',\n",
       " 'ぶん',\n",
       " '云え',\n",
       " '戯曲',\n",
       " '説明',\n",
       " '歩き',\n",
       " 'すこし',\n",
       " 'かく',\n",
       " '化',\n",
       " 'とにかく',\n",
       " 'ソヴェト',\n",
       " '代り',\n",
       " '母親',\n",
       " '立',\n",
       " 'じっと',\n",
       " '絵',\n",
       " 'きっと',\n",
       " '地',\n",
       " '向っ',\n",
       " 'ころ',\n",
       " 'どうしても',\n",
       " '幾',\n",
       " 'ぼんやり',\n",
       " 'ゆ',\n",
       " '親',\n",
       " '江戸',\n",
       " '頬',\n",
       " '姉',\n",
       " '与え',\n",
       " 'そのもの',\n",
       " 'わかる',\n",
       " '理由',\n",
       " '実は',\n",
       " '林',\n",
       " 'ゆき',\n",
       " '引',\n",
       " '上げ',\n",
       " 'だの',\n",
       " 'ちょっと',\n",
       " 'しばらく',\n",
       " '通',\n",
       " '幸福',\n",
       " '家庭',\n",
       " '丁度',\n",
       " 'くら',\n",
       " '用',\n",
       " '不安',\n",
       " 'そのまま',\n",
       " '世間',\n",
       " 'ども',\n",
       " '音楽',\n",
       " '字',\n",
       " 'こちら',\n",
       " 'らし',\n",
       " 'しい',\n",
       " '病気',\n",
       " '樣子',\n",
       " '両',\n",
       " '息',\n",
       " 'とっ',\n",
       " '旦那',\n",
       " '方法',\n",
       " '経験',\n",
       " '込ん',\n",
       " '階級',\n",
       " '深く',\n",
       " '寝',\n",
       " '我',\n",
       " '考える',\n",
       " '向う',\n",
       " '近い',\n",
       " '思想',\n",
       " '弟',\n",
       " '野郎',\n",
       " '長',\n",
       " '発見',\n",
       " '煙草',\n",
       " '我々',\n",
       " '客',\n",
       " '注意',\n",
       " '友達',\n",
       " '批評',\n",
       " '一般',\n",
       " 'さすが',\n",
       " 'おや',\n",
       " 'しく',\n",
       " '言わ',\n",
       " 'ひどい',\n",
       " '努力',\n",
       " 'ずつ',\n",
       " '意識',\n",
       " '秋',\n",
       " '現れ',\n",
       " '次第に',\n",
       " 'しん',\n",
       " '活動',\n",
       " '机',\n",
       " '特に',\n",
       " '何だか',\n",
       " '限り',\n",
       " '生',\n",
       " '夫婦',\n",
       " '使',\n",
       " '待つ',\n",
       " '出かけ',\n",
       " 'とても',\n",
       " '気分',\n",
       " 'づ',\n",
       " '土',\n",
       " '敵',\n",
       " 'ごと',\n",
       " '離れ',\n",
       " '大事',\n",
       " '答',\n",
       " '仕方',\n",
       " '語',\n",
       " '状態',\n",
       " '相違',\n",
       " '処',\n",
       " '考',\n",
       " '当然',\n",
       " '書か',\n",
       " '立ち',\n",
       " '迄',\n",
       " '戸',\n",
       " 'うに',\n",
       " '待っ',\n",
       " '調べ',\n",
       " 'おく',\n",
       " '小さな',\n",
       " 'つれ',\n",
       " 'ところで',\n",
       " 'もち',\n",
       " 'わし',\n",
       " '老人',\n",
       " '安心',\n",
       " '命',\n",
       " '嘘',\n",
       " 'ほんとう',\n",
       " 'させ',\n",
       " '田舎',\n",
       " '感',\n",
       " '常に',\n",
       " '枝',\n",
       " 'やし',\n",
       " '帰り',\n",
       " 'ひとつ',\n",
       " '赤い',\n",
       " '追',\n",
       " '兎',\n",
       " '杯',\n",
       " '違い',\n",
       " '一種',\n",
       " '味',\n",
       " 'いきなり',\n",
       " 'がっ',\n",
       " '由',\n",
       " '近く',\n",
       " '大きく',\n",
       " '表情',\n",
       " '床',\n",
       " '元',\n",
       " 'かえ',\n",
       " '少く',\n",
       " '歸',\n",
       " 'いつか',\n",
       " 'かね',\n",
       " 'かれ',\n",
       " '上っ',\n",
       " '父親',\n",
       " '暫く',\n",
       " '乍',\n",
       " 'どうか',\n",
       " '死ぬ',\n",
       " '黒',\n",
       " 'さし',\n",
       " '国民',\n",
       " '車',\n",
       " '真',\n",
       " '突然',\n",
       " '錢形',\n",
       " '遠く',\n",
       " 'あけ',\n",
       " 'おお',\n",
       " '相当',\n",
       " 'プロレタリア',\n",
       " '自ら',\n",
       " 'どういう',\n",
       " '驚い',\n",
       " '知り',\n",
       " '遊び',\n",
       " '多',\n",
       " '近頃',\n",
       " '勉強',\n",
       " '作',\n",
       " 'やっぱり',\n",
       " '膝',\n",
       " '名前',\n",
       " 'お母さん',\n",
       " '空気',\n",
       " 'あんまり',\n",
       " '強い',\n",
       " '午後',\n",
       " '示し',\n",
       " '降り',\n",
       " '事情',\n",
       " '電車',\n",
       " '指',\n",
       " '文章',\n",
       " '振り',\n",
       " '肉体',\n",
       " '高',\n",
       " '貰',\n",
       " ...]"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(wordbody[wordbody>100].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>の</th>\n",
       "      <th>。</th>\n",
       "      <th>に</th>\n",
       "      <th>は</th>\n",
       "      <th>た</th>\n",
       "      <th>て</th>\n",
       "      <th>を</th>\n",
       "      <th>が</th>\n",
       "      <th>で</th>\n",
       "      <th>と</th>\n",
       "      <th>...</th>\n",
       "      <th>みつ子</th>\n",
       "      <th>曲る</th>\n",
       "      <th>説得</th>\n",
       "      <th>野良犬</th>\n",
       "      <th>血の気</th>\n",
       "      <th>鍛錬</th>\n",
       "      <th>ごころ</th>\n",
       "      <th>飛ば</th>\n",
       "      <th>颱風</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455741</td>\n",
       "      <td>0.323014</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.391644</td>\n",
       "      <td>0.171900</td>\n",
       "      <td>0.200348</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.219130</td>\n",
       "      <td>0.469565</td>\n",
       "      <td>0.104348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.397998</td>\n",
       "      <td>0.205155</td>\n",
       "      <td>0.515464</td>\n",
       "      <td>0.487536</td>\n",
       "      <td>0.259844</td>\n",
       "      <td>0.237526</td>\n",
       "      <td>0.386598</td>\n",
       "      <td>0.375773</td>\n",
       "      <td>0.306186</td>\n",
       "      <td>0.293814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.366221</td>\n",
       "      <td>0.230725</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.342688</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.667826</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.370006</td>\n",
       "      <td>0.351721</td>\n",
       "      <td>0.445736</td>\n",
       "      <td>0.445153</td>\n",
       "      <td>0.606848</td>\n",
       "      <td>0.285767</td>\n",
       "      <td>0.562791</td>\n",
       "      <td>0.288837</td>\n",
       "      <td>0.117209</td>\n",
       "      <td>0.181395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.204264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.455923</td>\n",
       "      <td>0.412082</td>\n",
       "      <td>0.528864</td>\n",
       "      <td>0.493165</td>\n",
       "      <td>0.693555</td>\n",
       "      <td>0.416179</td>\n",
       "      <td>0.415642</td>\n",
       "      <td>0.293631</td>\n",
       "      <td>0.189050</td>\n",
       "      <td>0.254749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>0.435613</td>\n",
       "      <td>0.212751</td>\n",
       "      <td>0.417210</td>\n",
       "      <td>0.325902</td>\n",
       "      <td>0.371143</td>\n",
       "      <td>0.330430</td>\n",
       "      <td>0.398957</td>\n",
       "      <td>0.359061</td>\n",
       "      <td>0.288657</td>\n",
       "      <td>0.383312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>0.283719</td>\n",
       "      <td>0.222758</td>\n",
       "      <td>0.302770</td>\n",
       "      <td>0.433613</td>\n",
       "      <td>0.342126</td>\n",
       "      <td>0.221158</td>\n",
       "      <td>0.342693</td>\n",
       "      <td>0.232092</td>\n",
       "      <td>0.133066</td>\n",
       "      <td>0.209742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.007775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>0.124786</td>\n",
       "      <td>0.221111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.600520</td>\n",
       "      <td>0.263580</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>0.179931</td>\n",
       "      <td>0.225051</td>\n",
       "      <td>0.262166</td>\n",
       "      <td>0.317187</td>\n",
       "      <td>0.335347</td>\n",
       "      <td>0.177656</td>\n",
       "      <td>0.222070</td>\n",
       "      <td>0.133242</td>\n",
       "      <td>0.120288</td>\n",
       "      <td>0.224126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>0.312153</td>\n",
       "      <td>0.228412</td>\n",
       "      <td>0.280946</td>\n",
       "      <td>0.436385</td>\n",
       "      <td>0.233548</td>\n",
       "      <td>0.236513</td>\n",
       "      <td>0.272302</td>\n",
       "      <td>0.258038</td>\n",
       "      <td>0.124481</td>\n",
       "      <td>0.187297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4732 rows × 12284 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             の         。         に         は         た         て         を  \\\n",
       "0     0.455741  0.323014  0.579710  0.391644  0.171900  0.200348  0.521739   \n",
       "1     0.397998  0.205155  0.515464  0.487536  0.259844  0.237526  0.386598   \n",
       "2     0.366221  0.230725  0.217391  0.342688  0.515700  0.667826  0.478261   \n",
       "3     0.370006  0.351721  0.445736  0.445153  0.606848  0.285767  0.562791   \n",
       "4     0.455923  0.412082  0.528864  0.493165  0.693555  0.416179  0.415642   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1415  0.435613  0.212751  0.417210  0.325902  0.371143  0.330430  0.398957   \n",
       "1416  0.283719  0.222758  0.302770  0.433613  0.342126  0.221158  0.342693   \n",
       "1417  0.124786  0.221111  0.444444  0.600520  0.263580  0.128000  0.266667   \n",
       "1418  0.179931  0.225051  0.262166  0.317187  0.335347  0.177656  0.222070   \n",
       "1419  0.312153  0.228412  0.280946  0.436385  0.233548  0.236513  0.272302   \n",
       "\n",
       "             が         で         と  ...  みつ子        曲る   説得  野良犬  血の気   鍛錬  \\\n",
       "0     0.219130  0.469565  0.104348  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "1     0.375773  0.306186  0.293814  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "2     0.195652  0.391304  0.173913  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "3     0.288837  0.117209  0.181395  ...  0.0  0.204264  0.0  0.0  0.0  0.0   \n",
       "4     0.293631  0.189050  0.254749  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "...        ...       ...       ...  ...  ...       ...  ...  ...  ...  ...   \n",
       "1415  0.359061  0.288657  0.383312  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "1416  0.232092  0.133066  0.209742  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "1417  0.180000  0.300000  0.400000  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "1418  0.133242  0.120288  0.224126  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "1419  0.258038  0.124481  0.187297  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "\n",
       "           ごころ   飛ば   颱風    target  \n",
       "0     0.000000  0.0  0.0  0.000000  \n",
       "1     0.000000  0.0  0.0  0.000000  \n",
       "2     0.000000  0.0  0.0  0.000000  \n",
       "3     0.000000  0.0  0.0  1.000000  \n",
       "4     0.000000  0.0  0.0  0.000000  \n",
       "...        ...  ...  ...       ...  \n",
       "1415  0.000000  0.0  0.0  0.011011  \n",
       "1416  0.000000  0.0  0.0 -0.007775  \n",
       "1417  0.000000  0.0  0.0  0.023420  \n",
       "1418  0.048321  0.0  0.0  0.003535  \n",
       "1419  0.000000  0.0  0.0 -0.002657  \n",
       "\n",
       "[4732 rows x 12284 columns]"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.concat([pd.concat([pd.DataFrame(X),pd.DataFrame(test)]),pd.concat([y,pd.Series(pred)])],axis=1)\n",
    "data.columns=list(wordbody[wordbody>100].index)+[\"target\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>の</th>\n",
       "      <th>。</th>\n",
       "      <th>に</th>\n",
       "      <th>は</th>\n",
       "      <th>た</th>\n",
       "      <th>て</th>\n",
       "      <th>を</th>\n",
       "      <th>が</th>\n",
       "      <th>で</th>\n",
       "      <th>と</th>\n",
       "      <th>...</th>\n",
       "      <th>みつ子</th>\n",
       "      <th>曲る</th>\n",
       "      <th>説得</th>\n",
       "      <th>野良犬</th>\n",
       "      <th>血の気</th>\n",
       "      <th>鍛錬</th>\n",
       "      <th>ごころ</th>\n",
       "      <th>飛ば</th>\n",
       "      <th>颱風</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455741</td>\n",
       "      <td>0.323014</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.391644</td>\n",
       "      <td>0.171900</td>\n",
       "      <td>0.200348</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.219130</td>\n",
       "      <td>0.469565</td>\n",
       "      <td>0.104348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.397998</td>\n",
       "      <td>0.205155</td>\n",
       "      <td>0.515464</td>\n",
       "      <td>0.487536</td>\n",
       "      <td>0.259844</td>\n",
       "      <td>0.237526</td>\n",
       "      <td>0.386598</td>\n",
       "      <td>0.375773</td>\n",
       "      <td>0.306186</td>\n",
       "      <td>0.293814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.366221</td>\n",
       "      <td>0.230725</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.342688</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.667826</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.370006</td>\n",
       "      <td>0.351721</td>\n",
       "      <td>0.445736</td>\n",
       "      <td>0.445153</td>\n",
       "      <td>0.606848</td>\n",
       "      <td>0.285767</td>\n",
       "      <td>0.562791</td>\n",
       "      <td>0.288837</td>\n",
       "      <td>0.117209</td>\n",
       "      <td>0.181395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.204264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.455923</td>\n",
       "      <td>0.412082</td>\n",
       "      <td>0.528864</td>\n",
       "      <td>0.493165</td>\n",
       "      <td>0.693555</td>\n",
       "      <td>0.416179</td>\n",
       "      <td>0.415642</td>\n",
       "      <td>0.293631</td>\n",
       "      <td>0.189050</td>\n",
       "      <td>0.254749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>0.435613</td>\n",
       "      <td>0.212751</td>\n",
       "      <td>0.417210</td>\n",
       "      <td>0.325902</td>\n",
       "      <td>0.371143</td>\n",
       "      <td>0.330430</td>\n",
       "      <td>0.398957</td>\n",
       "      <td>0.359061</td>\n",
       "      <td>0.288657</td>\n",
       "      <td>0.383312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>0.283719</td>\n",
       "      <td>0.222758</td>\n",
       "      <td>0.302770</td>\n",
       "      <td>0.433613</td>\n",
       "      <td>0.342126</td>\n",
       "      <td>0.221158</td>\n",
       "      <td>0.342693</td>\n",
       "      <td>0.232092</td>\n",
       "      <td>0.133066</td>\n",
       "      <td>0.209742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.007775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>0.124786</td>\n",
       "      <td>0.221111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.600520</td>\n",
       "      <td>0.263580</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>0.179931</td>\n",
       "      <td>0.225051</td>\n",
       "      <td>0.262166</td>\n",
       "      <td>0.317187</td>\n",
       "      <td>0.335347</td>\n",
       "      <td>0.177656</td>\n",
       "      <td>0.222070</td>\n",
       "      <td>0.133242</td>\n",
       "      <td>0.120288</td>\n",
       "      <td>0.224126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>0.312153</td>\n",
       "      <td>0.228412</td>\n",
       "      <td>0.280946</td>\n",
       "      <td>0.436385</td>\n",
       "      <td>0.233548</td>\n",
       "      <td>0.236513</td>\n",
       "      <td>0.272302</td>\n",
       "      <td>0.258038</td>\n",
       "      <td>0.124481</td>\n",
       "      <td>0.187297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4655 rows × 12284 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             の         。         に         は         た         て         を  \\\n",
       "0     0.455741  0.323014  0.579710  0.391644  0.171900  0.200348  0.521739   \n",
       "1     0.397998  0.205155  0.515464  0.487536  0.259844  0.237526  0.386598   \n",
       "2     0.366221  0.230725  0.217391  0.342688  0.515700  0.667826  0.478261   \n",
       "3     0.370006  0.351721  0.445736  0.445153  0.606848  0.285767  0.562791   \n",
       "4     0.455923  0.412082  0.528864  0.493165  0.693555  0.416179  0.415642   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1415  0.435613  0.212751  0.417210  0.325902  0.371143  0.330430  0.398957   \n",
       "1416  0.283719  0.222758  0.302770  0.433613  0.342126  0.221158  0.342693   \n",
       "1417  0.124786  0.221111  0.444444  0.600520  0.263580  0.128000  0.266667   \n",
       "1418  0.179931  0.225051  0.262166  0.317187  0.335347  0.177656  0.222070   \n",
       "1419  0.312153  0.228412  0.280946  0.436385  0.233548  0.236513  0.272302   \n",
       "\n",
       "             が         で         と  ...  みつ子        曲る   説得  野良犬  血の気   鍛錬  \\\n",
       "0     0.219130  0.469565  0.104348  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "1     0.375773  0.306186  0.293814  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "2     0.195652  0.391304  0.173913  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "3     0.288837  0.117209  0.181395  ...  0.0  0.204264  0.0  0.0  0.0  0.0   \n",
       "4     0.293631  0.189050  0.254749  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "...        ...       ...       ...  ...  ...       ...  ...  ...  ...  ...   \n",
       "1415  0.359061  0.288657  0.383312  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "1416  0.232092  0.133066  0.209742  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "1417  0.180000  0.300000  0.400000  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "1418  0.133242  0.120288  0.224126  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "1419  0.258038  0.124481  0.187297  ...  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "\n",
       "           ごころ   飛ば   颱風    target  \n",
       "0     0.000000  0.0  0.0  0.000000  \n",
       "1     0.000000  0.0  0.0  0.000000  \n",
       "2     0.000000  0.0  0.0  0.000000  \n",
       "3     0.000000  0.0  0.0  1.000000  \n",
       "4     0.000000  0.0  0.0  0.000000  \n",
       "...        ...  ...  ...       ...  \n",
       "1415  0.000000  0.0  0.0  0.011011  \n",
       "1416  0.000000  0.0  0.0 -0.007775  \n",
       "1417  0.000000  0.0  0.0  0.023420  \n",
       "1418  0.048321  0.0  0.0  0.003535  \n",
       "1419  0.000000  0.0  0.0 -0.002657  \n",
       "\n",
       "[4655 rows x 12284 columns]"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[(data.target>0.9)|(data.target<0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291     0.311938\n",
       "412     0.615388\n",
       "547     0.394943\n",
       "625     0.348613\n",
       "682     0.540990\n",
       "702     0.567220\n",
       "774     0.376710\n",
       "815     0.357455\n",
       "825     0.321671\n",
       "955     0.598116\n",
       "962     0.290931\n",
       "1029    0.428522\n",
       "1050    0.290529\n",
       "1116    0.609978\n",
       "1140    0.315641\n",
       "1194    0.653080\n",
       "1200    0.428232\n",
       "1281    0.442822\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[(data.target<0.7)&(data.target>0.29)].target\n",
    "#291 0\n",
    "#412 1\n",
    "#547 1 0.39\n",
    "#625 1 0.34\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>writing_id</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>998</td>\n",
       "      <td>一私が生れて始めて蓄音器と云うものを見聞いたのはもう十四五年前父が英国から土産に買って来たも...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1379</td>\n",
       "      <td>昔々バグダツドのマホメツト教のお寺の前に一人の乞食が寝て居りました。丁度その時説教がすんだの...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>1862</td>\n",
       "      <td>或る一つの作品を書かうと思つてそれが色々の径路を辿《たど》つてから出来上がる場合と直ぐ初めの...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>2137</td>\n",
       "      <td>菊池は生き方が何時も徹底している。中途半端のところにこだわっていない。彼自身の正しいと思うと...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>2321</td>\n",
       "      <td>十年前は阿佐ヶ谷に住んで居りやはり目下と同様吶々と小説ばかり書いて居りました。もう十年も経つ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>2385</td>\n",
       "      <td>すべて背景を用いない。宦官《かんがん》が二人話しながら出て来る。――今月も生み月になっている...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>2649</td>\n",
       "      <td>ナポレオンの遺書――セント・ヘレナの島で臨終より三週間ほど前に彼が自ら口述し浄書したもので現...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>2791</td>\n",
       "      <td>僕はこれからも今月のと同じような材料を使って創作するつもりである。あれを単なる歴史小説の仲間...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>2828</td>\n",
       "      <td>一崇拝する偉人一現存の人の裡世界第一の偉人と思惟する人一右理由一プラトーシェイキスピアゲーテ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>3204</td>\n",
       "      <td>一ロマンスの中の女性は善悪共皆好み候。二あゝ云ふ女性は到底この世の中にゐないからに候。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>3214</td>\n",
       "      <td>国民文芸会が昨年度の演劇賞金を土方与志君に贈つたことは正に当を得た措置である。土方君は高の知...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>3428</td>\n",
       "      <td>赤沢雑木の暗い林を出ると案内者がここが赤沢《あかざわ》ですと言った。暑さと疲れとで目のくらみ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>3506</td>\n",
       "      <td>海《うみ》海《うみ》黒《くろ》い黒《くろ》い旗《はた》のように黒《くろ》い海《うみ》海《うみ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>3720</td>\n",
       "      <td>わたしの作品がロシア語に飜譯されると云ふことは勿論甚だ愉快です。近代の外國文藝中ロシア文藝ほ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>3789</td>\n",
       "      <td>変化の激しい都会僕に東京の印象を話せといふのは無理である。何故といへば或る印象を得るためには...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>3965</td>\n",
       "      <td>七八年｜前《ぜん》のことです。加賀《かが》でしたか能登《のと》でしたかなんでも北国の方の同人...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>3987</td>\n",
       "      <td>岸田國士加宮貴一君「光明の文学の序曲」を拝見しました。君が中村武羅夫氏に対して云つてをられる...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>4224</td>\n",
       "      <td>寺院に特殊な生活があるとすれば禁欲生活より外にはないと思われます。しかし一般人間に即した生活...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      writing_id                                               body\n",
       "291          998  一私が生れて始めて蓄音器と云うものを見聞いたのはもう十四五年前父が英国から土産に買って来たも...\n",
       "412         1379  昔々バグダツドのマホメツト教のお寺の前に一人の乞食が寝て居りました。丁度その時説教がすんだの...\n",
       "547         1862  或る一つの作品を書かうと思つてそれが色々の径路を辿《たど》つてから出来上がる場合と直ぐ初めの...\n",
       "625         2137  菊池は生き方が何時も徹底している。中途半端のところにこだわっていない。彼自身の正しいと思うと...\n",
       "682         2321  十年前は阿佐ヶ谷に住んで居りやはり目下と同様吶々と小説ばかり書いて居りました。もう十年も経つ...\n",
       "702         2385  すべて背景を用いない。宦官《かんがん》が二人話しながら出て来る。――今月も生み月になっている...\n",
       "774         2649  ナポレオンの遺書――セント・ヘレナの島で臨終より三週間ほど前に彼が自ら口述し浄書したもので現...\n",
       "815         2791  僕はこれからも今月のと同じような材料を使って創作するつもりである。あれを単なる歴史小説の仲間...\n",
       "825         2828  一崇拝する偉人一現存の人の裡世界第一の偉人と思惟する人一右理由一プラトーシェイキスピアゲーテ...\n",
       "955         3204        一ロマンスの中の女性は善悪共皆好み候。二あゝ云ふ女性は到底この世の中にゐないからに候。\n",
       "962         3214  国民文芸会が昨年度の演劇賞金を土方与志君に贈つたことは正に当を得た措置である。土方君は高の知...\n",
       "1029        3428  赤沢雑木の暗い林を出ると案内者がここが赤沢《あかざわ》ですと言った。暑さと疲れとで目のくらみ...\n",
       "1050        3506  海《うみ》海《うみ》黒《くろ》い黒《くろ》い旗《はた》のように黒《くろ》い海《うみ》海《うみ...\n",
       "1116        3720  わたしの作品がロシア語に飜譯されると云ふことは勿論甚だ愉快です。近代の外國文藝中ロシア文藝ほ...\n",
       "1140        3789  変化の激しい都会僕に東京の印象を話せといふのは無理である。何故といへば或る印象を得るためには...\n",
       "1194        3965  七八年｜前《ぜん》のことです。加賀《かが》でしたか能登《のと》でしたかなんでも北国の方の同人...\n",
       "1200        3987  岸田國士加宮貴一君「光明の文学の序曲」を拝見しました。君が中村武羅夫氏に対して云つてをられる...\n",
       "1281        4224  寺院に特殊な生活があるとすれば禁欲生活より外にはないと思われます。しかし一般人間に即した生活..."
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf.ix[data[(data.target<0.7)&(data.target>0.29)].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19      0.661900\n",
       "412     0.598179\n",
       "432     0.293549\n",
       "521     0.695174\n",
       "547     0.384147\n",
       "625     0.395439\n",
       "702     0.550592\n",
       "774     0.406029\n",
       "955     0.542888\n",
       "1029    0.429942\n",
       "1082    0.327872\n",
       "1116    0.397879\n",
       "1194    0.493948\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[(data.target<0.7)&(data.target>0.29)].target\n",
    "#19 1\n",
    "#412 1\n",
    "#432 0\n",
    "#521 1\n",
    "#547 1 0.38○\n",
    "#625 1 \n",
    "#702 1\n",
    "#774 0 0.406029✖︎\n",
    "#955 1\n",
    "#1029 1\n",
    "#1082 0\n",
    "#1116 0\n",
    "#1194 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>writing_id</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>57</td>\n",
       "      <td>「貴君《あなた》の作品の中《うち》で愛着を持つてゐらつしやるものか好きなものはありませんか」...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1379</td>\n",
       "      <td>※昔々バグダツドのマホメツト教のお寺の前に一人の乞食が寝て居りました。丁度その時説教がすんだ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>1454</td>\n",
       "      <td>所收――「猿面冠者」「ダス・ゲマイネ」「二十世紀旗手」「新ハムレツト」このたびの選集には大戰...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>1794</td>\n",
       "      <td>問《とひ》現代の作家に就いて比較上の問題ですが東洋種と西洋種とに区別したら如何《いかが》なも...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>1862</td>\n",
       "      <td>或る一つの作品を書かうと思つてそれが色々の径路を辿《たど》つてから出来上がる場合と直ぐ初めの...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>2137</td>\n",
       "      <td>菊池は生き方が何時も徹底している。中途半端のところにこだわっていない。彼自身の正しいと思うと...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>2385</td>\n",
       "      <td>×すべて背景を用いない。宦官《かんがん》が二人話しながら出て来る。――今月も生み月になってい...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>2649</td>\n",
       "      <td>ナポレオンの遺書――セント・ヘレナの島で臨終より三週間ほど前に彼が自ら口述し浄書したもので現...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>3204</td>\n",
       "      <td>一ロマンスの中の女性は善悪共皆好み候。二あゝ云ふ女性は到底この世の中にゐないからに候。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>3428</td>\n",
       "      <td>赤沢雑木の暗い林を出ると案内者がここが赤沢《あかざわ》ですと言った。暑さと疲れとで目のくらみ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>3615</td>\n",
       "      <td>長二《ちょうじ》は貧乏《びんぼう》の家《いえ》に生《う》まれておもちゃも持《も》たずに死《し...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>3720</td>\n",
       "      <td>わたしの作品がロシア語に飜譯されると云ふことは勿論甚だ愉快です。近代の外國文藝中ロシア文藝ほ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>3965</td>\n",
       "      <td>七八年｜前《ぜん》のことです。加賀《かが》でしたか能登《のと》でしたかなんでも北国の方の同人...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      writing_id                                               body\n",
       "19            57  「貴君《あなた》の作品の中《うち》で愛着を持つてゐらつしやるものか好きなものはありませんか」...\n",
       "412         1379  ※昔々バグダツドのマホメツト教のお寺の前に一人の乞食が寝て居りました。丁度その時説教がすんだ...\n",
       "432         1454  所收――「猿面冠者」「ダス・ゲマイネ」「二十世紀旗手」「新ハムレツト」このたびの選集には大戰...\n",
       "521         1794  問《とひ》現代の作家に就いて比較上の問題ですが東洋種と西洋種とに区別したら如何《いかが》なも...\n",
       "547         1862  或る一つの作品を書かうと思つてそれが色々の径路を辿《たど》つてから出来上がる場合と直ぐ初めの...\n",
       "625         2137  菊池は生き方が何時も徹底している。中途半端のところにこだわっていない。彼自身の正しいと思うと...\n",
       "702         2385  ×すべて背景を用いない。宦官《かんがん》が二人話しながら出て来る。――今月も生み月になってい...\n",
       "774         2649  ナポレオンの遺書――セント・ヘレナの島で臨終より三週間ほど前に彼が自ら口述し浄書したもので現...\n",
       "955         3204        一ロマンスの中の女性は善悪共皆好み候。二あゝ云ふ女性は到底この世の中にゐないからに候。\n",
       "1029        3428  赤沢雑木の暗い林を出ると案内者がここが赤沢《あかざわ》ですと言った。暑さと疲れとで目のくらみ...\n",
       "1082        3615  長二《ちょうじ》は貧乏《びんぼう》の家《いえ》に生《う》まれておもちゃも持《も》たずに死《し...\n",
       "1116        3720  わたしの作品がロシア語に飜譯されると云ふことは勿論甚だ愉快です。近代の外國文藝中ロシア文藝ほ...\n",
       "1194        3965  七八年｜前《ぜん》のことです。加賀《かが》でしたか能登《のと》でしたかなんでも北国の方の同人..."
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf.ix[data[(data.target<0.7)&(data.target>0.29)].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85      0.298738\n",
       "291     0.350676\n",
       "345     0.683154\n",
       "412     0.487139\n",
       "506     0.687995\n",
       "521     0.656661\n",
       "547     0.401651\n",
       "570     0.311457\n",
       "625     0.436175\n",
       "682     0.301153\n",
       "699     0.654944\n",
       "702     0.497013\n",
       "774     0.399603\n",
       "815     0.297874\n",
       "955     0.534397\n",
       "1029    0.436192\n",
       "1082    0.296646\n",
       "1106    0.296893\n",
       "1116    0.550350\n",
       "1140    0.395612\n",
       "1194    0.575942\n",
       "1281    0.368825\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[(data.target<0.7)&(data.target>0.29)].target\n",
    "#0 85\n",
    "#0 291\n",
    "#1 345\n",
    "#1 412 ○0.487139\n",
    "#1 506\n",
    "#1 521 \n",
    "#1 547 ○0.401651\n",
    "#0 570\n",
    "#1 625 ○0.436175\n",
    "#0 682\n",
    "#1 699\n",
    "#1 702 ○0.497013\n",
    "#0 774 ✖︎0.399603\n",
    "#1 815 ○0.297874\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>writing_id</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>277</td>\n",
       "      <td>虫の中でも人間に評判のよくないものの随一《ずいいち》は蛆《うじ》である。「蛆虫めら」というの...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>998</td>\n",
       "      <td>一私が生れて始めて蓄音器と云うものを見聞いたのはもう十四五年前父が英国から土産に買って来たも...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>1159</td>\n",
       "      <td>ある機会で予《よ》は下《しも》に掲げる二つの手紙を手に入れた。一つは本年二月中旬もう一つは三...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1379</td>\n",
       "      <td>昔々バグダツドのマホメツト教のお寺の前に一人の乞食が寝て居りました。丁度その時説教がすんだの...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>1757</td>\n",
       "      <td>離れで電話をかけて皺《しわ》くちゃになったフロックの袖《そで》を気にしながら玄関へ来ると誰《...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>1794</td>\n",
       "      <td>問《とひ》現代の作家に就いて比較上の問題ですが東洋種と西洋種とに区別したら如何《いかが》なも...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>1862</td>\n",
       "      <td>或る一つの作品を書かうと思つてそれが色々の径路を辿《たど》つてから出来上がる場合と直ぐ初めの...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>1969</td>\n",
       "      <td>僅々一枚か二枚の六号どうしても書けない書けないといふ事を誇張するわけではない。書く事はいくら...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>2137</td>\n",
       "      <td>菊池は生き方が何時も徹底している。中途半端のところにこだわっていない。彼自身の正しいと思うと...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>2321</td>\n",
       "      <td>十年前は阿佐ヶ谷に住んで居りやはり目下と同様吶々と小説ばかり書いて居りました。もう十年も経つ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>2377</td>\n",
       "      <td>檢非違使に問はれたる木樵りの物語さやうでございます。あの死骸《しがい》を見《み》つけたのはわ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>2385</td>\n",
       "      <td>すべて背景を用いない。宦官《かんがん》が二人話しながら出て来る。今月も生み月になっている妃《...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>2649</td>\n",
       "      <td>ナポレオンの遺書セント・ヘレナの島で臨終より三週間ほど前に彼が自ら口述し浄書したもので現に文...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>2791</td>\n",
       "      <td>○僕はこれからも今月のと同じような材料を使って創作するつもりである。あれを単なる歴史小説の仲...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>3204</td>\n",
       "      <td>一ロマンスの中の女性は善悪共皆好み候。二あゝ云ふ女性は到底この世の中にゐないからに候。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>3428</td>\n",
       "      <td>赤沢雑木の暗い林を出ると案内者がここが赤沢《あかざわ》ですと言った。暑さと疲れとで目のくらみ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>3615</td>\n",
       "      <td>長二《ちょうじ》は貧乏《びんぼう》の家《いえ》に生《う》まれておもちゃも持《も》たずに死《し...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>3693</td>\n",
       "      <td>一霹靂一声一九二六年四月二十日水曜日の朝端しなくも東京に発表せられしロイテル電報は政治社会及...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>3720</td>\n",
       "      <td>わたしの作品がロシア語に飜譯されると云ふことは勿論甚だ愉快です。近代の外國文藝中ロシア文藝ほ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>3789</td>\n",
       "      <td>変化の激しい都会僕に東京の印象を話せといふのは無理である。何故といへば或る印象を得るためには...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>3965</td>\n",
       "      <td>七八年｜前《ぜん》のことです。加賀《かが》でしたか能登《のと》でしたかなんでも北国の方の同人...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>4224</td>\n",
       "      <td>寺院に特殊な生活があるとすれば禁欲生活より外にはないと思われます。しかし一般人間に即した生活...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      writing_id                                               body\n",
       "85           277  虫の中でも人間に評判のよくないものの随一《ずいいち》は蛆《うじ》である。「蛆虫めら」というの...\n",
       "291          998  一私が生れて始めて蓄音器と云うものを見聞いたのはもう十四五年前父が英国から土産に買って来たも...\n",
       "345         1159  ある機会で予《よ》は下《しも》に掲げる二つの手紙を手に入れた。一つは本年二月中旬もう一つは三...\n",
       "412         1379  昔々バグダツドのマホメツト教のお寺の前に一人の乞食が寝て居りました。丁度その時説教がすんだの...\n",
       "506         1757  離れで電話をかけて皺《しわ》くちゃになったフロックの袖《そで》を気にしながら玄関へ来ると誰《...\n",
       "521         1794  問《とひ》現代の作家に就いて比較上の問題ですが東洋種と西洋種とに区別したら如何《いかが》なも...\n",
       "547         1862  或る一つの作品を書かうと思つてそれが色々の径路を辿《たど》つてから出来上がる場合と直ぐ初めの...\n",
       "570         1969  僅々一枚か二枚の六号どうしても書けない書けないといふ事を誇張するわけではない。書く事はいくら...\n",
       "625         2137  菊池は生き方が何時も徹底している。中途半端のところにこだわっていない。彼自身の正しいと思うと...\n",
       "682         2321  十年前は阿佐ヶ谷に住んで居りやはり目下と同様吶々と小説ばかり書いて居りました。もう十年も経つ...\n",
       "699         2377  檢非違使に問はれたる木樵りの物語さやうでございます。あの死骸《しがい》を見《み》つけたのはわ...\n",
       "702         2385  すべて背景を用いない。宦官《かんがん》が二人話しながら出て来る。今月も生み月になっている妃《...\n",
       "774         2649  ナポレオンの遺書セント・ヘレナの島で臨終より三週間ほど前に彼が自ら口述し浄書したもので現に文...\n",
       "815         2791  ○僕はこれからも今月のと同じような材料を使って創作するつもりである。あれを単なる歴史小説の仲...\n",
       "955         3204        一ロマンスの中の女性は善悪共皆好み候。二あゝ云ふ女性は到底この世の中にゐないからに候。\n",
       "1029        3428  赤沢雑木の暗い林を出ると案内者がここが赤沢《あかざわ》ですと言った。暑さと疲れとで目のくらみ...\n",
       "1082        3615  長二《ちょうじ》は貧乏《びんぼう》の家《いえ》に生《う》まれておもちゃも持《も》たずに死《し...\n",
       "1106        3693  一霹靂一声一九二六年四月二十日水曜日の朝端しなくも東京に発表せられしロイテル電報は政治社会及...\n",
       "1116        3720  わたしの作品がロシア語に飜譯されると云ふことは勿論甚だ愉快です。近代の外國文藝中ロシア文藝ほ...\n",
       "1140        3789  変化の激しい都会僕に東京の印象を話せといふのは無理である。何故といへば或る印象を得るためには...\n",
       "1194        3965  七八年｜前《ぜん》のことです。加賀《かが》でしたか能登《のと》でしたかなんでも北国の方の同人...\n",
       "1281        4224  寺院に特殊な生活があるとすれば禁欲生活より外にはないと思われます。しかし一般人間に即した生活..."
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf.ix[data[(data.target<0.7)&(data.target>0.29)].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'○僕はこれからも今月のと同じような材料を使って創作するつもりである。あれを単なる歴史小説の仲間入をさせられてはたまらない。もちろん今のがたいしたものだとは思わないが。そのうちにもう少しどうにかできるだろう。（新思潮創刊号）○酒虫《しゅちゅう》は材料を聊斎志異《りょうさいしい》からとった。原《もと》の話とほとんど変わったところはない。（新思潮第四号）○酒虫は「しゅちゅう」で「さかむし」ではない。気になるから書き加える。（新思潮第六号）○僕は新小説の九月号に「芋粥《いもがゆ》」という小説を書いた。○まだあき地があるそうだからもう少し書く。松岡の手紙によると新思潮は新潟《にいがた》県にまじめな読者をかなり持っているそうだ。そうしてその人たちの中には創作に志している青年も多いそうだ。ひとり新思潮のためのみならず日本のためにもそういう人たちの多くなることを祈りたい。もし同人のうぬぼれが単にうぬぼれにとどまらない以上は。○僕の書くものを小さくまとまりすぎていると言うて非難する人がある。しかし僕は小さくとも完成品を作りたいと思っている。芸術の境に未成品はない。大いなる完成品に至る途《みち》は小なる完成品あるのみである。流行の大なる未成品のごときは僕にとってなんらの意味もない。（以上新思潮第七号）○「煙草《たばこ》」の材料は昔高木さんの比較神話学を読んだ時に見た話を少し変えて使った。どこの伝説だかその本にも書いてなかったように思う。○新小説へ書いた「煙管《きせる》」の材料も加州藩の古老に聞いた話をやはり少し変えて使った。前に出した「虱《しらみ》」とこれと来月出す「明君」とは皆同じ人の集めてくれた材料である。○同人は皆非常に自信家のように思う人があるがそれは大ちがいだ。ほかの作家の書いたものに帽子をとることもずいぶんある。なんでもしっかりつかまえて書いてある人を見ると書いていることはしばらく問題外に置いてつかまえ方書き方のうまいのには敬意を表せずにはいられないことが多い。（そういう人は自然派の作家の中にもいる）傾向ばかり見て感心するよりこういう感心のしかたのほうがより合理的だと思っているから。○ほめられれば作家が必ずよろこぶと思うのは少し虫がいい。○批評家が作家に折紙をつけるばかりではない。作家も批評家へ折紙をつける。しかも作家のつける折紙のほうが論理的な部分は客観的にも正否がきめられうるから。（以上新思潮第九号）○夏目先生の逝去《せいきょ》ほど惜しいものはない。先生は過去において十二分に仕事をされた人である。が先生の逝去ほど惜しいものはない。先生はこのごろある転機の上に立っていられたようだから。すべての偉大な人のように五十歳を期としてさらに大踏歩《だいとうほ》を進められようとしていたから。○僕一身から言うとほかの人にどんな悪口を言われても先生にほめられればそれで満足だった。同時に先生を唯一の標準にすることの危険を時々は怖《おそ》れもした。○それから僕はいろんな事情に妨げられてこの正月にはちっとも働けなかった。働いた範囲においても時間が足りないので無理をしたのが多い。これは今考えても不快である。自分の良心の上からばかりでなくほかの雑誌の編輯者《へんしゅうしゃ》にさぞ迷惑をかけたろうと思うと実際いい気はしない。○これからは作ができてから遣《つか》うものなら遣ってもらうようにしたいと思う。とうからもそう思っていたがこのごろは特にその感が深い。○そうしてゆっくり腰をすえて自分の力の許す範囲で少しは大きなものにぶつかりたい。計画がないでもないがどうも失敗しそうで逡巡《しゅんじゅん》したくなる。アミエルの言ったように腕だめしに剣を揮《ふ》ってみるばかりで一度もそれを実際に使わないようなことになってはたいへんだと思う。○絶えず必然に底力強く進歩していかれた夏目先生を思うと自分のいくじないのが恥かしい。心から恥かしい。○文壇は来るべきなにものかに向かって動きつつある。亡《ほろ》ぶべき者が亡びるとともに生まるべき者は必ず生まれそうに思われる。今年は必ず何かある。何かあらずにはいられない僕らは皆小手しらべはすんだという気がしている。（以上新思潮第二年第一号）（大正五年三月大正六年一月）'"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf.ix[815].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>の</th>\n",
       "      <th>。</th>\n",
       "      <th>に</th>\n",
       "      <th>は</th>\n",
       "      <th>た</th>\n",
       "      <th>て</th>\n",
       "      <th>を</th>\n",
       "      <th>が</th>\n",
       "      <th>で</th>\n",
       "      <th>と</th>\n",
       "      <th>...</th>\n",
       "      <th>旅客</th>\n",
       "      <th>数子</th>\n",
       "      <th>俗人</th>\n",
       "      <th>野良犬</th>\n",
       "      <th>ただいま</th>\n",
       "      <th>見出せ</th>\n",
       "      <th>政権</th>\n",
       "      <th>日本海</th>\n",
       "      <th>手筈</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455741</td>\n",
       "      <td>0.321391</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.390674</td>\n",
       "      <td>0.171900</td>\n",
       "      <td>0.200348</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.219130</td>\n",
       "      <td>0.469565</td>\n",
       "      <td>0.104348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.397998</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.515464</td>\n",
       "      <td>0.486329</td>\n",
       "      <td>0.259844</td>\n",
       "      <td>0.237526</td>\n",
       "      <td>0.386598</td>\n",
       "      <td>0.375773</td>\n",
       "      <td>0.306186</td>\n",
       "      <td>0.293814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.366221</td>\n",
       "      <td>0.229565</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.341840</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.667826</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.371590</td>\n",
       "      <td>0.351452</td>\n",
       "      <td>0.447645</td>\n",
       "      <td>0.445953</td>\n",
       "      <td>0.609446</td>\n",
       "      <td>0.286991</td>\n",
       "      <td>0.565200</td>\n",
       "      <td>0.290074</td>\n",
       "      <td>0.117711</td>\n",
       "      <td>0.182172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.456433</td>\n",
       "      <td>0.410470</td>\n",
       "      <td>0.529456</td>\n",
       "      <td>0.492494</td>\n",
       "      <td>0.694331</td>\n",
       "      <td>0.416644</td>\n",
       "      <td>0.416107</td>\n",
       "      <td>0.293960</td>\n",
       "      <td>0.189262</td>\n",
       "      <td>0.255034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>0.435613</td>\n",
       "      <td>0.211682</td>\n",
       "      <td>0.417210</td>\n",
       "      <td>0.325095</td>\n",
       "      <td>0.371143</td>\n",
       "      <td>0.330430</td>\n",
       "      <td>0.398957</td>\n",
       "      <td>0.359061</td>\n",
       "      <td>0.288657</td>\n",
       "      <td>0.383312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>0.286732</td>\n",
       "      <td>0.223992</td>\n",
       "      <td>0.305985</td>\n",
       "      <td>0.437133</td>\n",
       "      <td>0.345758</td>\n",
       "      <td>0.224618</td>\n",
       "      <td>0.346332</td>\n",
       "      <td>0.234556</td>\n",
       "      <td>0.134479</td>\n",
       "      <td>0.211969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>0.124786</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.599034</td>\n",
       "      <td>0.263580</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>0.179931</td>\n",
       "      <td>0.223920</td>\n",
       "      <td>0.262166</td>\n",
       "      <td>0.316402</td>\n",
       "      <td>0.335347</td>\n",
       "      <td>0.177656</td>\n",
       "      <td>0.222070</td>\n",
       "      <td>0.133242</td>\n",
       "      <td>0.120288</td>\n",
       "      <td>0.224126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>0.315142</td>\n",
       "      <td>0.229440</td>\n",
       "      <td>0.283636</td>\n",
       "      <td>0.439473</td>\n",
       "      <td>0.234347</td>\n",
       "      <td>0.238778</td>\n",
       "      <td>0.274909</td>\n",
       "      <td>0.260509</td>\n",
       "      <td>0.125673</td>\n",
       "      <td>0.189091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4732 rows × 12283 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             の         。         に         は         た         て         を  \\\n",
       "0     0.455741  0.321391  0.579710  0.390674  0.171900  0.200348  0.521739   \n",
       "1     0.397998  0.204124  0.515464  0.486329  0.259844  0.237526  0.386598   \n",
       "2     0.366221  0.229565  0.217391  0.341840  0.515700  0.667826  0.478261   \n",
       "3     0.371590  0.351452  0.447645  0.445953  0.609446  0.286991  0.565200   \n",
       "4     0.456433  0.410470  0.529456  0.492494  0.694331  0.416644  0.416107   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1415  0.435613  0.211682  0.417210  0.325095  0.371143  0.330430  0.398957   \n",
       "1416  0.286732  0.223992  0.305985  0.437133  0.345758  0.224618  0.346332   \n",
       "1417  0.124786  0.220000  0.444444  0.599034  0.263580  0.128000  0.266667   \n",
       "1418  0.179931  0.223920  0.262166  0.316402  0.335347  0.177656  0.222070   \n",
       "1419  0.315142  0.229440  0.283636  0.439473  0.234347  0.238778  0.274909   \n",
       "\n",
       "             が         で         と  ...   旅客   数子   俗人  野良犬  ただいま  見出せ   政権  \\\n",
       "0     0.219130  0.469565  0.104348  ...  0.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
       "1     0.375773  0.306186  0.293814  ...  0.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
       "2     0.195652  0.391304  0.173913  ...  0.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
       "3     0.290074  0.117711  0.182172  ...  0.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
       "4     0.293960  0.189262  0.255034  ...  0.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
       "...        ...       ...       ...  ...  ...  ...  ...  ...   ...  ...  ...   \n",
       "1415  0.359061  0.288657  0.383312  ...  0.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
       "1416  0.234556  0.134479  0.211969  ...  0.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
       "1417  0.180000  0.300000  0.400000  ...  0.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
       "1418  0.133242  0.120288  0.224126  ...  0.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
       "1419  0.260509  0.125673  0.189091  ...  0.0  0.0  0.0  0.0   0.0  0.0  0.0   \n",
       "\n",
       "      日本海   手筈    target  \n",
       "0     0.0  0.0  0.000000  \n",
       "1     0.0  0.0  0.000000  \n",
       "2     0.0  0.0  0.000000  \n",
       "3     0.0  0.0  1.000000  \n",
       "4     0.0  0.0  0.000000  \n",
       "...   ...  ...       ...  \n",
       "1415  0.0  0.0  0.006589  \n",
       "1416  0.0  0.0 -0.001234  \n",
       "1417  0.0  0.0  0.172746  \n",
       "1418  0.0  0.0  0.000583  \n",
       "1419  0.0  0.0  0.003642  \n",
       "\n",
       "[4732 rows x 12283 columns]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12272</th>\n",
       "      <th>12273</th>\n",
       "      <th>12274</th>\n",
       "      <th>12275</th>\n",
       "      <th>12276</th>\n",
       "      <th>12277</th>\n",
       "      <th>12278</th>\n",
       "      <th>12279</th>\n",
       "      <th>12280</th>\n",
       "      <th>12281</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455741</td>\n",
       "      <td>0.321391</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.390674</td>\n",
       "      <td>0.171900</td>\n",
       "      <td>0.200348</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.219130</td>\n",
       "      <td>0.469565</td>\n",
       "      <td>0.104348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.397998</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.515464</td>\n",
       "      <td>0.486329</td>\n",
       "      <td>0.259844</td>\n",
       "      <td>0.237526</td>\n",
       "      <td>0.386598</td>\n",
       "      <td>0.375773</td>\n",
       "      <td>0.306186</td>\n",
       "      <td>0.293814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.366221</td>\n",
       "      <td>0.229565</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.341840</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.667826</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.371590</td>\n",
       "      <td>0.351452</td>\n",
       "      <td>0.447645</td>\n",
       "      <td>0.445953</td>\n",
       "      <td>0.609446</td>\n",
       "      <td>0.286991</td>\n",
       "      <td>0.565200</td>\n",
       "      <td>0.290074</td>\n",
       "      <td>0.117711</td>\n",
       "      <td>0.182172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.456433</td>\n",
       "      <td>0.410470</td>\n",
       "      <td>0.529456</td>\n",
       "      <td>0.492494</td>\n",
       "      <td>0.694331</td>\n",
       "      <td>0.416644</td>\n",
       "      <td>0.416107</td>\n",
       "      <td>0.293960</td>\n",
       "      <td>0.189262</td>\n",
       "      <td>0.255034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>0.310063</td>\n",
       "      <td>0.237303</td>\n",
       "      <td>0.280364</td>\n",
       "      <td>0.419708</td>\n",
       "      <td>0.322389</td>\n",
       "      <td>0.256411</td>\n",
       "      <td>0.294061</td>\n",
       "      <td>0.255409</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>0.206742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>0.377505</td>\n",
       "      <td>0.337210</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.513458</td>\n",
       "      <td>0.139542</td>\n",
       "      <td>0.232336</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.278319</td>\n",
       "      <td>0.248067</td>\n",
       "      <td>0.194958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>0.390502</td>\n",
       "      <td>0.370386</td>\n",
       "      <td>0.382630</td>\n",
       "      <td>0.585300</td>\n",
       "      <td>0.352988</td>\n",
       "      <td>0.244883</td>\n",
       "      <td>0.455512</td>\n",
       "      <td>0.085272</td>\n",
       "      <td>0.485393</td>\n",
       "      <td>0.098391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>0.252842</td>\n",
       "      <td>0.214685</td>\n",
       "      <td>0.283543</td>\n",
       "      <td>0.454014</td>\n",
       "      <td>0.282503</td>\n",
       "      <td>0.235182</td>\n",
       "      <td>0.314393</td>\n",
       "      <td>0.230282</td>\n",
       "      <td>0.138414</td>\n",
       "      <td>0.230010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>0.327957</td>\n",
       "      <td>0.206673</td>\n",
       "      <td>0.385213</td>\n",
       "      <td>0.527574</td>\n",
       "      <td>0.073694</td>\n",
       "      <td>0.232619</td>\n",
       "      <td>0.290774</td>\n",
       "      <td>0.348928</td>\n",
       "      <td>0.312022</td>\n",
       "      <td>0.246039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.717614</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3312 rows × 12282 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6      \\\n",
       "0     0.455741  0.321391  0.579710  0.390674  0.171900  0.200348  0.521739   \n",
       "1     0.397998  0.204124  0.515464  0.486329  0.259844  0.237526  0.386598   \n",
       "2     0.366221  0.229565  0.217391  0.341840  0.515700  0.667826  0.478261   \n",
       "3     0.371590  0.351452  0.447645  0.445953  0.609446  0.286991  0.565200   \n",
       "4     0.456433  0.410470  0.529456  0.492494  0.694331  0.416644  0.416107   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3307  0.310063  0.237303  0.280364  0.419708  0.322389  0.256411  0.294061   \n",
       "3308  0.377505  0.337210  0.352941  0.513458  0.139542  0.232336  0.571429   \n",
       "3309  0.390502  0.370386  0.382630  0.585300  0.352988  0.244883  0.455512   \n",
       "3310  0.252842  0.214685  0.283543  0.454014  0.282503  0.235182  0.314393   \n",
       "3311  0.327957  0.206673  0.385213  0.527574  0.073694  0.232619  0.290774   \n",
       "\n",
       "         7         8         9      ...     12272  12273  12274  12275  \\\n",
       "0     0.219130  0.469565  0.104348  ...  0.000000    0.0    0.0    0.0   \n",
       "1     0.375773  0.306186  0.293814  ...  0.000000    0.0    0.0    0.0   \n",
       "2     0.195652  0.391304  0.173913  ...  0.000000    0.0    0.0    0.0   \n",
       "3     0.290074  0.117711  0.182172  ...  0.000000    0.0    0.0    0.0   \n",
       "4     0.293960  0.189262  0.255034  ...  0.000000    0.0    0.0    0.0   \n",
       "...        ...       ...       ...  ...       ...    ...    ...    ...   \n",
       "3307  0.255409  0.180289  0.206742  ...  0.000000    0.0    0.0    0.0   \n",
       "3308  0.278319  0.248067  0.194958  ...  0.000000    0.0    0.0    0.0   \n",
       "3309  0.085272  0.485393  0.098391  ...  0.000000    0.0    0.0    0.0   \n",
       "3310  0.230282  0.138414  0.230010  ...  0.000000    0.0    0.0    0.0   \n",
       "3311  0.348928  0.312022  0.246039  ...  0.717614    0.0    0.0    0.0   \n",
       "\n",
       "         12276  12277  12278  12279  12280  12281  \n",
       "0     0.000000    0.0    0.0    0.0    0.0    0.0  \n",
       "1     0.000000    0.0    0.0    0.0    0.0    0.0  \n",
       "2     0.000000    0.0    0.0    0.0    0.0    0.0  \n",
       "3     0.000000    0.0    0.0    0.0    0.0    0.0  \n",
       "4     0.000000    0.0    0.0    0.0    0.0    0.0  \n",
       "...        ...    ...    ...    ...    ...    ...  \n",
       "3307  0.053291    0.0    0.0    0.0    0.0    0.0  \n",
       "3308  0.000000    0.0    0.0    0.0    0.0    0.0  \n",
       "3309  0.000000    0.0    0.0    0.0    0.0    0.0  \n",
       "3310  0.000000    0.0    0.0    0.0    0.0    0.0  \n",
       "3311  0.000000    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[3312 rows x 12282 columns]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=pd.DataFrame(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-12-31 21:49:28,191] Finished trial#0 resulted in value: 1.0. Current best value is 1.0 with parameters: {'batch_size': 512, 'n_layers': 8, 'activation': 'relu', 'dropout_rate': 0.30712251289509057, 'learning_rate': 7.398814280028737e-07}.\n",
      "[I 2019-12-31 21:50:03,780] Finished trial#1 resulted in value: 1.0. Current best value is 1.0 with parameters: {'batch_size': 512, 'n_layers': 8, 'activation': 'relu', 'dropout_rate': 0.30712251289509057, 'learning_rate': 7.398814280028737e-07}.\n",
      "[I 2019-12-31 21:50:41,620] Finished trial#2 resulted in value: 1.0. Current best value is 1.0 with parameters: {'batch_size': 512, 'n_layers': 8, 'activation': 'relu', 'dropout_rate': 0.30712251289509057, 'learning_rate': 7.398814280028737e-07}.\n",
      "[I 2019-12-31 21:51:26,919] Finished trial#3 resulted in value: 0.9433380084151473. Current best value is 0.9433380084151473 with parameters: {'batch_size': 512, 'n_layers': 9, 'activation': 'sigmoid', 'dropout_rate': 0.0005279254841767389, 'learning_rate': 2.7358715972511846e-05}.\n",
      "[I 2019-12-31 21:52:20,963] Finished trial#4 resulted in value: 0.21945031699374717. Current best value is 0.21945031699374717 with parameters: {'batch_size': 256, 'n_layers': 10, 'activation': 'relu', 'dropout_rate': 0.31619240416598327, 'learning_rate': 0.00018368767508131277}.\n",
      "[I 2019-12-31 21:52:58,014] Finished trial#5 resulted in value: 1.0. Current best value is 0.21945031699374717 with parameters: {'batch_size': 256, 'n_layers': 10, 'activation': 'relu', 'dropout_rate': 0.31619240416598327, 'learning_rate': 0.00018368767508131277}.\n",
      "[I 2019-12-31 21:53:38,558] Finished trial#6 resulted in value: 1.0. Current best value is 0.21945031699374717 with parameters: {'batch_size': 256, 'n_layers': 10, 'activation': 'relu', 'dropout_rate': 0.31619240416598327, 'learning_rate': 0.00018368767508131277}.\n",
      "[I 2019-12-31 21:54:16,140] Finished trial#7 resulted in value: 1.0. Current best value is 0.21945031699374717 with parameters: {'batch_size': 256, 'n_layers': 10, 'activation': 'relu', 'dropout_rate': 0.31619240416598327, 'learning_rate': 0.00018368767508131277}.\n",
      "[I 2019-12-31 21:54:53,091] Finished trial#8 resulted in value: 1.0. Current best value is 0.21945031699374717 with parameters: {'batch_size': 256, 'n_layers': 10, 'activation': 'relu', 'dropout_rate': 0.31619240416598327, 'learning_rate': 0.00018368767508131277}.\n",
      "[I 2019-12-31 21:55:27,729] Finished trial#9 resulted in value: 0.3958008827808286. Current best value is 0.21945031699374717 with parameters: {'batch_size': 256, 'n_layers': 10, 'activation': 'relu', 'dropout_rate': 0.31619240416598327, 'learning_rate': 0.00018368767508131277}.\n",
      "[I 2019-12-31 21:55:56,528] Finished trial#10 resulted in value: 0.2168094828516164. Current best value is 0.2168094828516164 with parameters: {'batch_size': 256, 'n_layers': 2, 'activation': 'sigmoid', 'dropout_rate': 0.4989957435158886, 'learning_rate': 0.001952967809608517}.\n",
      "[I 2019-12-31 21:56:24,009] Finished trial#11 resulted in value: 0.1010594155309199. Current best value is 0.1010594155309199 with parameters: {'batch_size': 256, 'n_layers': 1, 'activation': 'sigmoid', 'dropout_rate': 0.4829289642167508, 'learning_rate': 0.0016942350146310677}.\n",
      "[I 2019-12-31 21:56:51,214] Finished trial#12 resulted in value: 0.048862707340968314. Current best value is 0.048862707340968314 with parameters: {'batch_size': 256, 'n_layers': 1, 'activation': 'sigmoid', 'dropout_rate': 0.4889400429698288, 'learning_rate': 0.006996420326081947}.\n",
      "[I 2019-12-31 21:57:18,665] Finished trial#13 resulted in value: 0.08645264914476436. Current best value is 0.048862707340968314 with parameters: {'batch_size': 256, 'n_layers': 1, 'activation': 'sigmoid', 'dropout_rate': 0.4889400429698288, 'learning_rate': 0.006996420326081947}.\n",
      "[I 2019-12-31 21:57:49,926] Finished trial#14 resulted in value: 0.02681197692964976. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 21:58:20,851] Finished trial#15 resulted in value: 0.032213370572793054. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 21:58:53,091] Finished trial#16 resulted in value: 0.6065497076023392. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 21:59:23,616] Finished trial#17 resulted in value: 1.0. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 21:59:55,764] Finished trial#18 resulted in value: 0.9213589555999935. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 22:00:27,095] Finished trial#19 resulted in value: 1.0. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 22:01:00,453] Finished trial#20 resulted in value: 1.0. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 22:01:38,873] Finished trial#21 resulted in value: 0.033792161135661636. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 22:02:08,517] Finished trial#22 resulted in value: 0.03502355482104069. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 22:02:39,667] Finished trial#23 resulted in value: 0.6638333333333334. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 22:03:13,341] Finished trial#24 resulted in value: 0.6143717728055077. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 22:03:43,012] Finished trial#25 resulted in value: 0.4630830039525693. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 22:04:17,487] Finished trial#26 resulted in value: 0.4162358463093342. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 22:04:47,308] Finished trial#27 resulted in value: 1.0. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 22:05:18,311] Finished trial#28 resulted in value: 0.02759263304971271. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-12-31 22:05:48,643] Finished trial#29 resulted in value: 0.9423170779551964. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 22:06:21,230] Finished trial#30 resulted in value: 0.22312443766078227. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 22:06:56,136] Finished trial#31 resulted in value: 0.04085547168090109. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 22:07:27,291] Finished trial#32 resulted in value: 0.24491828176074681. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 22:08:01,767] Finished trial#33 resulted in value: 0.6174484052532833. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 22:08:30,954] Finished trial#34 resulted in value: 1.0. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 22:09:05,773] Finished trial#35 resulted in value: 0.808695652173913. Current best value is 0.02681197692964976 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40476345765939126, 'learning_rate': 0.009600154219701668}.\n",
      "[I 2019-12-31 22:09:37,059] Finished trial#36 resulted in value: 0.02516590082925918. Current best value is 0.02516590082925918 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.29575357475741515, 'learning_rate': 0.009541021247441304}.\n",
      "[I 2019-12-31 22:10:08,818] Finished trial#37 resulted in value: 1.0. Current best value is 0.02516590082925918 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.29575357475741515, 'learning_rate': 0.009541021247441304}.\n",
      "[I 2019-12-31 22:10:40,160] Finished trial#38 resulted in value: 1.0. Current best value is 0.02516590082925918 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.29575357475741515, 'learning_rate': 0.009541021247441304}.\n",
      "[I 2019-12-31 22:11:15,575] Finished trial#39 resulted in value: 0.9422975902078287. Current best value is 0.02516590082925918 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.29575357475741515, 'learning_rate': 0.009541021247441304}.\n",
      "[I 2019-12-31 22:11:49,836] Finished trial#40 resulted in value: 0.04266789711836161. Current best value is 0.02516590082925918 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.29575357475741515, 'learning_rate': 0.009541021247441304}.\n",
      "[I 2019-12-31 22:12:19,618] Finished trial#41 resulted in value: 0.028856547537571386. Current best value is 0.02516590082925918 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.29575357475741515, 'learning_rate': 0.009541021247441304}.\n",
      "[I 2019-12-31 22:12:51,253] Finished trial#42 resulted in value: 0.023412663217541207. Current best value is 0.023412663217541207 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40737213386912147, 'learning_rate': 0.00482153720887046}.\n",
      "[I 2019-12-31 22:13:21,377] Finished trial#43 resulted in value: 0.0489501219486026. Current best value is 0.023412663217541207 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40737213386912147, 'learning_rate': 0.00482153720887046}.\n",
      "[I 2019-12-31 22:13:50,309] Finished trial#44 resulted in value: 0.9475340176653139. Current best value is 0.023412663217541207 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40737213386912147, 'learning_rate': 0.00482153720887046}.\n",
      "[I 2019-12-31 22:14:21,738] Finished trial#45 resulted in value: 1.0. Current best value is 0.023412663217541207 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40737213386912147, 'learning_rate': 0.00482153720887046}.\n",
      "[I 2019-12-31 22:14:54,438] Finished trial#46 resulted in value: 0.44700884442511235. Current best value is 0.023412663217541207 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40737213386912147, 'learning_rate': 0.00482153720887046}.\n",
      "[I 2019-12-31 22:15:24,530] Finished trial#47 resulted in value: 0.05398587099245. Current best value is 0.023412663217541207 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40737213386912147, 'learning_rate': 0.00482153720887046}.\n",
      "[I 2019-12-31 22:15:57,452] Finished trial#48 resulted in value: 0.02493145507287642. Current best value is 0.023412663217541207 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40737213386912147, 'learning_rate': 0.00482153720887046}.\n",
      "[I 2019-12-31 22:16:30,719] Finished trial#49 resulted in value: 0.03762411631319185. Current best value is 0.023412663217541207 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40737213386912147, 'learning_rate': 0.00482153720887046}.\n",
      "[I 2019-12-31 22:17:01,595] Finished trial#50 resulted in value: 0.21875379693473307. Current best value is 0.023412663217541207 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40737213386912147, 'learning_rate': 0.00482153720887046}.\n",
      "[I 2019-12-31 22:17:33,352] Finished trial#51 resulted in value: 0.040372102445994096. Current best value is 0.023412663217541207 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.40737213386912147, 'learning_rate': 0.00482153720887046}.\n",
      "[I 2019-12-31 22:18:10,803] Finished trial#52 resulted in value: 0.020739842937622988. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:18:42,254] Finished trial#53 resulted in value: 0.032420367805125694. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:19:14,761] Finished trial#54 resulted in value: 0.030936602267993663. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:19:55,093] Finished trial#55 resulted in value: 1.0. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:20:29,738] Finished trial#56 resulted in value: 0.024346244942893613. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:21:00,859] Finished trial#57 resulted in value: 0.028041627616369857. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-12-31 22:21:33,107] Finished trial#58 resulted in value: 1.0. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:22:04,369] Finished trial#59 resulted in value: 0.23197638558463296. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:22:33,304] Finished trial#60 resulted in value: 0.052335377278146966. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:23:04,845] Finished trial#61 resulted in value: 0.02375638091885235. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:23:36,219] Finished trial#62 resulted in value: 0.03763628056201096. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:24:08,989] Finished trial#63 resulted in value: 0.21574365320248812. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:24:38,696] Finished trial#64 resulted in value: 0.02965346994292306. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:25:14,853] Finished trial#65 resulted in value: 0.24110059229972414. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:25:51,073] Finished trial#66 resulted in value: 0.23871489189735295. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:26:21,829] Finished trial#67 resulted in value: 0.035174513105829086. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:26:53,940] Finished trial#68 resulted in value: 0.03638418921755304. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:27:27,265] Finished trial#69 resulted in value: 0.9160884400782076. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:27:57,576] Finished trial#70 resulted in value: 0.03247296780909381. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:28:29,799] Finished trial#71 resulted in value: 0.02200606649457071. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:29:00,476] Finished trial#72 resulted in value: 0.02598274887685581. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:29:31,006] Finished trial#73 resulted in value: 0.03309329980039499. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:30:03,304] Finished trial#74 resulted in value: 0.034636123328986246. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:30:34,798] Finished trial#75 resulted in value: 0.039336442513340564. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:31:07,120] Finished trial#76 resulted in value: 0.025222339995962018. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:31:37,459] Finished trial#77 resulted in value: 0.29721394194352835. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:32:09,354] Finished trial#78 resulted in value: 0.028970368481416875. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:32:42,417] Finished trial#79 resulted in value: 1.0. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:33:15,116] Finished trial#80 resulted in value: 1.0. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:33:48,990] Finished trial#81 resulted in value: 0.03459847094873569. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:34:28,349] Finished trial#82 resulted in value: 0.040230856359888656. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:35:01,358] Finished trial#83 resulted in value: 0.028369545452692213. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:35:33,084] Finished trial#84 resulted in value: 0.030540433504710585. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:36:06,206] Finished trial#85 resulted in value: 0.8898166153084461. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:36:36,430] Finished trial#86 resulted in value: 0.0362721423968575. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-12-31 22:37:08,062] Finished trial#87 resulted in value: 0.024448390736134806. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:37:41,519] Finished trial#88 resulted in value: 0.03186432446485765. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:38:11,658] Finished trial#89 resulted in value: 0.04057894256928163. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:38:43,469] Finished trial#90 resulted in value: 0.026117022801315448. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:39:15,506] Finished trial#91 resulted in value: 0.028500289514157062. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:39:48,140] Finished trial#92 resulted in value: 0.025637182038070527. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:40:20,044] Finished trial#93 resulted in value: 0.43336467247400134. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:40:53,533] Finished trial#94 resulted in value: 0.03366005605170952. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:41:31,884] Finished trial#95 resulted in value: 0.02771368416529696. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:42:09,240] Finished trial#96 resulted in value: 0.041275923870711884. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:42:49,793] Finished trial#97 resulted in value: 0.034876481465898834. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:43:24,478] Finished trial#98 resulted in value: 1.0. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:43:56,712] Finished trial#99 resulted in value: 0.05677106155678735. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:44:31,391] Finished trial#100 resulted in value: 0.04235778954075042. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:45:04,116] Finished trial#101 resulted in value: 0.02171556583321288. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:45:35,366] Finished trial#102 resulted in value: 0.024478729741887584. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:46:06,370] Finished trial#103 resulted in value: 0.03406966140385459. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:46:36,558] Finished trial#104 resulted in value: 0.044817353356436884. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:47:07,657] Finished trial#105 resulted in value: 0.05202499901620927. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:47:45,573] Finished trial#106 resulted in value: 0.027894839788043635. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:48:29,410] Finished trial#107 resulted in value: 1.0. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:49:02,475] Finished trial#108 resulted in value: 0.024018609488536846. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:49:33,676] Finished trial#109 resulted in value: 0.026445221445221434. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:50:05,154] Finished trial#110 resulted in value: 0.02788102169505058. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:50:36,679] Finished trial#111 resulted in value: 0.036228393149324356. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:51:07,833] Finished trial#112 resulted in value: 0.029590866765381918. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:51:39,798] Finished trial#113 resulted in value: 0.024135595587208458. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:52:12,088] Finished trial#114 resulted in value: 0.029078117217652077. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-12-31 22:52:46,947] Finished trial#115 resulted in value: 0.03933274572521983. Current best value is 0.020739842937622988 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.38677123393454105, 'learning_rate': 0.009947579683758307}.\n",
      "[I 2019-12-31 22:53:19,924] Finished trial#116 resulted in value: 0.018965040807146116. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 22:53:58,077] Finished trial#117 resulted in value: 0.027831778397214513. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 22:54:41,162] Finished trial#118 resulted in value: 1.0. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 22:55:15,279] Finished trial#119 resulted in value: 0.9464898202903841. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 22:55:45,409] Finished trial#120 resulted in value: 0.05945370366285663. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 22:56:17,918] Finished trial#121 resulted in value: 0.03198245861225213. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 22:56:50,608] Finished trial#122 resulted in value: 0.028876603567594472. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 22:57:23,072] Finished trial#123 resulted in value: 0.03918070649569694. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 22:58:00,624] Finished trial#124 resulted in value: 0.9491173814718034. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 22:58:39,199] Finished trial#125 resulted in value: 0.027681289484979388. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 22:59:13,000] Finished trial#126 resulted in value: 0.022358965967213207. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 22:59:43,722] Finished trial#127 resulted in value: 0.03928283811133826. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:00:17,811] Finished trial#128 resulted in value: 0.9697902097902098. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:00:54,038] Finished trial#129 resulted in value: 0.03222753909186815. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:01:26,278] Finished trial#130 resulted in value: 0.7494080741991727. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:01:58,496] Finished trial#131 resulted in value: 0.028407503157027447. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:02:30,931] Finished trial#132 resulted in value: 0.03416442522402252. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:03:05,375] Finished trial#133 resulted in value: 0.026175677576900647. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:03:43,392] Finished trial#134 resulted in value: 0.024929882989844443. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:04:41,751] Finished trial#135 resulted in value: 0.033923737916219054. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:05:36,993] Finished trial#136 resulted in value: 0.029508586651443736. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:06:17,036] Finished trial#137 resulted in value: 0.043938952189106595. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:06:53,373] Finished trial#138 resulted in value: 0.03843937680098719. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:07:33,351] Finished trial#139 resulted in value: 0.23886573071927764. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:08:13,310] Finished trial#140 resulted in value: 0.810989010989011. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:08:51,421] Finished trial#141 resulted in value: 0.027100818420020323. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:09:29,386] Finished trial#142 resulted in value: 0.02627013264551281. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-12-31 23:10:06,435] Finished trial#143 resulted in value: 0.027194400355302628. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:10:39,609] Finished trial#144 resulted in value: 0.02759876717485421. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:11:15,512] Finished trial#145 resulted in value: 0.0267626928203486. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:11:50,053] Finished trial#146 resulted in value: 0.023829818690816906. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:12:25,775] Finished trial#147 resulted in value: 0.02733588166516987. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:12:58,253] Finished trial#148 resulted in value: 0.03081247438951018. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:13:29,134] Finished trial#149 resulted in value: 0.03460400228311611. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:14:02,752] Finished trial#150 resulted in value: 0.025249805275998738. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:14:35,262] Finished trial#151 resulted in value: 0.020692376228475418. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:15:10,474] Finished trial#152 resulted in value: 0.03513157491652108. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:15:50,779] Finished trial#153 resulted in value: 0.030004126692246613. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:16:26,414] Finished trial#154 resulted in value: 0.022532508604614754. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:17:05,765] Finished trial#155 resulted in value: 0.036638951992851276. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:17:44,464] Finished trial#156 resulted in value: 0.021085792907719836. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:18:22,224] Finished trial#157 resulted in value: 0.020089070468671366. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:18:58,666] Finished trial#158 resulted in value: 0.028254150769961184. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:19:31,687] Finished trial#159 resulted in value: 0.02593838627974221. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:20:25,021] Finished trial#160 resulted in value: 0.0281508001927826. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:21:09,288] Finished trial#161 resulted in value: 0.026425067078002407. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:21:53,622] Finished trial#162 resulted in value: 0.028048701476551585. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:22:38,360] Finished trial#163 resulted in value: 0.03405831363278167. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:23:18,721] Finished trial#164 resulted in value: 0.029207603572997343. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:23:59,907] Finished trial#165 resulted in value: 0.03210542672864247. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:24:49,041] Finished trial#166 resulted in value: 0.027363442305219032. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:25:39,244] Finished trial#167 resulted in value: 0.026158206172902432. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:26:29,067] Finished trial#168 resulted in value: 0.019886678092046473. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:27:24,784] Finished trial#169 resulted in value: 0.03485080224086268. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:28:20,710] Finished trial#170 resulted in value: 0.22493229229710643. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-12-31 23:29:07,632] Finished trial#171 resulted in value: 0.42780441330137076. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:29:42,424] Finished trial#172 resulted in value: 0.031194659613927422. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:30:15,751] Finished trial#173 resulted in value: 0.22232333527470394. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:30:47,523] Finished trial#174 resulted in value: 0.06673334131128106. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:31:24,892] Finished trial#175 resulted in value: 0.021867698503089117. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:32:02,233] Finished trial#176 resulted in value: 0.03828585574536125. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:32:39,971] Finished trial#177 resulted in value: 0.03488287317015659. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:33:19,970] Finished trial#178 resulted in value: 0.029170368376183475. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:33:54,267] Finished trial#179 resulted in value: 0.053914801098766674. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:34:29,230] Finished trial#180 resulted in value: 1.0. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:35:03,259] Finished trial#181 resulted in value: 0.029779345975005755. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:35:45,593] Finished trial#182 resulted in value: 1.0. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:36:19,249] Finished trial#183 resulted in value: 0.026883066416573564. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:36:52,222] Finished trial#184 resulted in value: 0.024695440076730968. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:37:26,588] Finished trial#185 resulted in value: 0.02530226046657591. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:52:17,789] Finished trial#186 resulted in value: 0.0221418495785215. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:52:56,443] Finished trial#187 resulted in value: 0.023481032393901424. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:53:33,905] Finished trial#188 resulted in value: 0.02851976809298651. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:54:08,345] Finished trial#189 resulted in value: 0.022558226865721287. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:54:43,070] Finished trial#190 resulted in value: 0.022276295839436577. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:55:17,979] Finished trial#191 resulted in value: 0.02749081460945868. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:56:28,621] Finished trial#192 resulted in value: 0.02419102832736897. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:57:09,476] Finished trial#193 resulted in value: 0.03345396359737407. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:57:48,017] Finished trial#194 resulted in value: 0.023398402839396537. Current best value is 0.018965040807146116 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3465303937097647, 'learning_rate': 0.007989772476569165}.\n",
      "[I 2019-12-31 23:58:22,760] Finished trial#195 resulted in value: 0.01600410860029977. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2019-12-31 23:59:01,403] Finished trial#196 resulted in value: 0.022442845326716254. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2019-12-31 23:59:51,245] Finished trial#197 resulted in value: 0.02282716258403039. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:00:33,856] Finished trial#198 resulted in value: 0.02874748222587231. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:01:18,064] Finished trial#199 resulted in value: 0.01996559898561112. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 00:01:54,267] Finished trial#200 resulted in value: 0.024290843199642587. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:02:31,071] Finished trial#201 resulted in value: 0.024174276948202178. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:03:07,512] Finished trial#202 resulted in value: 0.030044498676574216. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:03:42,512] Finished trial#203 resulted in value: 0.02441911606248459. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:04:18,005] Finished trial#204 resulted in value: 0.030707718207718115. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:04:57,484] Finished trial#205 resulted in value: 0.029288930529039248. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:05:33,638] Finished trial#206 resulted in value: 0.02696383186705764. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:06:10,295] Finished trial#207 resulted in value: 0.02403113930224432. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:06:45,753] Finished trial#208 resulted in value: 0.02962709039286726. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:07:24,392] Finished trial#209 resulted in value: 0.028466322915764453. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:08:01,115] Finished trial#210 resulted in value: 0.02524204466035551. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:08:42,410] Finished trial#211 resulted in value: 0.021347687400319004. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:09:18,320] Finished trial#212 resulted in value: 0.026745414245976873. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:09:59,180] Finished trial#213 resulted in value: 0.035588079490518454. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:10:35,986] Finished trial#214 resulted in value: 0.02374603553668453. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:11:19,766] Finished trial#215 resulted in value: 0.02854934492819594. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:11:54,726] Finished trial#216 resulted in value: 0.03342350982586173. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:12:47,439] Finished trial#217 resulted in value: 0.0314967684866817. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:13:26,470] Finished trial#218 resulted in value: 0.028150937350498895. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:14:07,453] Finished trial#219 resulted in value: 0.02708202573263807. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:14:50,678] Finished trial#220 resulted in value: 0.23461893151722868. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:15:28,168] Finished trial#221 resulted in value: 0.026045419871865483. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:16:04,570] Finished trial#222 resulted in value: 0.0271766094189152. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:16:40,983] Finished trial#223 resulted in value: 0.033888181387798255. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:17:18,042] Finished trial#224 resulted in value: 0.027039802256890844. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:17:54,538] Finished trial#225 resulted in value: 0.029142234104985176. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:18:31,061] Finished trial#226 resulted in value: 0.03290336625594237. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:19:06,722] Finished trial#227 resulted in value: 0.0260267203804464. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 00:19:49,432] Finished trial#228 resulted in value: 0.021535702000098356. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:20:27,722] Finished trial#229 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:21:07,107] Finished trial#230 resulted in value: 0.02325344986183764. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:21:49,366] Finished trial#231 resulted in value: 0.03317629959551538. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:22:41,163] Finished trial#232 resulted in value: 0.024513244981787197. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:23:21,109] Finished trial#233 resulted in value: 0.024921479795670165. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:24:05,937] Finished trial#234 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:24:41,140] Finished trial#235 resulted in value: 0.025816012659755527. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:25:16,363] Finished trial#236 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:25:51,503] Finished trial#237 resulted in value: 0.024955122655122564. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:26:28,067] Finished trial#238 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:27:03,735] Finished trial#239 resulted in value: 0.027734151296053944. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:27:38,931] Finished trial#240 resulted in value: 0.025489040060468704. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:28:14,196] Finished trial#241 resulted in value: 0.02579173832827597. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:28:49,392] Finished trial#242 resulted in value: 0.02494606450721193. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:29:25,237] Finished trial#243 resulted in value: 0.01923084433335942. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:30:01,229] Finished trial#244 resulted in value: 0.025693999627396602. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:30:36,680] Finished trial#245 resulted in value: 0.02104834262398181. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:31:12,185] Finished trial#246 resulted in value: 0.024726083277362365. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:31:47,568] Finished trial#247 resulted in value: 0.023472840602883238. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:32:23,322] Finished trial#248 resulted in value: 0.20204179901798314. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:33:03,949] Finished trial#249 resulted in value: 0.030514797522679826. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:33:39,290] Finished trial#250 resulted in value: 0.0299993891637651. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:34:14,412] Finished trial#251 resulted in value: 0.027550514002127047. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:34:49,522] Finished trial#252 resulted in value: 0.02025034945388937. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:35:25,280] Finished trial#253 resulted in value: 0.026118585408555495. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:36:01,106] Finished trial#254 resulted in value: 0.029166298441697824. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:36:38,060] Finished trial#255 resulted in value: 0.036337226914476495. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:37:13,801] Finished trial#256 resulted in value: 0.024377205386129974. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 00:37:49,178] Finished trial#257 resulted in value: 0.0262092707721594. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:38:24,636] Finished trial#258 resulted in value: 0.026504459516227263. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:39:00,129] Finished trial#259 resulted in value: 0.02909374531350528. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:39:35,973] Finished trial#260 resulted in value: 0.022126160860677202. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:40:10,756] Finished trial#261 resulted in value: 0.03314768728574591. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:40:47,123] Finished trial#262 resulted in value: 0.025893747940765266. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:41:28,846] Finished trial#263 resulted in value: 0.4530594665300124. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:42:18,742] Finished trial#264 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:43:07,307] Finished trial#265 resulted in value: 0.026048106292904216. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:43:49,291] Finished trial#266 resulted in value: 0.030699142291658377. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:44:26,315] Finished trial#267 resulted in value: 0.024851798435088446. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:45:03,826] Finished trial#268 resulted in value: 0.03387593323006821. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:45:39,567] Finished trial#269 resulted in value: 0.027773671658207766. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:46:13,640] Finished trial#270 resulted in value: 0.030475572047000576. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:46:48,950] Finished trial#271 resulted in value: 0.35423167848699755. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:47:24,930] Finished trial#272 resulted in value: 0.023707831528785972. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:48:00,419] Finished trial#273 resulted in value: 0.028122888854490213. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:48:35,957] Finished trial#274 resulted in value: 0.027135860673981882. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:49:12,769] Finished trial#275 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:49:48,475] Finished trial#276 resulted in value: 0.2378052546757321. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:50:22,873] Finished trial#277 resulted in value: 0.03119700717646745. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:50:58,299] Finished trial#278 resulted in value: 0.02748260913398526. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:51:33,952] Finished trial#279 resulted in value: 0.9127753969756587. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:52:12,027] Finished trial#280 resulted in value: 0.029569522627381994. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:52:53,381] Finished trial#281 resulted in value: 0.026919071401567463. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:53:29,794] Finished trial#282 resulted in value: 0.029130153638573852. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:54:06,150] Finished trial#283 resulted in value: 0.024574238421625205. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:54:42,211] Finished trial#284 resulted in value: 0.02392798240791638. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:55:19,149] Finished trial#285 resulted in value: 0.023544827289103565. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 00:55:55,873] Finished trial#286 resulted in value: 0.9417819113510464. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:56:33,750] Finished trial#287 resulted in value: 0.040722071301344975. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:57:15,917] Finished trial#288 resulted in value: 0.03534162584827505. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:58:20,892] Finished trial#289 resulted in value: 0.02378694638550649. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 00:59:21,614] Finished trial#290 resulted in value: 0.030468011086822955. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:00:12,197] Finished trial#291 resulted in value: 0.03008178379375137. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:00:56,064] Finished trial#292 resulted in value: 0.023435471749675307. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:01:43,619] Finished trial#293 resulted in value: 0.03364355911886785. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:02:28,356] Finished trial#294 resulted in value: 0.0240216609402657. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:03:13,949] Finished trial#295 resulted in value: 0.02513797359777392. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:03:58,150] Finished trial#296 resulted in value: 0.021964759870924278. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:04:38,026] Finished trial#297 resulted in value: 0.02695544641910741. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:05:17,178] Finished trial#298 resulted in value: 0.9708683473389356. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:05:56,242] Finished trial#299 resulted in value: 0.04287641101372086. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:06:36,912] Finished trial#300 resulted in value: 0.035001409145353435. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:07:19,342] Finished trial#301 resulted in value: 0.03695169128131326. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:08:12,398] Finished trial#302 resulted in value: 0.028452128711472624. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:09:05,720] Finished trial#303 resulted in value: 0.6200481927710844. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:09:49,150] Finished trial#304 resulted in value: 0.025803073773517093. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:10:37,614] Finished trial#305 resulted in value: 0.03869446370647933. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:11:24,474] Finished trial#306 resulted in value: 0.031162176583860224. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:12:11,980] Finished trial#307 resulted in value: 0.022248227455561098. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:13:00,425] Finished trial#308 resulted in value: 0.0276659120695818. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:13:45,820] Finished trial#309 resulted in value: 0.018754614235584. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:14:30,952] Finished trial#310 resulted in value: 0.026385210515602386. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:15:17,826] Finished trial#311 resulted in value: 0.02669099016925114. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:16:00,612] Finished trial#312 resulted in value: 0.021062022090059473. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:16:43,863] Finished trial#313 resulted in value: 0.020362609427406198. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 01:17:25,635] Finished trial#314 resulted in value: 0.917611604417598. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:18:06,230] Finished trial#315 resulted in value: 0.02664852997327216. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:18:48,037] Finished trial#316 resulted in value: 0.0404071264141278. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:19:27,919] Finished trial#317 resulted in value: 0.027762710743561847. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:20:24,870] Finished trial#318 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:21:03,010] Finished trial#319 resulted in value: 0.4181454399941795. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:21:49,401] Finished trial#320 resulted in value: 0.02248271305859806. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:22:55,134] Finished trial#321 resulted in value: 0.03788961072926811. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:24:23,415] Finished trial#322 resulted in value: 0.024136604136604234. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:25:20,596] Finished trial#323 resulted in value: 0.02258897931169712. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:26:10,851] Finished trial#324 resulted in value: 0.023693683811651156. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:26:55,834] Finished trial#325 resulted in value: 0.9086308271404989. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:27:40,949] Finished trial#326 resulted in value: 0.027195383331436673. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:29:21,391] Finished trial#327 resulted in value: 0.027161640236121998. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:31:09,566] Finished trial#328 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:31:49,181] Finished trial#329 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:32:29,098] Finished trial#330 resulted in value: 0.022423571565777678. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:33:08,340] Finished trial#331 resulted in value: 0.02600960454915824. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:33:57,715] Finished trial#332 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:34:36,605] Finished trial#333 resulted in value: 0.03866785565812236. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:35:17,787] Finished trial#334 resulted in value: 0.22322499616850722. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:35:58,413] Finished trial#335 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:36:36,783] Finished trial#336 resulted in value: 0.03862913253540545. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:37:16,267] Finished trial#337 resulted in value: 0.02215144405847469. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:37:55,430] Finished trial#338 resulted in value: 0.03047121020572341. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:38:34,288] Finished trial#339 resulted in value: 0.02853014890166583. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:39:12,949] Finished trial#340 resulted in value: 0.03012234441442685. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:39:52,399] Finished trial#341 resulted in value: 0.03107544021694797. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:40:30,752] Finished trial#342 resulted in value: 0.022435716123878513. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 01:41:09,183] Finished trial#343 resulted in value: 0.030970995254768652. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:41:49,435] Finished trial#344 resulted in value: 0.031053123295512797. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:42:27,291] Finished trial#345 resulted in value: 0.023015688802922818. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:43:05,334] Finished trial#346 resulted in value: 0.6290196078431373. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:43:42,654] Finished trial#347 resulted in value: 0.029134323051226874. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:44:20,046] Finished trial#348 resulted in value: 0.23318402428478147. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:44:56,123] Finished trial#349 resulted in value: 0.03302027091029647. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:45:33,628] Finished trial#350 resulted in value: 0.02440642384358893. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:46:11,240] Finished trial#351 resulted in value: 0.02772838920257714. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:46:48,894] Finished trial#352 resulted in value: 0.02897245144823568. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:47:26,083] Finished trial#353 resulted in value: 0.03135846864000036. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:48:03,497] Finished trial#354 resulted in value: 0.027045514229696588. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:48:42,558] Finished trial#355 resulted in value: 0.22620858575988534. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:49:20,387] Finished trial#356 resulted in value: 0.026873093084568866. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:49:58,322] Finished trial#357 resulted in value: 0.024903447776372323. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:50:34,173] Finished trial#358 resulted in value: 0.04083918551176746. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:51:14,106] Finished trial#359 resulted in value: 0.02703910039634927. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:51:51,087] Finished trial#360 resulted in value: 0.02510985031385804. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:52:28,115] Finished trial#361 resulted in value: 0.04274917705796688. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:53:05,575] Finished trial#362 resulted in value: 0.02486997503487598. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:53:42,142] Finished trial#363 resulted in value: 0.02772825808462509. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:54:20,790] Finished trial#364 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:54:57,789] Finished trial#365 resulted in value: 0.02404669682865168. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:55:34,589] Finished trial#366 resulted in value: 0.029919455673254824. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:56:12,184] Finished trial#367 resulted in value: 0.020165346995807965. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:56:53,062] Finished trial#368 resulted in value: 0.027170050556008185. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:57:37,466] Finished trial#369 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:58:14,547] Finished trial#370 resulted in value: 0.021963626599911357. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 01:58:51,952] Finished trial#371 resulted in value: 0.02801085320990404. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 01:59:27,401] Finished trial#372 resulted in value: 0.044155577591693196. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:00:04,219] Finished trial#373 resulted in value: 0.02798645743531536. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:00:40,710] Finished trial#374 resulted in value: 0.027403686425656604. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:01:17,040] Finished trial#375 resulted in value: 0.02639923974446956. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:01:53,511] Finished trial#376 resulted in value: 0.07003310415075126. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:02:31,300] Finished trial#377 resulted in value: 0.02613717017323114. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:03:10,845] Finished trial#378 resulted in value: 0.21529929521670987. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:03:49,018] Finished trial#379 resulted in value: 0.0231879798566359. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:04:24,266] Finished trial#380 resulted in value: 0.03513116554207085. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:05:01,753] Finished trial#381 resulted in value: 0.032367244028942865. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:05:38,786] Finished trial#382 resulted in value: 0.027147337641805436. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:06:17,079] Finished trial#383 resulted in value: 0.02503444367134977. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:06:54,826] Finished trial#384 resulted in value: 0.030187563215600455. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:07:33,163] Finished trial#385 resulted in value: 0.030983935027737552. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:08:10,795] Finished trial#386 resulted in value: 0.03193197276393178. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:08:49,092] Finished trial#387 resulted in value: 0.023423698240842894. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:09:26,450] Finished trial#388 resulted in value: 0.02014948114578119. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:10:02,872] Finished trial#389 resulted in value: 0.4283051577006157. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:10:40,060] Finished trial#390 resulted in value: 0.02843865238016252. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:11:19,690] Finished trial#391 resulted in value: 0.036691405675599364. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:11:57,037] Finished trial#392 resulted in value: 0.02822113456424158. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:12:34,239] Finished trial#393 resulted in value: 0.025795838620843226. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:13:13,191] Finished trial#394 resulted in value: 0.5333606349600881. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:14:15,721] Finished trial#395 resulted in value: 0.0423494517169809. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:15:29,811] Finished trial#396 resulted in value: 0.027542087542087534. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:16:27,955] Finished trial#397 resulted in value: 0.019963765972321967. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:17:08,125] Finished trial#398 resulted in value: 0.023091980849268223. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:17:47,931] Finished trial#399 resulted in value: 0.019619296309536738. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 02:18:32,009] Finished trial#400 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:19:14,102] Finished trial#401 resulted in value: 0.22745822319273645. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:19:53,600] Finished trial#402 resulted in value: 0.02089374141194167. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:20:33,411] Finished trial#403 resulted in value: 0.024460569376868135. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:21:14,680] Finished trial#404 resulted in value: 0.018874572022499314. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:21:53,475] Finished trial#405 resulted in value: 0.4291963738694302. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:22:31,728] Finished trial#406 resulted in value: 0.019165065912922108. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:23:10,045] Finished trial#407 resulted in value: 0.022136227092590333. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:23:48,177] Finished trial#408 resulted in value: 0.6246217074537643. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:24:25,664] Finished trial#409 resulted in value: 0.0263900511237688. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:25:03,698] Finished trial#410 resulted in value: 0.02952516987843068. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:25:42,053] Finished trial#411 resulted in value: 0.22667708039806178. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:26:18,811] Finished trial#412 resulted in value: 0.02241272250835813. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:26:54,020] Finished trial#413 resulted in value: 0.027579179283663158. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:27:33,160] Finished trial#414 resulted in value: 0.027458922362824323. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:28:12,276] Finished trial#415 resulted in value: 0.021959228135698616. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:28:51,507] Finished trial#416 resulted in value: 0.9724331926863572. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:29:30,099] Finished trial#417 resulted in value: 0.0216042780748662. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:30:09,173] Finished trial#418 resulted in value: 0.025216417547105463. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:30:47,641] Finished trial#419 resulted in value: 0.4244132178094442. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:31:25,928] Finished trial#420 resulted in value: 0.033128831927255575. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:32:11,897] Finished trial#421 resulted in value: 0.036923805909877316. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:32:49,760] Finished trial#422 resulted in value: 0.030765423141775217. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:33:27,881] Finished trial#423 resulted in value: 0.026379363815377532. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:34:05,529] Finished trial#424 resulted in value: 0.028562881016781838. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:34:45,271] Finished trial#425 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:35:23,394] Finished trial#426 resulted in value: 0.03162261719218051. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:36:07,416] Finished trial#427 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:36:45,942] Finished trial#428 resulted in value: 0.02489168601140701. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 02:37:22,895] Finished trial#429 resulted in value: 0.03320012159560304. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:38:01,832] Finished trial#430 resulted in value: 0.023421944856279353. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:38:42,363] Finished trial#431 resulted in value: 0.03194395561514651. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:39:22,003] Finished trial#432 resulted in value: 0.029424460547271103. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:39:58,900] Finished trial#433 resulted in value: 0.2358000155872496. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:40:37,696] Finished trial#434 resulted in value: 0.027976718752736773. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:41:16,118] Finished trial#435 resulted in value: 0.9453909391652304. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:41:55,749] Finished trial#436 resulted in value: 0.026213192105246086. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:42:34,509] Finished trial#437 resulted in value: 0.030278061007257384. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:43:12,990] Finished trial#438 resulted in value: 0.02167167698593353. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:43:50,238] Finished trial#439 resulted in value: 0.023665609718241276. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:44:27,346] Finished trial#440 resulted in value: 0.020167069338512866. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:45:04,037] Finished trial#441 resulted in value: 0.027504848883220134. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:45:41,435] Finished trial#442 resulted in value: 0.031052831173931317. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:46:18,403] Finished trial#443 resulted in value: 0.021948342880156013. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:46:55,169] Finished trial#444 resulted in value: 0.031854329887398114. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:47:32,399] Finished trial#445 resulted in value: 0.032181023641402184. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:48:08,093] Finished trial#446 resulted in value: 0.04612432080495432. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:48:53,648] Finished trial#447 resulted in value: 0.9729577464788732. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:49:32,737] Finished trial#448 resulted in value: 0.026442387573292647. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:50:10,244] Finished trial#449 resulted in value: 0.043145336991285865. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:50:48,947] Finished trial#450 resulted in value: 0.02741732428167043. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:51:28,202] Finished trial#451 resulted in value: 0.027906875371507822. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:52:07,007] Finished trial#452 resulted in value: 0.025954263479459394. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:52:45,202] Finished trial#453 resulted in value: 0.026360689203217547. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:53:23,882] Finished trial#454 resulted in value: 0.02531244340883243. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:54:03,627] Finished trial#455 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:54:43,050] Finished trial#456 resulted in value: 0.025986266143517178. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 02:55:22,001] Finished trial#457 resulted in value: 0.0283788578217663. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:56:00,992] Finished trial#458 resulted in value: 0.026051654365270904. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:56:39,722] Finished trial#459 resulted in value: 0.029662523820599418. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:57:20,223] Finished trial#460 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:57:58,674] Finished trial#461 resulted in value: 0.9703496503496504. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:58:35,404] Finished trial#462 resulted in value: 0.6342000555195706. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:59:14,651] Finished trial#463 resulted in value: 0.02182708641673481. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 02:59:53,740] Finished trial#464 resulted in value: 0.0308903754608002. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:00:33,297] Finished trial#465 resulted in value: 0.025691823754417698. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:01:13,030] Finished trial#466 resulted in value: 0.034328068335958806. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:01:54,637] Finished trial#467 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:02:34,303] Finished trial#468 resulted in value: 0.023998059681520134. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:03:13,647] Finished trial#469 resulted in value: 0.02558681887906178. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:04:03,707] Finished trial#470 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:04:42,875] Finished trial#471 resulted in value: 0.027253872665066736. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:05:20,468] Finished trial#472 resulted in value: 0.08146849338025819. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:06:00,712] Finished trial#473 resulted in value: 0.020038784744667026. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:06:40,457] Finished trial#474 resulted in value: 0.022886336774432636. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:07:20,503] Finished trial#475 resulted in value: 0.044797728026434336. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:08:00,299] Finished trial#476 resulted in value: 0.023369700239279867. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:08:39,918] Finished trial#477 resulted in value: 0.024262832207731067. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:09:18,146] Finished trial#478 resulted in value: 0.022968079496721194. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:10:20,834] Finished trial#479 resulted in value: 0.05244329015414573. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:12:13,321] Finished trial#480 resulted in value: 0.018111752188591068. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:13:36,921] Finished trial#481 resulted in value: 0.018350098539851922. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:14:25,871] Finished trial#482 resulted in value: 0.2271545134493882. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:15:09,592] Finished trial#483 resulted in value: 0.21349852632322053. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:15:53,094] Finished trial#484 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:16:38,773] Finished trial#485 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 03:17:25,218] Finished trial#486 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:18:08,396] Finished trial#487 resulted in value: 0.22578276686600718. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:18:53,845] Finished trial#488 resulted in value: 0.03320981874680484. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:19:35,176] Finished trial#489 resulted in value: 0.6121469784391133. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:20:16,951] Finished trial#490 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:20:56,305] Finished trial#491 resulted in value: 0.03151711729061479. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:21:36,723] Finished trial#492 resulted in value: 0.030208181101798237. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:22:20,353] Finished trial#493 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:23:03,930] Finished trial#494 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:23:45,183] Finished trial#495 resulted in value: 0.020780863072653033. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:24:27,035] Finished trial#496 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:25:09,518] Finished trial#497 resulted in value: 0.032611491584014884. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:25:52,270] Finished trial#498 resulted in value: 0.031365730588774676. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:26:34,530] Finished trial#499 resulted in value: 0.026392833874151256. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:27:14,925] Finished trial#500 resulted in value: 0.022000738279807974. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:27:57,192] Finished trial#501 resulted in value: 0.03659060653436064. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:28:37,729] Finished trial#502 resulted in value: 0.020623626245751425. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:29:19,266] Finished trial#503 resulted in value: 0.026742675808096417. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:30:01,496] Finished trial#504 resulted in value: 0.8153846153846154. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:30:43,682] Finished trial#505 resulted in value: 0.026471947218976055. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:31:24,644] Finished trial#506 resulted in value: 0.9428629715648735. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:32:05,584] Finished trial#507 resulted in value: 0.027563278467379604. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:32:45,938] Finished trial#508 resulted in value: 0.024019420406663272. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:33:26,581] Finished trial#509 resulted in value: 0.024492163648328402. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:34:10,550] Finished trial#510 resulted in value: 0.03395070809230094. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:34:53,098] Finished trial#511 resulted in value: 0.026664346837966924. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:35:33,666] Finished trial#512 resulted in value: 0.02262310788626576. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:36:14,455] Finished trial#513 resulted in value: 0.02991311331508706. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:36:56,535] Finished trial#514 resulted in value: 0.024593106919571506. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 03:37:38,975] Finished trial#515 resulted in value: 0.05395163530957314. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:38:22,780] Finished trial#516 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:39:06,199] Finished trial#517 resulted in value: 0.02972760091960891. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:39:47,496] Finished trial#518 resulted in value: 0.8042553191489361. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:40:30,186] Finished trial#519 resulted in value: 0.025615668445378104. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:41:12,192] Finished trial#520 resulted in value: 0.029344711634468723. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:41:52,613] Finished trial#521 resulted in value: 0.022829184099595445. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:42:32,030] Finished trial#522 resulted in value: 0.02377851445428658. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:43:10,256] Finished trial#523 resulted in value: 0.02758341481052029. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:43:53,080] Finished trial#524 resulted in value: 0.027688451330962538. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:44:36,822] Finished trial#525 resulted in value: 0.030469015125815346. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:45:18,484] Finished trial#526 resulted in value: 0.027191040476240103. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:46:00,793] Finished trial#527 resulted in value: 0.027224917538121618. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:46:46,545] Finished trial#528 resulted in value: 0.044344124671026997. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:47:31,042] Finished trial#529 resulted in value: 0.028328897579785073. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:48:12,361] Finished trial#530 resulted in value: 0.2429223918090816. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:48:54,232] Finished trial#531 resulted in value: 0.02123909020115511. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:49:36,355] Finished trial#532 resulted in value: 0.46311355311355307. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:50:20,376] Finished trial#533 resulted in value: 0.027128605923967042. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:51:01,809] Finished trial#534 resulted in value: 0.022403248219558747. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:51:45,845] Finished trial#535 resulted in value: 0.04134738267574034. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:52:27,994] Finished trial#536 resulted in value: 0.02696052602980059. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:53:10,617] Finished trial#537 resulted in value: 0.9433645517976681. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:53:52,762] Finished trial#538 resulted in value: 0.02735151022395743. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:54:35,266] Finished trial#539 resulted in value: 0.021202191060133324. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:55:18,742] Finished trial#540 resulted in value: 0.030317183501886946. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:56:00,161] Finished trial#541 resulted in value: 0.022879644760592677. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:56:42,845] Finished trial#542 resulted in value: 0.026250330507864517. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:57:27,498] Finished trial#543 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 03:58:09,582] Finished trial#544 resulted in value: 0.023646371735665084. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:58:52,754] Finished trial#545 resulted in value: 0.8113207547169812. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 03:59:33,756] Finished trial#546 resulted in value: 0.04554216593254401. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:00:18,376] Finished trial#547 resulted in value: 0.022150342161028624. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:01:00,755] Finished trial#548 resulted in value: 0.021943958086037774. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:01:46,394] Finished trial#549 resulted in value: 0.03305024970857262. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:02:30,916] Finished trial#550 resulted in value: 0.032745447807478234. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:03:13,025] Finished trial#551 resulted in value: 0.027286470500266446. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:03:55,466] Finished trial#552 resulted in value: 0.029613567835527377. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:04:39,634] Finished trial#553 resulted in value: 0.027587353132950576. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:05:23,377] Finished trial#554 resulted in value: 0.029150271201656253. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:06:06,614] Finished trial#555 resulted in value: 0.030456746748881658. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:06:48,566] Finished trial#556 resulted in value: 0.028398418842386697. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:07:34,693] Finished trial#557 resulted in value: 0.02968577285073193. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:08:18,641] Finished trial#558 resulted in value: 0.025373539736521344. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:09:03,945] Finished trial#559 resulted in value: 0.04076207900617612. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:09:47,394] Finished trial#560 resulted in value: 0.02597538680673872. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:10:29,542] Finished trial#561 resulted in value: 0.028827196840681246. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:11:12,975] Finished trial#562 resulted in value: 0.02822640794820186. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:11:57,686] Finished trial#563 resulted in value: 0.02449828748066274. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:12:42,576] Finished trial#564 resulted in value: 0.02700557793170366. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:13:26,363] Finished trial#565 resulted in value: 0.02506407583525705. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:14:08,257] Finished trial#566 resulted in value: 0.03467222324158592. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:14:53,252] Finished trial#567 resulted in value: 0.023474611566146164. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:15:36,430] Finished trial#568 resulted in value: 0.0367111152543359. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:16:21,436] Finished trial#569 resulted in value: 0.9476101800124147. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:17:07,090] Finished trial#570 resulted in value: 0.031704432671490146. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:17:51,486] Finished trial#571 resulted in value: 0.022467010900745876. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 04:18:38,970] Finished trial#572 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:19:23,681] Finished trial#573 resulted in value: 0.2366383045036622. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:20:06,140] Finished trial#574 resulted in value: 0.024453829675522676. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:20:48,260] Finished trial#575 resulted in value: 0.0304687434683395. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:21:29,299] Finished trial#576 resulted in value: 0.03799139493126957. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:22:16,994] Finished trial#577 resulted in value: 0.04129430121365607. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:23:00,965] Finished trial#578 resulted in value: 0.018044818754446412. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:25:01,313] Finished trial#579 resulted in value: 0.023195494016383322. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:27:09,119] Finished trial#580 resulted in value: 0.023571335286106. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:28:34,040] Finished trial#581 resulted in value: 0.021789866460143692. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:29:22,378] Finished trial#582 resulted in value: 0.026062480659859055. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:30:11,304] Finished trial#583 resulted in value: 0.03465984107391784. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:30:59,192] Finished trial#584 resulted in value: 0.9258444263504094. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:31:54,663] Finished trial#585 resulted in value: 0.027228355833440965. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:32:37,103] Finished trial#586 resulted in value: 0.053518358089378326. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:33:29,727] Finished trial#587 resulted in value: 0.4248473748473749. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:34:15,660] Finished trial#588 resulted in value: 0.018601774293407702. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:35:01,820] Finished trial#589 resulted in value: 0.021603455541508665. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:35:47,426] Finished trial#590 resulted in value: 0.026675951206425297. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:36:33,571] Finished trial#591 resulted in value: 0.022395296181703883. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:37:18,520] Finished trial#592 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:38:24,873] Finished trial#593 resulted in value: 0.803921568627451. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:39:10,373] Finished trial#594 resulted in value: 0.020368205922020932. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:39:54,938] Finished trial#595 resulted in value: 0.026321620875873863. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:40:43,299] Finished trial#596 resulted in value: 0.03433551198257079. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:41:29,429] Finished trial#597 resulted in value: 0.04241569368341269. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:42:13,639] Finished trial#598 resulted in value: 0.03193515733142771. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:43:00,026] Finished trial#599 resulted in value: 0.02769391626022011. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:43:44,825] Finished trial#600 resulted in value: 0.029000705198724885. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 04:44:29,450] Finished trial#601 resulted in value: 0.2298738015229569. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:45:16,766] Finished trial#602 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:46:03,215] Finished trial#603 resulted in value: 0.024656479656315122. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:46:48,385] Finished trial#604 resulted in value: 0.030255179819420386. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:47:40,779] Finished trial#605 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:48:27,817] Finished trial#606 resulted in value: 0.019741683956932876. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:49:13,305] Finished trial#607 resulted in value: 0.02239192488314168. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:49:55,911] Finished trial#608 resulted in value: 0.033794971506061366. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:50:44,562] Finished trial#609 resulted in value: 0.028957574724723267. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:51:29,862] Finished trial#610 resulted in value: 0.028204172447760967. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:52:16,597] Finished trial#611 resulted in value: 0.9496111327037196. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:52:58,061] Finished trial#612 resulted in value: 0.04820873539441839. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:53:44,647] Finished trial#613 resulted in value: 0.03218969158619933. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:54:33,366] Finished trial#614 resulted in value: 0.028242651280433462. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:55:19,578] Finished trial#615 resulted in value: 0.42767751874293936. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:56:05,575] Finished trial#616 resulted in value: 0.024820728601001152. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:56:50,059] Finished trial#617 resulted in value: 0.02753679069468551. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:57:37,214] Finished trial#618 resulted in value: 0.02262286418402315. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:58:24,491] Finished trial#619 resulted in value: 0.0395893788226902. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 04:59:13,732] Finished trial#620 resulted in value: 0.030318227735937064. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:00:00,179] Finished trial#621 resulted in value: 0.02691630517153598. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:00:48,221] Finished trial#622 resulted in value: 0.018682673044043696. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:01:32,858] Finished trial#623 resulted in value: 0.033698809284411935. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:02:21,529] Finished trial#624 resulted in value: 0.02910603605090356. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:03:21,404] Finished trial#625 resulted in value: 0.026207607994842008. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:04:21,871] Finished trial#626 resulted in value: 0.022073199026263146. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:05:08,455] Finished trial#627 resulted in value: 0.03288703801936954. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:05:56,670] Finished trial#628 resulted in value: 0.027287506170853737. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:06:43,575] Finished trial#629 resulted in value: 0.02947476007166505. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 05:07:31,590] Finished trial#630 resulted in value: 0.03877354149586443. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:08:17,620] Finished trial#631 resulted in value: 0.02752026677987074. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:09:16,490] Finished trial#632 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:10:01,001] Finished trial#633 resulted in value: 0.030991529934550455. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:10:51,382] Finished trial#634 resulted in value: 0.030155560253030678. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:11:41,014] Finished trial#635 resulted in value: 0.020910318225650926. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:12:29,313] Finished trial#636 resulted in value: 0.03443640747438226. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:13:25,611] Finished trial#637 resulted in value: 0.0253095864353442. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:14:14,149] Finished trial#638 resulted in value: 0.03215077605321515. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:15:04,739] Finished trial#639 resulted in value: 0.0203971350240556. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:15:52,068] Finished trial#640 resulted in value: 0.027680307558972883. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:16:42,227] Finished trial#641 resulted in value: 0.019952985610880325. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:17:32,554] Finished trial#642 resulted in value: 0.02200136364758587. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:18:20,324] Finished trial#643 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:19:05,516] Finished trial#644 resulted in value: 0.03736135670694163. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:19:57,039] Finished trial#645 resulted in value: 0.21386035895839817. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:20:51,303] Finished trial#646 resulted in value: 0.04722252435335983. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:21:38,396] Finished trial#647 resulted in value: 0.023899216314655747. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:22:31,844] Finished trial#648 resulted in value: 0.025337141362726023. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:23:25,166] Finished trial#649 resulted in value: 0.02451880654573768. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:24:16,863] Finished trial#650 resulted in value: 0.04030682438308797. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:25:06,113] Finished trial#651 resulted in value: 0.028881800460747842. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:25:54,751] Finished trial#652 resulted in value: 0.027440548603431814. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:26:46,164] Finished trial#653 resulted in value: 0.027894441879162235. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:27:31,680] Finished trial#654 resulted in value: 0.03573437438914495. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:28:20,351] Finished trial#655 resulted in value: 0.4712301371424428. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:29:07,254] Finished trial#656 resulted in value: 0.08715418405952102. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:29:55,718] Finished trial#657 resulted in value: 0.6146491228070176. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:30:46,107] Finished trial#658 resulted in value: 0.028207287779320134. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 05:31:34,527] Finished trial#659 resulted in value: 0.026991923682164076. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:32:23,364] Finished trial#660 resulted in value: 0.022384570849616425. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:33:16,593] Finished trial#661 resulted in value: 0.7161469161469161. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:34:02,359] Finished trial#662 resulted in value: 0.04329746092724329. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:34:55,056] Finished trial#663 resulted in value: 0.03275405883109472. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:35:44,081] Finished trial#664 resulted in value: 0.0333711722430714. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:36:34,133] Finished trial#665 resulted in value: 0.03167088697880571. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:37:30,850] Finished trial#666 resulted in value: 0.0263241660300485. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:38:20,141] Finished trial#667 resulted in value: 0.024053919107338784. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:39:10,341] Finished trial#668 resulted in value: 0.034223303578142295. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:40:04,301] Finished trial#669 resulted in value: 0.02273102455473275. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:40:54,226] Finished trial#670 resulted in value: 0.0842160255804083. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:41:52,963] Finished trial#671 resulted in value: 0.22133982302844835. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:42:47,483] Finished trial#672 resulted in value: 0.031198115609880284. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:43:35,616] Finished trial#673 resulted in value: 0.035639645566077016. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:44:30,705] Finished trial#674 resulted in value: 0.026655923277516536. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:45:31,011] Finished trial#675 resulted in value: 0.22784168987098996. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:46:23,143] Finished trial#676 resulted in value: 0.019826143909296823. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:47:16,399] Finished trial#677 resulted in value: 0.025729944926564996. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:48:07,349] Finished trial#678 resulted in value: 0.024496057347670352. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:48:58,854] Finished trial#679 resulted in value: 0.03220408761188909. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:49:53,910] Finished trial#680 resulted in value: 0.030390930213012757. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:50:41,168] Finished trial#681 resulted in value: 0.03133452578127949. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:51:40,902] Finished trial#682 resulted in value: 0.029433318257132912. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:52:40,304] Finished trial#683 resulted in value: 0.02470247893976707. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:53:35,786] Finished trial#684 resulted in value: 0.031226093402262345. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:54:28,336] Finished trial#685 resulted in value: 0.03293660429426182. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 05:55:29,972] Finished trial#686 resulted in value: 0.030446489863965476. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 05:58:13,619] Finished trial#687 resulted in value: 0.5953277516029969. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:01:16,354] Finished trial#688 resulted in value: 0.026150831790240248. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:03:17,136] Finished trial#689 resulted in value: 0.029471927059501413. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:04:34,526] Finished trial#690 resulted in value: 0.8114942528735632. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:05:31,790] Finished trial#691 resulted in value: 0.03745062945383215. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:06:28,947] Finished trial#692 resulted in value: 0.023835892424811655. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:07:33,035] Finished trial#693 resulted in value: 0.023013852696386783. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:08:33,409] Finished trial#694 resulted in value: 0.03165098278842249. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:09:34,705] Finished trial#695 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:10:33,171] Finished trial#696 resulted in value: 0.022739190116678132. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:11:28,695] Finished trial#697 resulted in value: 0.030201509405049287. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:12:17,563] Finished trial#698 resulted in value: 0.037341379826135834. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:13:16,144] Finished trial#699 resulted in value: 0.02948021835180159. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:14:17,466] Finished trial#700 resulted in value: 0.03323398768330965. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:15:15,805] Finished trial#701 resulted in value: 0.0245265748456458. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:16:23,375] Finished trial#702 resulted in value: 0.022249705149843257. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:17:27,349] Finished trial#703 resulted in value: 0.026210380332357452. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:18:18,696] Finished trial#704 resulted in value: 0.03846267139158388. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:19:33,304] Finished trial#705 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:20:26,756] Finished trial#706 resulted in value: 0.024004007543611428. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:21:26,554] Finished trial#707 resulted in value: 0.027554192399762423. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:22:23,109] Finished trial#708 resulted in value: 0.2250300495510965. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:23:34,648] Finished trial#709 resulted in value: 0.028326159965777142. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:24:38,405] Finished trial#710 resulted in value: 0.020174723785761683. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:25:37,467] Finished trial#711 resulted in value: 0.02536197325433387. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:26:36,860] Finished trial#712 resulted in value: 0.025731080955208485. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:27:32,056] Finished trial#713 resulted in value: 0.2321052631578947. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:28:31,098] Finished trial#714 resulted in value: 0.4232600732600733. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:29:33,306] Finished trial#715 resulted in value: 0.020331961225741546. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 06:30:30,548] Finished trial#716 resulted in value: 0.02660302842936857. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:31:25,293] Finished trial#717 resulted in value: 0.03431919314272247. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:32:24,854] Finished trial#718 resulted in value: 0.020346860848312054. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:33:34,624] Finished trial#719 resulted in value: 1.0. Current best value is 0.01600410860029977 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.39529206391006094, 'learning_rate': 0.009817984673829709}.\n",
      "[I 2020-01-01 06:34:32,271] Finished trial#720 resulted in value: 0.015510467745563639. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:35:34,044] Finished trial#721 resulted in value: 0.0224883936357394. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:36:39,994] Finished trial#722 resulted in value: 0.9143544757195384. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:37:41,041] Finished trial#723 resulted in value: 0.026864806553426712. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:38:45,100] Finished trial#724 resulted in value: 0.03901697193901765. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:39:50,324] Finished trial#725 resulted in value: 0.02437347768617104. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:41:07,198] Finished trial#726 resulted in value: 0.02537618766796279. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:42:25,558] Finished trial#727 resulted in value: 0.7760797342192691. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:43:27,886] Finished trial#728 resulted in value: 0.22267723968658548. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:44:25,612] Finished trial#729 resulted in value: 0.03196435029708089. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:45:26,054] Finished trial#730 resulted in value: 0.027648043780119202. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:46:26,887] Finished trial#731 resulted in value: 0.03139568975435236. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:47:28,208] Finished trial#732 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:48:34,752] Finished trial#733 resulted in value: 0.027790200673057486. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:49:51,378] Finished trial#734 resulted in value: 0.9403777879557536. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:50:48,658] Finished trial#735 resulted in value: 0.026175208439359388. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:51:50,261] Finished trial#736 resulted in value: 0.0373857971109951. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:52:47,194] Finished trial#737 resulted in value: 0.025286405597862882. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:53:52,214] Finished trial#738 resulted in value: 0.023051313628899783. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:54:58,397] Finished trial#739 resulted in value: 0.02661971889498016. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:56:03,161] Finished trial#740 resulted in value: 0.041291214379180374. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:57:10,038] Finished trial#741 resulted in value: 0.028934637700999355. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:58:18,481] Finished trial#742 resulted in value: 0.022858663003590407. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 06:59:50,569] Finished trial#743 resulted in value: 0.9713884992987377. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:00:56,956] Finished trial#744 resulted in value: 0.028003341687552252. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 07:02:04,168] Finished trial#745 resulted in value: 0.035601410474076056. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:03:14,551] Finished trial#746 resulted in value: 0.8090909090909091. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:04:21,474] Finished trial#747 resulted in value: 0.024306566282354836. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:05:31,057] Finished trial#748 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:06:35,770] Finished trial#749 resulted in value: 0.0274054866447484. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:07:44,339] Finished trial#750 resulted in value: 0.029569556264925856. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:08:56,319] Finished trial#751 resulted in value: 0.03039109193127909. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:10:07,640] Finished trial#752 resulted in value: 0.035526035241683074. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:11:20,265] Finished trial#753 resulted in value: 0.029921603249175255. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:12:29,998] Finished trial#754 resulted in value: 0.807843137254902. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:13:37,348] Finished trial#755 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:14:41,732] Finished trial#756 resulted in value: 0.025556465556465624. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:15:42,356] Finished trial#757 resulted in value: 0.030695841141939573. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:16:56,620] Finished trial#758 resulted in value: 0.02842344484963788. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:18:10,874] Finished trial#759 resulted in value: 0.02700894362636963. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:19:22,709] Finished trial#760 resulted in value: 0.02397535380214799. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:20:47,541] Finished trial#761 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:22:07,410] Finished trial#762 resulted in value: 0.02199107059931804. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:23:36,805] Finished trial#763 resulted in value: 0.020190806099346204. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:25:03,749] Finished trial#764 resulted in value: 0.018114437469821332. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:26:15,084] Finished trial#765 resulted in value: 0.020622561489150315. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:27:26,391] Finished trial#766 resulted in value: 0.024618303188357338. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:28:43,589] Finished trial#767 resulted in value: 0.025995955730468978. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:29:52,673] Finished trial#768 resulted in value: 0.029048012831327186. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:30:50,472] Finished trial#769 resulted in value: 0.031239848539562964. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:31:57,143] Finished trial#770 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:33:15,061] Finished trial#771 resulted in value: 0.024114075986474304. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:34:29,180] Finished trial#772 resulted in value: 0.03446776719222533. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:35:41,791] Finished trial#773 resulted in value: 0.024837569665155934. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 07:37:05,605] Finished trial#774 resulted in value: 0.029266277792743267. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:38:25,234] Finished trial#775 resulted in value: 0.8895021082495292. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:39:39,723] Finished trial#776 resulted in value: 0.028680498947887267. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:40:55,860] Finished trial#777 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:42:26,936] Finished trial#778 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:43:43,815] Finished trial#779 resulted in value: 0.028903572255368437. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:44:52,133] Finished trial#780 resulted in value: 0.03203035234346374. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:46:11,416] Finished trial#781 resulted in value: 0.6106975461814171. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:49:11,500] Finished trial#782 resulted in value: 0.4782675438596492. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:52:02,939] Finished trial#783 resulted in value: 0.03213659776687661. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:55:32,780] Finished trial#784 resulted in value: 0.02192815744331733. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:56:57,784] Finished trial#785 resulted in value: 0.22304221307067984. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:58:14,952] Finished trial#786 resulted in value: 0.02579995194453022. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 07:59:25,609] Finished trial#787 resulted in value: 0.02632779691064724. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:00:37,726] Finished trial#788 resulted in value: 0.028858309848670438. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:02:00,143] Finished trial#789 resulted in value: 0.02844919786096245. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:03:21,970] Finished trial#790 resulted in value: 0.023652045006166134. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:04:41,557] Finished trial#791 resulted in value: 0.027493189495477743. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:05:57,597] Finished trial#792 resulted in value: 0.030318180151328145. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:07:37,157] Finished trial#793 resulted in value: 0.026825684901909952. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:08:59,004] Finished trial#794 resulted in value: 0.024430742156633878. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:10:15,585] Finished trial#795 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:11:50,144] Finished trial#796 resulted in value: 0.233206042238986. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:13:13,959] Finished trial#797 resulted in value: 0.024428461977018978. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:14:31,962] Finished trial#798 resulted in value: 0.024243321158141207. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:15:45,975] Finished trial#799 resulted in value: 0.038034467814835216. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:17:08,946] Finished trial#800 resulted in value: 0.029465395380680603. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:18:18,701] Finished trial#801 resulted in value: 0.06595667109934611. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:19:35,852] Finished trial#802 resulted in value: 0.023955018585738785. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 08:20:57,549] Finished trial#803 resulted in value: 0.029479684927427252. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:22:17,283] Finished trial#804 resulted in value: 0.026695260187025283. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:23:50,479] Finished trial#805 resulted in value: 0.6833333333333333. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:25:06,535] Finished trial#806 resulted in value: 0.034845072453319914. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:26:26,482] Finished trial#807 resulted in value: 0.023611219468117617. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:27:48,521] Finished trial#808 resulted in value: 0.030990363281005573. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:29:05,548] Finished trial#809 resulted in value: 0.23099486494433852. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:30:28,000] Finished trial#810 resulted in value: 0.029510562392271567. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:31:57,163] Finished trial#811 resulted in value: 0.028505655089580584. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:33:26,113] Finished trial#812 resulted in value: 0.024466831719731408. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:34:52,009] Finished trial#813 resulted in value: 0.024091495537659435. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:36:13,350] Finished trial#814 resulted in value: 0.0351646724410859. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:37:41,244] Finished trial#815 resulted in value: 0.02580901091245913. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:39:11,014] Finished trial#816 resulted in value: 0.23145992683737948. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:40:33,729] Finished trial#817 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:41:59,785] Finished trial#818 resulted in value: 0.02268448325222816. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:43:26,082] Finished trial#819 resulted in value: 0.03518849534738311. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:44:47,367] Finished trial#820 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:45:51,234] Finished trial#821 resulted in value: 0.02785617589964784. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:47:08,111] Finished trial#822 resulted in value: 0.4233283431557714. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:48:48,661] Finished trial#823 resulted in value: 0.02437452518979666. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:50:11,755] Finished trial#824 resulted in value: 0.03842155003291603. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:51:37,591] Finished trial#825 resulted in value: 0.020942403692274958. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:53:08,089] Finished trial#826 resulted in value: 0.029229593923218666. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:54:46,198] Finished trial#827 resulted in value: 0.028202310153290555. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:56:09,295] Finished trial#828 resulted in value: 0.030552990869287866. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:57:39,425] Finished trial#829 resulted in value: 0.02612674165349793. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 08:59:15,880] Finished trial#830 resulted in value: 0.03313515966454594. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:00:47,921] Finished trial#831 resulted in value: 0.028608082068002583. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 09:02:18,639] Finished trial#832 resulted in value: 0.023625009497407867. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:03:52,660] Finished trial#833 resulted in value: 0.9037974683544304. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:05:23,515] Finished trial#834 resulted in value: 0.025918829214984895. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:06:47,895] Finished trial#835 resulted in value: 0.018463184755319495. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:08:20,564] Finished trial#836 resulted in value: 0.23409379534444663. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:09:49,153] Finished trial#837 resulted in value: 0.0370354052702766. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:11:23,568] Finished trial#838 resulted in value: 0.134238280403582. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:12:51,234] Finished trial#839 resulted in value: 0.024325149484294206. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:14:20,372] Finished trial#840 resulted in value: 0.2195084553997887. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:15:50,760] Finished trial#841 resulted in value: 0.9377203686925616. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:17:20,803] Finished trial#842 resulted in value: 0.026041200273312803. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:18:51,230] Finished trial#843 resulted in value: 0.027706331608770562. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:20:23,028] Finished trial#844 resulted in value: 0.027353294301380338. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:21:55,741] Finished trial#845 resulted in value: 0.026834544295691765. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:24:30,036] Finished trial#846 resulted in value: 0.026130055882667258. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:26:13,988] Finished trial#847 resulted in value: 0.028037181514179443. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:27:36,435] Finished trial#848 resulted in value: 0.02653851657166828. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:29:03,945] Finished trial#849 resulted in value: 0.022273547102234836. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:30:30,153] Finished trial#850 resulted in value: 0.4184732082077215. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:31:55,373] Finished trial#851 resulted in value: 0.02206540476638441. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:33:21,814] Finished trial#852 resulted in value: 0.024059403990582595. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:34:54,095] Finished trial#853 resulted in value: 0.026107877883488784. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:36:14,721] Finished trial#854 resulted in value: 0.03215060462000208. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:38:06,784] Finished trial#855 resulted in value: 0.028469824748894368. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:39:50,096] Finished trial#856 resulted in value: 0.030111797388152395. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:41:33,040] Finished trial#857 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:43:26,183] Finished trial#858 resulted in value: 0.028280239077670744. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:45:29,970] Finished trial#859 resulted in value: 0.021734792388942048. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:47:14,311] Finished trial#860 resulted in value: 0.024976422586529545. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 09:48:57,616] Finished trial#861 resulted in value: 0.022029610011354395. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:50:26,836] Finished trial#862 resulted in value: 0.8067415730337079. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:52:01,997] Finished trial#863 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:53:37,544] Finished trial#864 resulted in value: 0.04163444771848135. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:55:19,176] Finished trial#865 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:57:05,980] Finished trial#866 resulted in value: 0.03157520081658016. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 09:58:35,317] Finished trial#867 resulted in value: 0.033554413205528766. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:00:33,093] Finished trial#868 resulted in value: 0.4267540424211873. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:02:29,148] Finished trial#869 resulted in value: 0.02380180149278155. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:04:24,055] Finished trial#870 resulted in value: 0.02547513497603615. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:06:13,157] Finished trial#871 resulted in value: 0.026926521385896685. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:08:01,459] Finished trial#872 resulted in value: 0.023037089539801103. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:09:50,418] Finished trial#873 resulted in value: 0.032800198278518655. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:12:03,651] Finished trial#874 resulted in value: 0.9242701924448845. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:13:43,648] Finished trial#875 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:15:26,830] Finished trial#876 resulted in value: 0.02091713375122184. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:18:27,520] Finished trial#877 resulted in value: 0.03421833773716654. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:21:43,216] Finished trial#878 resulted in value: 0.031382391422831724. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:24:47,418] Finished trial#879 resulted in value: 0.021395486358759208. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:29:33,599] Finished trial#880 resulted in value: 0.02441180873604254. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:32:25,226] Finished trial#881 resulted in value: 0.02949243810605595. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:34:36,336] Finished trial#882 resulted in value: 0.028758835710683273. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:36:22,057] Finished trial#883 resulted in value: 0.028436959334792178. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:38:12,878] Finished trial#884 resulted in value: 0.024166132627143844. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:39:47,707] Finished trial#885 resulted in value: 0.035929304781959326. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:41:15,696] Finished trial#886 resulted in value: 0.03565735464826525. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:43:10,534] Finished trial#887 resulted in value: 0.02561438561438556. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:45:03,076] Finished trial#888 resulted in value: 0.02474702786391103. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:46:54,271] Finished trial#889 resulted in value: 0.030285147736705098. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 10:48:55,379] Finished trial#890 resulted in value: 0.023614559655747214. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:50:58,458] Finished trial#891 resulted in value: 0.02171667998945792. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:53:06,010] Finished trial#892 resulted in value: 0.2470769620194908. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:55:04,912] Finished trial#893 resulted in value: 0.025963585241997045. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:56:52,248] Finished trial#894 resulted in value: 0.02632985452581771. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 10:59:03,690] Finished trial#895 resulted in value: 0.019087450489889468. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:01:18,905] Finished trial#896 resulted in value: 0.02772958900279554. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:03:33,397] Finished trial#897 resulted in value: 0.030331142882726425. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:05:41,241] Finished trial#898 resulted in value: 0.9142471903960592. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:07:49,745] Finished trial#899 resulted in value: 0.022579083842997116. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:09:53,549] Finished trial#900 resulted in value: 0.019883674307193067. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:11:51,655] Finished trial#901 resulted in value: 0.022339265672598918. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:14:13,088] Finished trial#902 resulted in value: 0.023551774600099384. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:16:32,232] Finished trial#903 resulted in value: 0.03072989160857631. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:18:56,855] Finished trial#904 resulted in value: 0.026131553712738786. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:20:52,074] Finished trial#905 resulted in value: 0.43345543345543347. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:22:42,282] Finished trial#906 resulted in value: 0.03314237484374549. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:25:10,529] Finished trial#907 resulted in value: 0.024295493318771344. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:27:10,859] Finished trial#908 resulted in value: 0.028052485983520503. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:29:30,555] Finished trial#909 resulted in value: 0.04320565717164482. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:31:57,777] Finished trial#910 resulted in value: 0.03173496141417653. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:34:04,357] Finished trial#911 resulted in value: 0.019230266610966495. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:36:13,397] Finished trial#912 resulted in value: 0.01958698360418798. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:38:31,872] Finished trial#913 resulted in value: 0.029210712480129297. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:40:32,890] Finished trial#914 resulted in value: 0.04830550004122236. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:42:50,308] Finished trial#915 resulted in value: 0.02388949812357899. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:45:21,785] Finished trial#916 resulted in value: 0.03423636554783904. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:47:44,062] Finished trial#917 resulted in value: 0.029220734999469422. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:49:52,055] Finished trial#918 resulted in value: 0.02561327473981978. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 11:52:03,710] Finished trial#919 resulted in value: 0.040149002696733715. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:54:38,097] Finished trial#920 resulted in value: 0.02678128323289608. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:57:11,065] Finished trial#921 resulted in value: 0.2602842704657793. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 11:59:39,565] Finished trial#922 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:01:50,919] Finished trial#923 resulted in value: 0.01776748847951448. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:04:16,763] Finished trial#924 resulted in value: 0.021449066583155152. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:07:00,800] Finished trial#925 resulted in value: 0.02200738757726306. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:09:34,824] Finished trial#926 resulted in value: 0.04139094495514595. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:11:19,876] Finished trial#927 resulted in value: 0.02399423963526781. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:13:44,314] Finished trial#928 resulted in value: 0.030913335030982148. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:16:15,476] Finished trial#929 resulted in value: 0.02226553419562527. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:18:28,103] Finished trial#930 resulted in value: 0.027047689629939087. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:20:56,099] Finished trial#931 resulted in value: 0.02608178649085191. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:23:37,661] Finished trial#932 resulted in value: 0.817283950617284. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:25:53,626] Finished trial#933 resulted in value: 0.02548713247234602. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:28:28,628] Finished trial#934 resulted in value: 0.03168836068666292. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:30:54,114] Finished trial#935 resulted in value: 0.43305215277545817. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:33:12,509] Finished trial#936 resulted in value: 0.02406426282830787. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:35:46,905] Finished trial#937 resulted in value: 0.019986038189747646. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:37:29,777] Finished trial#938 resulted in value: 0.019899952296072776. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:40:08,617] Finished trial#939 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:42:34,493] Finished trial#940 resulted in value: 0.027043016938223752. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:45:08,541] Finished trial#941 resulted in value: 0.035906841627348696. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:47:31,030] Finished trial#942 resulted in value: 0.029984061704970477. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:50:14,708] Finished trial#943 resulted in value: 0.21950426147470492. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:52:48,888] Finished trial#944 resulted in value: 0.02335017723498556. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:54:24,360] Finished trial#945 resulted in value: 0.029790960336510297. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:56:52,704] Finished trial#946 resulted in value: 0.028128682055088894. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 12:59:36,791] Finished trial#947 resulted in value: 0.03235923779384764. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-01-01 13:02:41,807] Finished trial#948 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 13:05:30,622] Finished trial#949 resulted in value: 0.23590881926065055. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 13:08:20,126] Finished trial#950 resulted in value: 0.03945721323715057. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 13:10:10,092] Finished trial#951 resulted in value: 0.026690566465002563. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 13:11:38,521] Finished trial#952 resulted in value: 0.02716406400616922. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 13:13:17,425] Finished trial#953 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 13:16:04,657] Finished trial#954 resulted in value: 0.025625508600079727. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 13:19:09,909] Finished trial#955 resulted in value: 0.03010531827233831. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 13:23:03,494] Finished trial#956 resulted in value: 0.024416181857358388. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "[I 2020-01-01 13:27:28,038] Finished trial#957 resulted in value: 1.0. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n",
      "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x50eb20950>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/weakref.py\", line 358, in remove\n",
      "    def remove(k, selfref=ref(self)):\n",
      "KeyboardInterrupt\n",
      "[I 2020-01-01 13:35:31,723] Finished trial#958 resulted in value: 0.022770636584299453. Current best value is 0.015510467745563639 with parameters: {'batch_size': 256, 'n_layers': 3, 'activation': 'sigmoid', 'dropout_rate': 0.3572429090177327, 'learning_rate': 0.00988694316825537}.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class F1Callback(Callback):\n",
    "    def __init__(self, model, X_val, y_val):\n",
    "        self.model = model\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        pred = self.model.predict(self.X_val)\n",
    "        f1_val = f1_score(self.y_val, np.round(pred))\n",
    "        print(\"f1_val =\", f1_val)\n",
    "        # 以下チェックポイントなど必要なら書く\n",
    "\n",
    "def create_model(trial):\n",
    "    # num of hidden layer\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 10)\n",
    "    \n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'sigmoid']) \n",
    "    # dropout_rate\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.5)\n",
    "    \n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        layers.append(\n",
    "            tf.keras.layers.Dense(128,  activation=activation)\n",
    "        )\n",
    "    layers.append(\n",
    "        tf.keras.layers.Dense(1,  activation=activation)\n",
    "    )\n",
    "        \n",
    "    return tf.keras.Sequential(layers)\n",
    "\n",
    "\n",
    "def create_optimizer(trial):\n",
    "    # Loguniform parameter\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-7, 1e-2)\n",
    "\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def trainer(trial, x_train, y_train):\n",
    "\n",
    "    batch_size = trial.suggest_categorical('batch_size', [256, 512, 1024])\n",
    "    \n",
    "    # さっき作った、ハイパーパラメータを引数に取り、モデルを返す関数\n",
    "    model = create_model(trial)\n",
    "\n",
    "    # ハイパーパラメータを引数に取り、最適化手法を返す関数\n",
    "    optimizer = create_optimizer(trial)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit(x=x_train,\n",
    "              y=y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=10,\n",
    "              verbose=0,\n",
    "#               callbacks=[F1Callback(model, x_test, y_test)]\n",
    "              )\n",
    "\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    splits=5\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    kf = KFold(n_splits=splits, shuffle=True)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "#         model.fit(X_train, y_train)\n",
    "        model = trainer(trial, X_train.values, y_train.values)\n",
    "        scores.append(f1_score(np.where(model.predict(X_test.values)>0.5,1,0),y_test.values))\n",
    "\n",
    "    score = np.array(scores).mean()\n",
    "#     model = trainer(trial, X_train.values, y_train.values)\n",
    "    \n",
    "\n",
    "    return 1-score\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=1000)\n",
    "\n",
    "print(\"best params: \", study.best_params)\n",
    "print(\"best test accuracy: \", 1 - study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2423    0.0\n",
       "2262    1.0\n",
       "596     0.0\n",
       "564     0.0\n",
       "1114    0.0\n",
       "       ... \n",
       "835     0.0\n",
       "3264    0.0\n",
       "1653    0.0\n",
       "2607    0.0\n",
       "2732    0.0\n",
       "Name: author, Length: 2318, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3312 samples\n",
      "Epoch 1/20\n",
      "3312/3312 [==============================] - ETA: 7s - loss: 1.1517 - accuracy: 0.05 - ETA: 3s - loss: 0.7227 - accuracy: 0.48 - ETA: 1s - loss: 0.5343 - accuracy: 0.70 - ETA: 1s - loss: 0.4734 - accuracy: 0.75 - ETA: 1s - loss: 0.4843 - accuracy: 0.77 - ETA: 0s - loss: 0.4650 - accuracy: 0.80 - ETA: 0s - loss: 0.4500 - accuracy: 0.81 - ETA: 0s - loss: 0.4283 - accuracy: 0.83 - ETA: 0s - loss: 0.4465 - accuracy: 0.83 - ETA: 0s - loss: 0.4343 - accuracy: 0.84 - ETA: 0s - loss: 0.4437 - accuracy: 0.85 - 1s 439us/sample - loss: 0.4392 - accuracy: 0.8551\n",
      "Epoch 2/20\n",
      "3312/3312 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.92 - ETA: 0s - loss: 0.3194 - accuracy: 0.91 - ETA: 0s - loss: 0.2834 - accuracy: 0.92 - ETA: 0s - loss: 0.2730 - accuracy: 0.92 - ETA: 0s - loss: 0.2655 - accuracy: 0.93 - ETA: 0s - loss: 0.2699 - accuracy: 0.92 - ETA: 0s - loss: 0.2673 - accuracy: 0.92 - ETA: 0s - loss: 0.2769 - accuracy: 0.92 - ETA: 0s - loss: 0.2776 - accuracy: 0.92 - ETA: 0s - loss: 0.2789 - accuracy: 0.92 - ETA: 0s - loss: 0.2815 - accuracy: 0.92 - ETA: 0s - loss: 0.2808 - accuracy: 0.92 - 1s 322us/sample - loss: 0.2788 - accuracy: 0.9239\n",
      "Epoch 3/20\n",
      "3312/3312 [==============================] - ETA: 1s - loss: 0.3009 - accuracy: 0.90 - ETA: 0s - loss: 0.2567 - accuracy: 0.92 - ETA: 0s - loss: 0.2465 - accuracy: 0.92 - ETA: 0s - loss: 0.2477 - accuracy: 0.92 - ETA: 0s - loss: 0.2626 - accuracy: 0.92 - ETA: 0s - loss: 0.2538 - accuracy: 0.92 - ETA: 0s - loss: 0.2623 - accuracy: 0.92 - ETA: 0s - loss: 0.2549 - accuracy: 0.92 - ETA: 0s - loss: 0.2547 - accuracy: 0.92 - ETA: 0s - loss: 0.2556 - accuracy: 0.92 - ETA: 0s - loss: 0.2481 - accuracy: 0.92 - ETA: 0s - loss: 0.2488 - accuracy: 0.92 - 1s 334us/sample - loss: 0.2474 - accuracy: 0.9239\n",
      "Epoch 4/20\n",
      "3312/3312 [==============================] - ETA: 1s - loss: 0.2065 - accuracy: 0.92 - ETA: 1s - loss: 0.1890 - accuracy: 0.92 - ETA: 0s - loss: 0.1843 - accuracy: 0.92 - ETA: 0s - loss: 0.1776 - accuracy: 0.93 - ETA: 0s - loss: 0.1712 - accuracy: 0.93 - ETA: 0s - loss: 0.1616 - accuracy: 0.93 - ETA: 0s - loss: 0.1636 - accuracy: 0.93 - ETA: 0s - loss: 0.1611 - accuracy: 0.92 - ETA: 0s - loss: 0.1556 - accuracy: 0.92 - ETA: 0s - loss: 0.1525 - accuracy: 0.92 - ETA: 0s - loss: 0.1502 - accuracy: 0.92 - ETA: 0s - loss: 0.1468 - accuracy: 0.92 - 1s 309us/sample - loss: 0.1425 - accuracy: 0.9245\n",
      "Epoch 5/20\n",
      "3312/3312 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.98 - ETA: 0s - loss: 0.0694 - accuracy: 0.99 - ETA: 0s - loss: 0.0676 - accuracy: 0.99 - ETA: 0s - loss: 0.0647 - accuracy: 0.99 - ETA: 0s - loss: 0.0614 - accuracy: 0.99 - ETA: 0s - loss: 0.0605 - accuracy: 0.99 - ETA: 0s - loss: 0.0576 - accuracy: 0.99 - ETA: 0s - loss: 0.0555 - accuracy: 0.99 - ETA: 0s - loss: 0.0538 - accuracy: 0.99 - ETA: 0s - loss: 0.0520 - accuracy: 0.99 - ETA: 0s - loss: 0.0499 - accuracy: 0.99 - ETA: 0s - loss: 0.0483 - accuracy: 0.99 - 1s 298us/sample - loss: 0.0469 - accuracy: 0.9988\n",
      "Epoch 6/20\n",
      "3312/3312 [==============================] - ETA: 1s - loss: 0.0228 - accuracy: 1.00 - ETA: 0s - loss: 0.0248 - accuracy: 1.00 - ETA: 0s - loss: 0.0234 - accuracy: 1.00 - ETA: 0s - loss: 0.0228 - accuracy: 1.00 - ETA: 0s - loss: 0.0223 - accuracy: 1.00 - ETA: 0s - loss: 0.0217 - accuracy: 1.00 - ETA: 0s - loss: 0.0210 - accuracy: 1.00 - ETA: 0s - loss: 0.0208 - accuracy: 1.00 - ETA: 0s - loss: 0.0199 - accuracy: 1.00 - ETA: 0s - loss: 0.0196 - accuracy: 1.00 - ETA: 0s - loss: 0.0192 - accuracy: 1.00 - ETA: 0s - loss: 0.0187 - accuracy: 1.00 - 1s 314us/sample - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "3312/3312 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 1.00 - ETA: 0s - loss: 0.0126 - accuracy: 1.00 - ETA: 0s - loss: 0.0123 - accuracy: 1.00 - ETA: 0s - loss: 0.0121 - accuracy: 1.00 - ETA: 0s - loss: 0.0115 - accuracy: 1.00 - ETA: 0s - loss: 0.0115 - accuracy: 1.00 - ETA: 0s - loss: 0.0113 - accuracy: 1.00 - ETA: 0s - loss: 0.0110 - accuracy: 1.00 - ETA: 0s - loss: 0.0108 - accuracy: 1.00 - ETA: 0s - loss: 0.0106 - accuracy: 1.00 - ETA: 0s - loss: 0.0104 - accuracy: 1.00 - ETA: 0s - loss: 0.0101 - accuracy: 1.00 - 1s 280us/sample - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "3312/3312 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 1.00 - ETA: 0s - loss: 0.0078 - accuracy: 1.00 - ETA: 0s - loss: 0.0074 - accuracy: 1.00 - ETA: 0s - loss: 0.0072 - accuracy: 1.00 - ETA: 0s - loss: 0.0074 - accuracy: 1.00 - ETA: 0s - loss: 0.0073 - accuracy: 1.00 - ETA: 0s - loss: 0.0072 - accuracy: 1.00 - ETA: 0s - loss: 0.0070 - accuracy: 1.00 - ETA: 0s - loss: 0.0068 - accuracy: 1.00 - ETA: 0s - loss: 0.0068 - accuracy: 1.00 - ETA: 0s - loss: 0.0068 - accuracy: 1.00 - ETA: 0s - loss: 0.0067 - accuracy: 1.00 - 1s 302us/sample - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "3312/3312 [==============================] - ETA: 1s - loss: 0.0057 - accuracy: 1.00 - ETA: 0s - loss: 0.0057 - accuracy: 1.00 - ETA: 0s - loss: 0.0053 - accuracy: 1.00 - ETA: 0s - loss: 0.0054 - accuracy: 1.00 - ETA: 0s - loss: 0.0052 - accuracy: 1.00 - ETA: 0s - loss: 0.0052 - accuracy: 1.00 - ETA: 0s - loss: 0.0052 - accuracy: 1.00 - ETA: 0s - loss: 0.0052 - accuracy: 1.00 - ETA: 0s - loss: 0.0052 - accuracy: 1.00 - ETA: 0s - loss: 0.0051 - accuracy: 1.00 - ETA: 0s - loss: 0.0050 - accuracy: 1.00 - ETA: 0s - loss: 0.0050 - accuracy: 1.00 - 1s 330us/sample - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "3312/3312 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 1.00 - ETA: 0s - loss: 0.0044 - accuracy: 1.00 - ETA: 0s - loss: 0.0043 - accuracy: 1.00 - ETA: 0s - loss: 0.0043 - accuracy: 1.00 - ETA: 0s - loss: 0.0041 - accuracy: 1.00 - ETA: 0s - loss: 0.0042 - accuracy: 1.00 - ETA: 0s - loss: 0.0042 - accuracy: 1.00 - ETA: 0s - loss: 0.0041 - accuracy: 1.00 - ETA: 0s - loss: 0.0040 - accuracy: 1.00 - ETA: 0s - loss: 0.0040 - accuracy: 1.00 - ETA: 0s - loss: 0.0039 - accuracy: 1.00 - 1s 284us/sample - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "3312/3312 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 1.00 - ETA: 0s - loss: 0.0035 - accuracy: 1.00 - ETA: 0s - loss: 0.0032 - accuracy: 1.00 - ETA: 0s - loss: 0.0032 - accuracy: 1.00 - ETA: 0s - loss: 0.0031 - accuracy: 1.00 - ETA: 0s - loss: 0.0031 - accuracy: 1.00 - ETA: 0s - loss: 0.0031 - accuracy: 1.00 - ETA: 0s - loss: 0.0031 - accuracy: 1.00 - ETA: 0s - loss: 0.0031 - accuracy: 1.00 - ETA: 0s - loss: 0.0031 - accuracy: 1.00 - ETA: 0s - loss: 0.0032 - accuracy: 1.00 - ETA: 0s - loss: 0.0032 - accuracy: 1.00 - 1s 285us/sample - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "3312/3312 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 1.00 - ETA: 0s - loss: 0.0027 - accuracy: 1.00 - ETA: 0s - loss: 0.0029 - accuracy: 1.00 - ETA: 0s - loss: 0.0028 - accuracy: 1.00 - ETA: 0s - loss: 0.0028 - accuracy: 1.00 - ETA: 0s - loss: 0.0029 - accuracy: 1.00 - ETA: 0s - loss: 0.0028 - accuracy: 1.00 - ETA: 0s - loss: 0.0028 - accuracy: 1.00 - ETA: 0s - loss: 0.0028 - accuracy: 1.00 - ETA: 0s - loss: 0.0027 - accuracy: 1.00 - ETA: 0s - loss: 0.0027 - accuracy: 1.00 - ETA: 0s - loss: 0.0027 - accuracy: 1.00 - 1s 331us/sample - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "3312/3312 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 1.00 - ETA: 0s - loss: 0.0028 - accuracy: 1.00 - ETA: 0s - loss: 0.0026 - accuracy: 1.00 - ETA: 0s - loss: 0.0025 - accuracy: 1.00 - ETA: 0s - loss: 0.0025 - accuracy: 1.00 - ETA: 0s - loss: 0.0024 - accuracy: 1.00 - ETA: 0s - loss: 0.0024 - accuracy: 1.00 - ETA: 0s - loss: 0.0024 - accuracy: 1.00 - ETA: 0s - loss: 0.0023 - accuracy: 1.00 - ETA: 0s - loss: 0.0023 - accuracy: 1.00 - ETA: 0s - loss: 0.0023 - accuracy: 1.00 - ETA: 0s - loss: 0.0023 - accuracy: 1.00 - 1s 278us/sample - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3312/3312 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.00 - ETA: 0s - loss: 0.0020 - accuracy: 1.00 - ETA: 0s - loss: 0.0021 - accuracy: 1.00 - ETA: 0s - loss: 0.0021 - accuracy: 1.00 - ETA: 0s - loss: 0.0021 - accuracy: 1.00 - ETA: 0s - loss: 0.0021 - accuracy: 1.00 - ETA: 0s - loss: 0.0021 - accuracy: 1.00 - ETA: 0s - loss: 0.0020 - accuracy: 1.00 - ETA: 0s - loss: 0.0020 - accuracy: 1.00 - ETA: 0s - loss: 0.0020 - accuracy: 1.00 - ETA: 0s - loss: 0.0020 - accuracy: 1.00 - ETA: 0s - loss: 0.0020 - accuracy: 1.00 - 1s 306us/sample - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "3312/3312 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 1.00 - ETA: 0s - loss: 0.0018 - accuracy: 1.00 - ETA: 0s - loss: 0.0019 - accuracy: 1.00 - ETA: 0s - loss: 0.0019 - accuracy: 1.00 - ETA: 0s - loss: 0.0018 - accuracy: 1.00 - ETA: 0s - loss: 0.0018 - accuracy: 1.00 - ETA: 0s - loss: 0.0018 - accuracy: 1.00 - ETA: 0s - loss: 0.0018 - accuracy: 1.00 - ETA: 0s - loss: 0.0018 - accuracy: 1.00 - ETA: 0s - loss: 0.0018 - accuracy: 1.00 - ETA: 0s - loss: 0.0018 - accuracy: 1.00 - ETA: 0s - loss: 0.0017 - accuracy: 1.00 - 1s 296us/sample - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "3312/3312 [==============================] - ETA: 1s - loss: 0.0017 - accuracy: 1.00 - ETA: 0s - loss: 0.0017 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - 1s 276us/sample - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "3312/3312 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0014 - accuracy: 1.00 - ETA: 0s - loss: 0.0014 - accuracy: 1.00 - ETA: 0s - loss: 0.0014 - accuracy: 1.00 - ETA: 0s - loss: 0.0014 - accuracy: 1.00 - ETA: 0s - loss: 0.0014 - accuracy: 1.00 - ETA: 0s - loss: 0.0014 - accuracy: 1.00 - ETA: 0s - loss: 0.0014 - accuracy: 1.00 - ETA: 0s - loss: 0.0014 - accuracy: 1.00 - 1s 285us/sample - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "3312/3312 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0013 - accuracy: 1.00 - ETA: 0s - loss: 0.0012 - accuracy: 1.00 - ETA: 0s - loss: 0.0012 - accuracy: 1.00 - ETA: 0s - loss: 0.0012 - accuracy: 1.00 - ETA: 0s - loss: 0.0013 - accuracy: 1.00 - ETA: 0s - loss: 0.0013 - accuracy: 1.00 - ETA: 0s - loss: 0.0013 - accuracy: 1.00 - ETA: 0s - loss: 0.0013 - accuracy: 1.00 - ETA: 0s - loss: 0.0012 - accuracy: 1.00 - ETA: 0s - loss: 0.0013 - accuracy: 1.00 - ETA: 0s - loss: 0.0013 - accuracy: 1.00 - 1s 295us/sample - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "3312/3312 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0010 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - 1s 323us/sample - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "3312/3312 [==============================] - ETA: 1s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0012 - accuracy: 1.00 - ETA: 0s - loss: 0.0012 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0010 - accuracy: 1.00 - ETA: 0s - loss: 0.0010 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0010 - accuracy: 1.00 - ETA: 0s - loss: 0.0010 - accuracy: 1.00 - ETA: 0s - loss: 0.0010 - accuracy: 1.00 - 1s 349us/sample - loss: 0.0010 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x354bc4f90>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "# num of hidden layer\n",
    "n_layers = 3\n",
    "\n",
    "activation =  'sigmoid'\n",
    "# dropout_rate\n",
    "dropout_rate = 0.3572429090177327\n",
    "learning_rate =  0.00988694316825537\n",
    "batch_size = 256\n",
    "layers = []\n",
    "for i in range(n_layers):\n",
    "    layers.append(\n",
    "        tf.keras.layers.Dense(128,  activation=activation)\n",
    "    )\n",
    "layers.append(\n",
    "    tf.keras.layers.Dense(1,  activation=activation)\n",
    ")\n",
    "\n",
    "tf.keras.Sequential(layers)\n",
    "\n",
    "\n",
    "\n",
    "# さっき作った、ハイパーパラメータを引数に取り、モデルを返す関数\n",
    "model = tf.keras.Sequential(layers)\n",
    "\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "# ハイパーパラメータを引数に取り、最適化手法を返す関数\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x=X,\n",
    "          y=y.values,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "#           verbose=0,\n",
    "          )\n",
    "# model.fit(x=train_X,\n",
    "#           y=train_y.values,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=20,\n",
    "# #           verbose=0,\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[919   0]\n",
      " [  4  71]]\n",
      "accuracy =  0.9959758551307847\n",
      "precision =  1.0\n",
      "recall =  0.9466666666666667\n",
      "f1 score =  0.9726027397260273\n"
     ]
    }
   ],
   "source": [
    "# pred=model.predict(test_X)\n",
    "# pred=pred.reshape(1,994)[0]\n",
    "# pred = np.where((pred)>0.45,1,0)\n",
    "# print('confusion matrix = \\n', confusion_matrix(y_true=test_y, y_pred=pred))\n",
    "# print('accuracy = ', accuracy_score(y_true=test_y, y_pred=pred))\n",
    "# print('precision = ', precision_score(y_true=test_y, y_pred=pred))\n",
    "# print('recall = ', recall_score(y_true=test_y, y_pred=pred))\n",
    "# print('f1 score = ', f1_score(y_true=test_y, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.0539761e-04, 2.0536780e-04, 2.0691752e-04, 2.0495057e-04,\n",
       "       2.0825863e-04, 2.0578504e-04, 9.9833214e-01, 2.1040440e-04,\n",
       "       2.0444393e-04, 2.0730495e-04, 2.3564696e-04, 2.0539761e-04,\n",
       "       2.0915270e-04, 2.0521879e-04, 2.0518899e-04, 2.0733476e-04,\n",
       "       2.0405650e-04, 2.0954013e-04, 2.0352006e-04, 2.0486116e-04,\n",
       "       2.0951033e-04, 2.0372868e-04, 2.2044778e-04, 2.0435452e-04,\n",
       "       2.0420551e-04, 2.2089481e-04, 2.0959973e-04, 2.0489097e-04,\n",
       "       2.0447373e-04, 2.0420551e-04, 9.9827641e-01, 1.3556182e-03,\n",
       "       2.0828843e-04, 2.0441413e-04, 2.0426512e-04, 2.0655990e-04,\n",
       "       2.0658970e-04, 2.0653009e-04, 2.0590425e-04, 2.2783875e-04,\n",
       "       2.0867586e-04, 2.0819902e-04, 2.0474195e-04, 2.0390749e-04,\n",
       "       2.0539761e-04, 2.0900369e-04, 3.2076240e-04, 2.0495057e-04,\n",
       "       9.9825764e-01, 2.0709634e-04, 2.0429492e-04, 2.0602345e-04,\n",
       "       2.0354986e-04, 2.0438433e-04, 2.0503998e-04, 2.0465255e-04,\n",
       "       2.0578504e-04, 2.0593405e-04, 2.0623207e-04, 2.0357966e-04,\n",
       "       2.0465255e-04, 2.1502376e-04, 2.0697713e-04, 2.0644069e-04,\n",
       "       2.0527840e-04, 2.0515919e-04, 9.9825251e-01, 2.0366907e-04,\n",
       "       2.0411611e-04, 2.0557642e-04, 2.0483136e-04, 2.0501018e-04,\n",
       "       2.0891428e-04, 2.0354986e-04, 2.0465255e-04, 2.0757318e-04,\n",
       "       2.2268295e-04, 2.0423532e-04, 2.0590425e-04, 2.0673871e-04,\n",
       "       2.0667911e-04, 2.0644069e-04, 2.0569563e-04, 2.1383166e-04,\n",
       "       2.5519729e-04, 2.0572543e-04, 2.0614266e-04, 2.0438433e-04,\n",
       "       2.8574467e-04, 2.0891428e-04, 2.0512938e-04, 2.0623207e-04,\n",
       "       2.0608306e-04, 2.0617247e-04, 2.0518899e-04, 2.0557642e-04,\n",
       "       2.0682812e-04, 2.1022558e-04, 2.0414591e-04, 2.0402670e-04,\n",
       "       2.0596385e-04, 2.0372868e-04, 2.0483136e-04, 2.0501018e-04,\n",
       "       2.0396709e-04, 2.2736192e-04, 2.0369887e-04, 2.3087859e-04,\n",
       "       2.0736456e-04, 2.0420551e-04, 2.0778179e-04, 2.1186471e-04,\n",
       "       9.9561274e-01, 2.0435452e-04, 2.1129847e-04, 9.9835169e-01,\n",
       "       2.0557642e-04, 2.5179982e-04, 2.0453334e-04, 2.0414591e-04,\n",
       "       2.0545721e-04, 2.1019578e-04, 9.9831861e-01, 2.0495057e-04,\n",
       "       2.0405650e-04, 2.0366907e-04, 2.1123886e-04, 2.0438433e-04,\n",
       "       2.0426512e-04, 2.0465255e-04, 2.0474195e-04, 2.0462275e-04,\n",
       "       2.0596385e-04, 2.0435452e-04, 2.0480156e-04, 2.0489097e-04,\n",
       "       2.0545721e-04, 2.0429492e-04, 2.0539761e-04, 2.0673871e-04,\n",
       "       2.0554662e-04, 2.1776557e-04, 2.0396709e-04, 2.0393729e-04,\n",
       "       2.0620227e-04, 2.0632148e-04, 2.0837784e-04, 2.0429492e-04,\n",
       "       2.0462275e-04, 2.0536780e-04, 2.0447373e-04, 2.0357966e-04,\n",
       "       2.0763278e-04, 2.1019578e-04, 2.0408630e-04, 2.0438433e-04,\n",
       "       2.0697713e-04, 2.0587444e-04, 2.0468235e-04, 2.0992756e-04,\n",
       "       2.0605326e-04, 9.9814653e-01, 2.0390749e-04, 2.0408630e-04,\n",
       "       2.0495057e-04, 2.0366907e-04, 9.9828482e-01, 2.0793080e-04,\n",
       "       2.0569563e-04, 2.0697713e-04, 2.0512938e-04, 2.0465255e-04,\n",
       "       2.0557642e-04, 2.0393729e-04, 2.0736456e-04, 2.0429492e-04,\n",
       "       9.9834734e-01, 2.1255016e-04, 2.0590425e-04, 2.0378828e-04,\n",
       "       2.0644069e-04, 2.0834804e-04, 2.0825863e-04, 2.0909309e-04,\n",
       "       2.0611286e-04, 2.0444393e-04, 2.0474195e-04, 2.0515919e-04,\n",
       "       2.0486116e-04, 2.2113323e-04, 2.1201372e-04, 2.0468235e-04,\n",
       "       2.0620227e-04, 2.0548701e-04, 2.0423532e-04, 2.0638108e-04,\n",
       "       2.0575523e-04, 2.0566583e-04, 9.9826074e-01, 2.0343065e-04,\n",
       "       2.0644069e-04, 2.3934245e-04, 2.0840764e-04, 2.0533800e-04,\n",
       "       2.1597743e-04, 2.0366907e-04, 2.0843744e-04, 2.0420551e-04,\n",
       "       2.0495057e-04, 2.0629168e-04, 2.0465255e-04, 2.0539761e-04,\n",
       "       2.0435452e-04, 2.0748377e-04, 2.0369887e-04, 2.1257997e-04,\n",
       "       2.0533800e-04, 2.0438433e-04, 2.0596385e-04, 2.0739436e-04,\n",
       "       9.9816406e-01, 2.8955936e-04, 2.0459294e-04, 2.2035837e-04,\n",
       "       2.0697713e-04, 2.0441413e-04, 9.9763900e-01, 2.0664930e-04,\n",
       "       2.0554662e-04, 2.0509958e-04, 2.1547079e-04, 2.0399690e-04,\n",
       "       2.0757318e-04, 3.7831068e-04, 2.1499395e-04, 1.1820167e-02,\n",
       "       2.0575523e-04, 2.0426512e-04, 2.0438433e-04, 2.0721555e-04,\n",
       "       2.0629168e-04, 2.0539761e-04, 2.0423532e-04, 2.0584464e-04,\n",
       "       2.0480156e-04, 2.0566583e-04, 9.9310994e-01, 2.0602345e-04,\n",
       "       2.0632148e-04, 2.0414591e-04, 2.0706654e-04, 2.0793080e-04,\n",
       "       2.0772219e-04, 2.0501018e-04, 2.0426512e-04, 2.0927191e-04,\n",
       "       2.0411611e-04, 2.0435452e-04, 3.3688545e-04, 2.1722913e-04,\n",
       "       2.0822883e-04, 2.0658970e-04, 2.0441413e-04, 2.0784140e-04,\n",
       "       2.0748377e-04, 2.0352006e-04, 2.1579862e-04, 2.0536780e-04,\n",
       "       2.0444393e-04, 2.0405650e-04, 2.0980835e-04, 2.0417571e-04,\n",
       "       2.0447373e-04, 2.0471215e-04, 2.0802021e-04, 2.0498037e-04,\n",
       "       2.0855665e-04, 2.0810962e-04, 2.0831823e-04, 2.0888448e-04,\n",
       "       2.0709634e-04, 2.0578504e-04, 9.9739635e-01, 2.0897388e-04,\n",
       "       2.4047494e-04, 2.0417571e-04, 2.0474195e-04, 2.0369887e-04,\n",
       "       2.0670891e-04, 2.0560622e-04, 2.0533800e-04, 2.0772219e-04,\n",
       "       2.2298098e-04, 2.1815300e-04, 9.2994666e-01, 2.0846725e-04,\n",
       "       2.0471215e-04, 2.0524859e-04, 2.0617247e-04, 2.0903349e-04,\n",
       "       7.2369140e-01, 2.0411611e-04, 2.1496415e-04, 2.0676851e-04,\n",
       "       9.9664527e-01, 2.0480156e-04, 9.9836910e-01, 2.0408630e-04,\n",
       "       2.0393729e-04, 2.6547909e-04, 2.0483136e-04, 9.9824989e-01,\n",
       "       2.0870566e-04, 2.0822883e-04, 2.0509958e-04, 2.0354986e-04,\n",
       "       2.0435452e-04, 2.0673871e-04, 2.0539761e-04, 2.0998716e-04,\n",
       "       9.9829692e-01, 2.0483136e-04, 2.0536780e-04, 2.0688772e-04,\n",
       "       2.0593405e-04, 2.0527840e-04, 2.0843744e-04, 2.0429492e-04,\n",
       "       2.0870566e-04, 2.0623207e-04, 2.0563602e-04, 2.1108985e-04,\n",
       "       9.6681786e-01, 2.0676851e-04, 2.0483136e-04, 2.0566583e-04,\n",
       "       2.0483136e-04, 2.0480156e-04, 9.9785292e-01, 2.0712614e-04,\n",
       "       2.0426512e-04, 2.0435452e-04, 2.0557642e-04, 2.0667911e-04,\n",
       "       2.0790100e-04, 2.0548701e-04, 2.2447109e-04, 2.0498037e-04,\n",
       "       2.0641088e-04, 2.0384789e-04, 9.9812281e-01, 2.0605326e-04,\n",
       "       2.0474195e-04, 2.0453334e-04, 2.0352006e-04, 2.1448731e-04,\n",
       "       2.0378828e-04, 9.9823141e-01, 2.1070242e-04, 2.0375848e-04,\n",
       "       2.0354986e-04, 2.1477848e-02, 2.0653009e-04, 2.0393729e-04,\n",
       "       2.0369887e-04, 9.9820733e-01, 2.0480156e-04, 2.0816922e-04,\n",
       "       2.0399690e-04, 2.0527840e-04, 9.9818814e-01, 2.0471215e-04,\n",
       "       2.0548701e-04, 2.2402406e-04, 2.0998716e-04, 2.0426512e-04,\n",
       "       2.1138787e-04, 2.0617247e-04, 2.0694733e-04, 2.0575523e-04,\n",
       "       2.0503998e-04, 2.0647049e-04, 2.0533800e-04, 2.1037459e-04,\n",
       "       2.0602345e-04, 2.0468235e-04, 2.0542741e-04, 2.1862984e-04,\n",
       "       2.0509958e-04, 2.0462275e-04, 2.0429492e-04, 2.0489097e-04,\n",
       "       2.0527840e-04, 9.9826330e-01, 2.0447373e-04, 2.0468235e-04,\n",
       "       2.0682812e-04, 2.0420551e-04, 9.9802244e-01, 9.9817288e-01,\n",
       "       2.0372868e-04, 2.0670891e-04, 2.0372868e-04, 2.0468235e-04,\n",
       "       2.0518899e-04, 2.0435452e-04, 2.2420287e-04, 2.0828843e-04,\n",
       "       2.0450354e-04, 2.0599365e-04, 9.7499371e-01, 2.0635128e-04,\n",
       "       2.0503998e-04, 2.0548701e-04, 2.0492077e-04, 2.0444393e-04,\n",
       "       2.0635128e-04, 2.0614266e-04, 2.0426512e-04, 2.2685528e-04,\n",
       "       2.0501018e-04, 2.0623207e-04, 2.0429492e-04, 2.0635128e-04,\n",
       "       2.0381808e-04, 2.0441413e-04, 2.0390749e-04, 2.0647049e-04,\n",
       "       2.0572543e-04, 2.0703673e-04, 2.1830201e-04, 2.0414591e-04,\n",
       "       2.0933151e-04, 2.0870566e-04, 2.0503998e-04, 2.0763278e-04,\n",
       "       2.0775199e-04, 9.9830902e-01, 2.0378828e-04, 2.0542741e-04,\n",
       "       2.0480156e-04, 2.0444393e-04, 9.9815333e-01, 2.0882487e-04,\n",
       "       2.0408630e-04, 2.0593405e-04, 2.0787120e-04, 2.0682812e-04,\n",
       "       2.0381808e-04, 2.0644069e-04, 2.0554662e-04, 2.0501018e-04,\n",
       "       2.1484494e-04, 2.0608306e-04, 9.9826419e-01, 2.0369887e-04,\n",
       "       2.0539761e-04, 2.0653009e-04, 2.0352006e-04, 2.0474195e-04,\n",
       "       2.0593405e-04, 2.0805001e-04, 2.0629168e-04, 9.9834877e-01,\n",
       "       2.0670891e-04, 2.1103024e-04, 2.0614266e-04, 2.1064281e-04,\n",
       "       2.0509958e-04, 9.9826550e-01, 2.0372868e-04, 2.0608306e-04,\n",
       "       2.0456314e-04, 2.0533800e-04, 2.0360947e-04, 2.1094084e-04,\n",
       "       2.0673871e-04, 9.9827367e-01, 2.0593405e-04, 2.1213293e-04,\n",
       "       2.0891428e-04, 2.0483136e-04, 2.0450354e-04, 2.0912290e-04,\n",
       "       2.0408630e-04, 2.0489097e-04, 2.1535158e-04, 2.0450354e-04,\n",
       "       2.0715594e-04, 2.0530820e-04, 2.0730495e-04, 2.0447373e-04,\n",
       "       2.0530820e-04, 9.9831605e-01, 2.0453334e-04, 2.0423532e-04,\n",
       "       2.0402670e-04, 2.0912290e-04, 2.0849705e-04, 2.0763278e-04,\n",
       "       2.5844574e-04, 2.0444393e-04, 2.0653009e-04, 2.0432472e-04,\n",
       "       2.0897388e-04, 2.0596385e-04, 2.0623207e-04, 1.8744379e-02,\n",
       "       2.0384789e-04, 2.0408630e-04, 9.9829608e-01, 2.0727515e-04,\n",
       "       2.0480156e-04, 2.1314621e-04, 9.9836576e-01, 2.0551682e-04,\n",
       "       2.0599365e-04, 2.0450354e-04, 7.9370373e-01, 2.0983815e-04,\n",
       "       2.0653009e-04, 3.6424398e-04, 2.0575523e-04, 2.1192431e-04,\n",
       "       2.0566583e-04, 2.0387769e-04, 9.9823910e-01, 2.0375848e-04,\n",
       "       9.9824274e-01, 2.0483136e-04, 2.0924211e-04, 2.0715594e-04,\n",
       "       2.0727515e-04, 2.0575523e-04, 2.0605326e-04, 2.0414591e-04,\n",
       "       9.9835753e-01, 2.0405650e-04, 3.3473969e-04, 9.9833435e-01,\n",
       "       2.0486116e-04, 2.0590425e-04, 2.0524859e-04, 5.0631166e-04,\n",
       "       2.0790100e-04, 2.1395087e-04, 2.0390749e-04, 2.0444393e-04,\n",
       "       2.0506978e-04, 2.0486116e-04, 2.0471215e-04, 2.0495057e-04,\n",
       "       9.9814284e-01, 2.0858645e-04, 2.0435452e-04, 2.0724535e-04,\n",
       "       9.5027071e-01, 2.0727515e-04, 2.0691752e-04, 2.1103024e-04,\n",
       "       2.0402670e-04, 2.5916100e-04, 2.0405650e-04, 2.2515655e-04,\n",
       "       2.0444393e-04, 2.0578504e-04, 2.0724535e-04, 2.0772219e-04,\n",
       "       2.0843744e-04, 2.0968914e-04, 2.0450354e-04, 2.1308661e-04,\n",
       "       2.0965934e-04, 2.0447373e-04, 9.9826682e-01, 2.0462275e-04,\n",
       "       9.9812138e-01, 2.1654367e-04, 2.0569563e-04, 2.0477176e-04,\n",
       "       2.0956993e-04, 2.0414591e-04, 2.0825863e-04, 2.0527840e-04,\n",
       "       2.0441413e-04, 2.1082163e-04, 2.0757318e-04, 2.0387769e-04,\n",
       "       2.0423532e-04, 2.0575523e-04, 2.0536780e-04, 2.0548701e-04,\n",
       "       2.0840764e-04, 2.0557642e-04, 2.0512938e-04, 2.0641088e-04,\n",
       "       2.1189451e-04, 2.0384789e-04, 2.0608306e-04, 2.0372868e-04,\n",
       "       2.0575523e-04, 2.0602345e-04, 2.0414591e-04, 2.0724535e-04,\n",
       "       2.0414591e-04, 2.0423532e-04, 2.0384789e-04, 2.2709370e-04,\n",
       "       2.0733476e-04, 2.1249056e-04, 2.0599365e-04, 2.0584464e-04,\n",
       "       2.0423532e-04, 2.0879507e-04, 2.0462275e-04, 2.0453334e-04,\n",
       "       2.0402670e-04, 2.0459294e-04, 2.0414591e-04, 2.0641088e-04,\n",
       "       2.0736456e-04, 2.0608306e-04, 2.1007657e-04, 2.0483136e-04,\n",
       "       2.0360947e-04, 2.1028519e-04, 2.0638108e-04, 2.0354986e-04,\n",
       "       2.0599365e-04, 2.0736456e-04, 2.0766258e-04, 2.0697713e-04,\n",
       "       2.0489097e-04, 2.0796061e-04, 2.1669269e-04, 2.1937490e-04,\n",
       "       9.9835074e-01, 2.0408630e-04, 2.0530820e-04, 2.0840764e-04,\n",
       "       2.0495057e-04, 2.0426512e-04, 2.0459294e-04, 2.0542741e-04,\n",
       "       2.0435452e-04, 9.9797690e-01, 2.0530820e-04, 2.0664930e-04,\n",
       "       2.0438433e-04, 2.0802021e-04, 2.0545721e-04, 2.0757318e-04,\n",
       "       2.5889277e-04, 2.0706654e-04, 2.0989776e-04, 2.0468235e-04,\n",
       "       2.0632148e-04, 2.0483136e-04, 2.0581484e-04, 2.0906329e-04,\n",
       "       2.0420551e-04, 9.9777716e-01, 2.0492077e-04, 2.0584464e-04,\n",
       "       2.0483136e-04, 9.9788976e-01, 2.0503998e-04, 2.0593405e-04,\n",
       "       9.3967277e-01, 2.0569563e-04, 2.0864606e-04, 2.0483136e-04,\n",
       "       2.1222234e-04, 2.0912290e-04, 2.2360682e-04, 2.3153424e-04,\n",
       "       2.9709935e-04, 2.2014976e-04, 2.1615624e-04, 2.0438433e-04,\n",
       "       2.0682812e-04, 2.0557642e-04, 2.0429492e-04, 2.0736456e-04,\n",
       "       2.0423532e-04, 2.0459294e-04, 2.0739436e-04, 2.0423532e-04,\n",
       "       2.0563602e-04, 2.0548701e-04, 9.9835277e-01, 2.0644069e-04,\n",
       "       2.0554662e-04, 9.9822044e-01, 2.1055341e-04, 2.1016598e-04,\n",
       "       2.0521879e-04, 2.0682812e-04, 2.0632148e-04, 2.0426512e-04,\n",
       "       2.0402670e-04, 2.0399690e-04, 2.0593405e-04, 2.5591254e-04,\n",
       "       2.0456314e-04, 2.0414591e-04, 2.0396709e-04, 2.0453334e-04,\n",
       "       2.0596385e-04, 2.1150708e-04, 2.0554662e-04, 2.5650859e-04,\n",
       "       2.0542741e-04, 2.0393729e-04, 2.0444393e-04, 2.0691752e-04,\n",
       "       9.9822295e-01, 2.0694733e-04, 2.0721555e-04, 2.0590425e-04,\n",
       "       2.0545721e-04, 2.0930171e-04, 2.0882487e-04, 2.0453334e-04,\n",
       "       2.0465255e-04, 2.0408630e-04, 2.0694733e-04, 2.0366907e-04,\n",
       "       2.1207333e-04, 2.0501018e-04, 2.0501018e-04, 2.0420551e-04,\n",
       "       2.0980835e-04, 2.0465255e-04, 2.0673871e-04, 2.0697713e-04,\n",
       "       2.0542741e-04, 2.0682812e-04, 2.0396709e-04, 2.0587444e-04,\n",
       "       2.1177530e-04, 2.1639466e-04, 2.0787120e-04, 2.0626187e-04,\n",
       "       2.0575523e-04, 2.0566583e-04, 2.2113323e-04, 2.0530820e-04,\n",
       "       2.0462275e-04, 2.0518899e-04, 9.9836802e-01, 2.0751357e-04,\n",
       "       2.0515919e-04, 2.0480156e-04, 2.0432472e-04, 2.0384789e-04,\n",
       "       2.0614266e-04, 2.0465255e-04, 2.0489097e-04, 2.0760298e-04,\n",
       "       2.0721555e-04, 2.0658970e-04, 2.0477176e-04, 9.9829245e-01,\n",
       "       2.1034479e-04, 2.0393729e-04, 2.0396709e-04, 2.0438433e-04,\n",
       "       2.0462275e-04, 2.0664930e-04, 2.0468235e-04, 2.0405650e-04,\n",
       "       2.0414591e-04, 2.0605326e-04, 2.0375848e-04, 2.1561980e-04,\n",
       "       2.0393729e-04, 2.0673871e-04, 2.0399690e-04, 2.0533800e-04,\n",
       "       2.0501018e-04, 2.0495057e-04, 2.0661950e-04, 1.1139968e-01,\n",
       "       2.2521615e-04, 2.0593405e-04, 2.0539761e-04, 2.0945072e-04,\n",
       "       2.0855665e-04, 2.0703673e-04, 2.0429492e-04, 9.9771839e-01,\n",
       "       2.0736456e-04, 2.0608306e-04, 2.0572543e-04, 2.0533800e-04,\n",
       "       2.0486116e-04, 2.0518899e-04, 2.0712614e-04, 2.0805001e-04,\n",
       "       2.1564960e-04, 2.0539761e-04, 2.0852685e-04, 9.9680054e-01,\n",
       "       2.0483136e-04, 2.0521879e-04, 2.1049380e-04, 2.1329522e-04,\n",
       "       2.5519729e-04, 2.3677945e-04, 2.0524859e-04, 2.0819902e-04,\n",
       "       2.0945072e-04, 9.9267912e-01, 2.0554662e-04, 2.0581484e-04,\n",
       "       2.0447373e-04, 2.1198392e-04, 2.0626187e-04, 2.0584464e-04,\n",
       "       2.0745397e-04, 2.0390749e-04, 2.0444393e-04, 2.0352006e-04,\n",
       "       2.0718575e-04, 2.0363927e-04, 2.6458502e-04, 2.0578504e-04,\n",
       "       2.0501018e-04, 2.0602345e-04, 2.0480156e-04, 2.0354986e-04,\n",
       "       2.0384789e-04, 2.1436810e-04, 2.0638108e-04, 2.0486116e-04,\n",
       "       2.0489097e-04, 2.0369887e-04, 2.0521879e-04, 2.0435452e-04,\n",
       "       2.3457408e-04, 2.0912290e-04, 2.0647049e-04, 2.2679567e-04,\n",
       "       2.0661950e-04, 4.2769313e-04, 2.0658970e-04, 2.0563602e-04,\n",
       "       2.0608306e-04, 2.0444393e-04, 2.0477176e-04, 2.0456314e-04,\n",
       "       2.0524859e-04, 2.0912290e-04, 2.0891428e-04, 2.0518899e-04,\n",
       "       2.0408630e-04, 2.0396709e-04, 2.0638108e-04, 2.0381808e-04,\n",
       "       2.0524859e-04, 2.0796061e-04, 2.0602345e-04, 2.0372868e-04,\n",
       "       2.0518899e-04, 2.1731853e-04, 2.1833181e-04, 2.0495057e-04,\n",
       "       2.0661950e-04, 2.0733476e-04, 2.0575523e-04, 2.0459294e-04,\n",
       "       2.0846725e-04, 2.1553040e-04, 2.0483136e-04, 9.9816763e-01,\n",
       "       2.0554662e-04, 2.0909309e-04, 9.9826109e-01, 9.9785709e-01,\n",
       "       2.0515919e-04, 2.0590425e-04, 2.0506978e-04, 2.0444393e-04,\n",
       "       2.8023124e-04, 2.0402670e-04, 2.0983815e-04, 2.0459294e-04,\n",
       "       2.0423532e-04, 2.0590425e-04, 2.0417571e-04, 2.0810962e-04,\n",
       "       2.0405650e-04, 2.0509958e-04, 2.0483136e-04, 2.0501018e-04,\n",
       "       2.0653009e-04, 9.9817431e-01, 2.0718575e-04, 2.0977855e-04,\n",
       "       2.0617247e-04, 2.0733476e-04, 2.0459294e-04, 2.0682812e-04,\n",
       "       2.0700693e-04, 2.0426512e-04, 2.0620227e-04, 2.0721555e-04,\n",
       "       2.0506978e-04, 2.0971894e-04, 2.0369887e-04, 2.0781159e-04,\n",
       "       2.0843744e-04, 2.0682812e-04, 2.0387769e-04, 2.0468235e-04,\n",
       "       2.0593405e-04, 2.0545721e-04, 2.1982193e-04, 2.0492077e-04,\n",
       "       2.0712614e-04, 2.0435452e-04, 2.0524859e-04, 2.0417571e-04,\n",
       "       2.0438433e-04, 2.0682812e-04, 2.0697713e-04, 2.0503998e-04,\n",
       "       2.0888448e-04, 2.0375848e-04, 2.0390749e-04, 2.0617247e-04,\n",
       "       2.0352006e-04, 2.0527840e-04, 2.0414591e-04, 2.0569563e-04,\n",
       "       2.1177530e-04, 9.9831724e-01, 2.0480156e-04, 2.0393729e-04,\n",
       "       4.9135089e-04, 2.0501018e-04, 2.0369887e-04, 2.0533800e-04,\n",
       "       2.0614266e-04, 2.0831823e-04, 9.9681515e-01, 2.0781159e-04,\n",
       "       2.0423532e-04, 2.1329522e-04, 9.9832737e-01, 2.3519993e-04,\n",
       "       2.2965670e-04, 2.0369887e-04, 2.0802021e-04, 2.0474195e-04,\n",
       "       2.0411611e-04, 2.0575523e-04, 2.0566583e-04, 9.9823797e-01,\n",
       "       2.2444129e-04, 2.0676851e-04, 2.0432472e-04, 9.9832225e-01,\n",
       "       2.2119284e-04, 2.0533800e-04, 2.0533800e-04, 2.0462275e-04,\n",
       "       2.0548701e-04, 2.0489097e-04, 2.0617247e-04, 2.0745397e-04,\n",
       "       2.0736456e-04, 2.0623207e-04, 2.0539761e-04, 2.0787120e-04,\n",
       "       2.0706654e-04, 2.0426512e-04, 2.0462275e-04, 2.0518899e-04,\n",
       "       2.0498037e-04, 2.1511316e-04, 2.0411611e-04, 2.0697713e-04,\n",
       "       2.0563602e-04, 2.0435452e-04, 2.0617247e-04, 2.1025538e-04,\n",
       "       2.0742416e-04, 2.0387769e-04, 2.0399690e-04, 2.0733476e-04,\n",
       "       5.4800447e-02, 2.0370183e-04], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred.reshape(1,994)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test)\n",
    "# pred = model.predict(np.array(test))\n",
    "pred=np.where(pred>0.45,1,0)\n",
    "# pred=pred.reshape(1,1420)\n",
    "sub = pd.DataFrame(pd.read_csv(\"data/test.csv\")['writing_id'])\n",
    "sub[\"author\"] = list(pred)\n",
    "sub.author=sub.author.map(lambda x:x[0])\n",
    "sub.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>writing_id</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>4716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>4719</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>4725</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>4726</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>4727</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1420 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      writing_id  author\n",
       "0              4       0\n",
       "1              5       0\n",
       "2              6       0\n",
       "3             10       0\n",
       "4             11       0\n",
       "...          ...     ...\n",
       "1415        4716       0\n",
       "1416        4719       0\n",
       "1417        4725       0\n",
       "1418        4726       0\n",
       "1419        4727       0\n",
       "\n",
       "[1420 rows x 2 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.reshape(1,1420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
