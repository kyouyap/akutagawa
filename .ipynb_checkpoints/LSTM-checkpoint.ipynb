{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import MeCab\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "# traindf=pd.read_csv(\"data/train.csv\")\n",
    "# testdf=pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "import mojimoji\n",
    "import re\n",
    "import jaconv\n",
    "\n",
    "\n",
    "\n",
    "def parse_text(text, debug=False):\n",
    "    '''\n",
    "    Get location\n",
    "    '''\n",
    "\n",
    "#     text = mojimoji.zen_to_han(text, kana=False)\n",
    "    text = re.sub(r'［[^］]+］', '', text)  \n",
    "    text = re.sub(r'[\\.*[\\]]', '', text)    \n",
    "    text = re.sub(r'[\\.*[\\]]', '', text)\n",
    "    text = re.sub(r'', '', text)    \n",
    "    text = re.sub(r'[\\(（[].*[）\\)]]', '', text)\n",
    "    text = re.sub(r'[\\s、]', '', text)\n",
    "    text = re.sub(r'一', '', text)\n",
    "    text = re.sub(r'…', '', text)  \n",
    "    text = re.sub(r'/＼', '', text)\n",
    "    text = re.sub(r'[0-9]', '0', text)\n",
    "\n",
    "\n",
    "#     text = jaconv.kata2hira(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# df=pd.concat([traindf,testdf])\n",
    "# df.body=df.body.map(parse_text)\n",
    "# traindf.body=traindf.body.map(parse_text)\n",
    "# testdf.body=testdf.body.map(parse_text)\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = normalize_unicode(text)\n",
    "    normalized_text = normalize_number(normalized_text)\n",
    "    normalized_text = lower_text(normalized_text)\n",
    "    return normalized_text\n",
    "\n",
    "\n",
    "def lower_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def normalize_unicode(text, form='NFKC'):\n",
    "    normalized_text = unicodedata.normalize(form, text)\n",
    "    return normalized_text\n",
    "\n",
    "\n",
    "def lemmatize_term(term, pos=None):\n",
    "    if pos is None:\n",
    "        synsets = wordnet.synsets(term)\n",
    "        if not synsets:\n",
    "            return term\n",
    "        pos = synsets[0].pos()\n",
    "        if pos == wordnet.ADJ_SAT:\n",
    "            pos = wordnet.ADJ\n",
    "    return nltk.WordNetLemmatizer().lemmatize(term, pos=pos)\n",
    "\n",
    "\n",
    "def normalize_number(text):\n",
    "    \"\"\"\n",
    "    pattern = r'\\d+'\n",
    "    replacer = re.compile(pattern)\n",
    "    result = replacer.sub('0', text)\n",
    "    \"\"\"\n",
    "    # 連続した数字を0で置換\n",
    "    replaced_text = re.sub(r'\\d+', '0', text)\n",
    "    return replaced_text\n",
    "\n",
    "\n",
    "# mecab = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd/')\n",
    "# mecab=MeCab.Tagger ('-d /usr/local/lib/mecab/dic/UniDic-kindai_1603')\n",
    "mecab = MeCab.Tagger('-Owakati')\n",
    "\n",
    "# 形態素解析をして、名詞だけ取り出す\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "#     available_norm = ['接尾', '一般', '形容動詞語幹', 'サ変接続']\n",
    "    node = mecab.parseToNode(text)\n",
    "    l = []\n",
    "    while node:\n",
    "        l.append(node.surface)\n",
    "        node = node.next\n",
    "    return l\n",
    "\n",
    "\n",
    "# 記事群のdictについて、形態素解析をしてリストに返す\n",
    "def get_words(contents):\n",
    "    available_norm = ['接尾', '一般', '形容動詞語幹', 'サ変接続']\n",
    "    node = mecab.parseToNode(contents)\n",
    "    l = []\n",
    "    while node:\n",
    "\n",
    "        l.append(node.surface)\n",
    "        node = node.next\n",
    "    return l\n",
    "\n",
    "# 一つの記事を形態素解析して返す\n",
    "\n",
    "\n",
    "def get_words_main(content):\n",
    "    return [token for token in tokenize1(content)]\n",
    "\n",
    "\n",
    "def get_words_main1(content):\n",
    "    retoken = \"\"\n",
    "    for token in tokenize(content):\n",
    "        retoken += token+\" \"\n",
    "    return retoken\n",
    "\n",
    "def tokenize1(text):\n",
    "#     available_norm = ['接尾', '一般', '形容動詞語幹', 'サ変接続']\n",
    "    node = mecab.parseToNode(text)\n",
    "    l = []\n",
    "    while node:\n",
    "        l.append(node.surface)\n",
    "        node = node.next\n",
    "    return ' '.join(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "    traindf = pd.read_csv(\"data/train.csv\")\n",
    "    testdf = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "    df = pd.concat([traindf, testdf])\n",
    "    df.body = df.body.map(parse_text)\n",
    "    count = CountVectorizer(analyzer=tokenize)\n",
    "    bags = count.fit_transform(df.body.values)\n",
    "    # print(bags.toarray())\n",
    "\n",
    "    features = count.get_feature_names()\n",
    "    # # print(features)\n",
    "\n",
    "    bodyvec = pd.DataFrame(bags.toarray(), columns=features)\n",
    "    newdf = pd.concat([df.reset_index(drop=True), pd.DataFrame(bodyvec)], axis=1)\n",
    "    train = newdf.dropna().drop([\"writing_id\", \"body\", ], axis=1)\n",
    "    test = newdf[newdf.author.isnull()].drop([\"writing_id\", \"body\"], axis=1)\n",
    "    test = test.drop([\"author\"], axis=1)\n",
    "    X = train.drop([\"author\"], axis=1)\n",
    "    y = train.author\n",
    "    return X, y, test\n",
    "\n",
    "\n",
    "# X, y, test = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess1():\n",
    "    traindf = pd.read_csv(\"data/train.csv\")\n",
    "    testdf = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "    df = pd.concat([traindf, testdf])\n",
    "    df.body = df.body.map(parse_text).map(tokenize1)\n",
    "#     count = CountVectorizer(analyzer=tokenize)\n",
    "#     bags = count.fit_transform(df.body.values)\n",
    "#     # print(bags.toarray())\n",
    "\n",
    "#     features = count.get_feature_names()\n",
    "#     # # print(features)\n",
    "\n",
    "#     bodyvec = pd.DataFrame(bags.toarray(), columns=features)\n",
    "#     newdf = pd.concat([df.reset_index(drop=True), pd.DataFrame(bodyvec)], axis=1)\n",
    "    train = df.dropna().drop([\"writing_id\", ], axis=1)\n",
    "    test = df[df.author.isnull()].drop([\"writing_id\"], axis=1)\n",
    "    test = test.drop([\"author\"], axis=1)\n",
    "    X = train.drop([\"author\"], axis=1)\n",
    "    y = train.author\n",
    "    return X, y, test\n",
    "X, y, test=preprocess1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>先ごろ の 本欄 に 僕 の 「 風 報 」 に かい た 「 天皇陛下 に 捧ぐる 言葉...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>旅 の 眼 に 映じ た 外国 の 正月 を といふ お 需 め で 昔 前 の 記憶 か...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>或 る 心持 の よい 夕方 日比谷公園 の 樹 の 繁み の 間 で 若葉 楓 の 梢 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>島々 《 しま ／ ＼ 》 と 云 ふ 町 の 宿屋 へ 着い た の は 午 過ぎ ――...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>或 る 田舎 に 母 と 子 と が 住ん で い た 。 そして 或 る 年 の 秋 次...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>八 五 郎 の 取柄 は 誰 と で も すぐ 友達 に なれる こと でし た 。 長 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>ある 婦人 が 私 に 言 つ た 。 私 が 情痴 作家 など ゝ 言 は れる こと ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>諸君 は 東京 市 某 町 某 番地 なる 風 博士 の 邸宅 を 御存じ で あろ う ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>「 御免 」 少し 職業 的 に 落着き 拂 つた 聲錢 形 平次 は それ を 聞く と...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>ちやう ど 今日 （ 十月 三 日 ） 文部省 で 著作 家 側 を 招い て 新 カナヅ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3312 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body\n",
       "0      先ごろ の 本欄 に 僕 の 「 風 報 」 に かい た 「 天皇陛下 に 捧ぐる 言葉...\n",
       "1      旅 の 眼 に 映じ た 外国 の 正月 を といふ お 需 め で 昔 前 の 記憶 か...\n",
       "2      或 る 心持 の よい 夕方 日比谷公園 の 樹 の 繁み の 間 で 若葉 楓 の 梢 ...\n",
       "3      島々 《 しま ／ ＼ 》 と 云 ふ 町 の 宿屋 へ 着い た の は 午 過ぎ ――...\n",
       "4      或 る 田舎 に 母 と 子 と が 住ん で い た 。 そして 或 る 年 の 秋 次...\n",
       "...                                                 ...\n",
       "3307   八 五 郎 の 取柄 は 誰 と で も すぐ 友達 に なれる こと でし た 。 長 ...\n",
       "3308   ある 婦人 が 私 に 言 つ た 。 私 が 情痴 作家 など ゝ 言 は れる こと ...\n",
       "3309   諸君 は 東京 市 某 町 某 番地 なる 風 博士 の 邸宅 を 御存じ で あろ う ...\n",
       "3310   「 御免 」 少し 職業 的 に 落着き 拂 つた 聲錢 形 平次 は それ を 聞く と...\n",
       "3311   ちやう ど 今日 （ 十月 三 日 ） 文部省 で 著作 家 側 を 招い て 新 カナヅ...\n",
       "\n",
       "[3312 rows x 1 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['body']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        先ごろ の 本欄 に 僕 の 「 風 報 」 に かい た 「 天皇陛下 に 捧ぐる 言葉...\n",
       "1        旅 の 眼 に 映じ た 外国 の 正月 を といふ お 需 め で 昔 前 の 記憶 か...\n",
       "2        或 る 心持 の よい 夕方 日比谷公園 の 樹 の 繁み の 間 で 若葉 楓 の 梢 ...\n",
       "3        島々 《 しま ／ ＼ 》 と 云 ふ 町 の 宿屋 へ 着い た の は 午 過ぎ ――...\n",
       "4        或 る 田舎 に 母 と 子 と が 住ん で い た 。 そして 或 る 年 の 秋 次...\n",
       "                              ...                        \n",
       "3307     八 五 郎 の 取柄 は 誰 と で も すぐ 友達 に なれる こと でし た 。 長 ...\n",
       "3308     ある 婦人 が 私 に 言 つ た 。 私 が 情痴 作家 など ゝ 言 は れる こと ...\n",
       "3309     諸君 は 東京 市 某 町 某 番地 なる 風 博士 の 邸宅 を 御存じ で あろ う ...\n",
       "3310     「 御免 」 少し 職業 的 に 落着き 拂 つた 聲錢 形 平次 は それ を 聞く と...\n",
       "3311     ちやう ど 今日 （ 十月 三 日 ） 文部省 で 著作 家 側 を 招い て 新 カナヅ...\n",
       "Name: body, Length: 3312, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)  # 80%のデータを学習データに、20%を検証データにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[606   2]\n",
      " [  7  48]]\n",
      "accuracy =  0.9864253393665159\n",
      "precision =  0.96\n",
      "recall =  0.8727272727272727\n",
      "f1 score =  0.9142857142857144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()  # ロジスティック回帰モデルのインスタンスを作成\n",
    "lr.fit(train_X, train_y)  # ロジスティック回帰モデルの重みを学習\n",
    "pred = lr.predict(test_X)\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=test_y, y_pred=pred))\n",
    "print('accuracy = ', accuracy_score(y_true=test_y, y_pred=pred))\n",
    "print('precision = ', precision_score(y_true=test_y, y_pred=pred))\n",
    "print('recall = ', recall_score(y_true=test_y, y_pred=pred))\n",
    "print('f1 score = ', f1_score(y_true=test_y, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[605   3]\n",
      " [  3  52]]\n",
      "accuracy =  0.9909502262443439\n",
      "precision =  0.9454545454545454\n",
      "recall =  0.9454545454545454\n",
      "f1 score =  0.9454545454545454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    **{\"hidden_layer_sizes\": (128, 128, 128, 128, 128, 128), \"random_state\": 42})\n",
    "mlp.fit(train_X, train_y)\n",
    "pred = mlp.predict(test_X)\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=test_y, y_pred=pred))\n",
    "print('accuracy = ', accuracy_score(y_true=test_y, y_pred=pred))\n",
    "print('precision = ', precision_score(y_true=test_y, y_pred=pred))\n",
    "print('recall = ', recall_score(y_true=test_y, y_pred=pred))\n",
    "print('f1 score = ', f1_score(y_true=test_y, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    **{\"hidden_layer_sizes\": (128, 128, 128, 128, 128, 128), \"random_state\": 42})\n",
    "mlp.fit(X, y)\n",
    "pred = mlp.predict(test)\n",
    "# pred = model.predict(np.array(test))\n",
    "pred = np.where(pred > 0.5, 1, 0)\n",
    "sub = pd.DataFrame(pd.read_csv(\"data/test.csv\")['writing_id'])\n",
    "sub[\"author\"] = list(pred)\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached https://files.pythonhosted.org/packages/2c/72/6b3264aa2889b7dde7663464b99587d95cd6a5f3b9b30181f14d78a63e64/tensorflow-2.0.0-cp37-cp37m-macosx_10_11_x86_64.whl\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
      "  Using cached https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (1.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (1.25.0)\n",
      "Collecting tensorboard<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 1.2MB/s eta 0:00:01     |████████▋                       | 1.0MB 1.5MB/s eta 0:00:02     |█████████████▏                  | 1.6MB 1.5MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (0.1.8)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (1.17.3)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz\n",
      "Collecting wheel>=0.26\n",
      "  Using cached https://files.pythonhosted.org/packages/00/83/b4a77d044e78ad1a45610eb88f745be2fd2c6d658f9798a15e384b7d57c9/wheel-0.33.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: h5py in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow) (41.2.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.2.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.25.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/ogata2/.pyenv/versions/3.7.5/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.0)\n",
      "Installing collected packages: termcolor, tensorflow-estimator, wheel, tensorboard, gast, opt-einsum, tensorflow\n",
      "    Running setup.py install for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Found existing installation: gast 0.3.2\n",
      "    Uninstalling gast-0.3.2:\n",
      "      Successfully uninstalled gast-0.3.2\n",
      "    Running setup.py install for gast ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for opt-einsum ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed gast-0.2.2 opt-einsum-3.1.0 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1 termcolor-1.1.0 wheel-0.33.6\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[, 夕方, 降り出し, た, 雨, は, その, 晩, 遅く, まで, 続い, た, 。,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[, この, 「, 東北, 文学, 」, という, 雑誌, の, 貴重, な, 紙面, の,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[, 幼少, の, ころ, 高知, 《, こうち, 》, の, 城下, から, 東, に, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[, ○, 「, 三, 人, 姉妹, 」, で, マーシャ, が, どんな, 風, に, 活...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[, 眼鏡, 或, 日, 趣味, に関して, 人, に, 問, はれ, た, 。, 稍, 暫...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>[, 伝統, 的, な, 女形, と, 云う, もの, の, 型, に, 嵌っ, て, 終始...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>[, 「, あ, ツヒ, 人殺し, ツ, 」, 宵闇, を, 劈, 《, つん, ざ, 》,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>[, 本誌, （, 「, 劇作, 」, ）, 三月, 号, に, 発表, さ, れ, た, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>[, 昔, 《, むかし, 》, は, いま, より, も, もっと, 松, 《, まつ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>[, 「, 親分, 」, 「, 何, ん, だ, 八, 。, 大層, な, 意, 氣, 込み...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1420 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body\n",
       "0     [, 夕方, 降り出し, た, 雨, は, その, 晩, 遅く, まで, 続い, た, 。,...\n",
       "1     [, この, 「, 東北, 文学, 」, という, 雑誌, の, 貴重, な, 紙面, の,...\n",
       "2     [, 幼少, の, ころ, 高知, 《, こうち, 》, の, 城下, から, 東, に, ...\n",
       "3     [, ○, 「, 三, 人, 姉妹, 」, で, マーシャ, が, どんな, 風, に, 活...\n",
       "4     [, 眼鏡, 或, 日, 趣味, に関して, 人, に, 問, はれ, た, 。, 稍, 暫...\n",
       "...                                                 ...\n",
       "1415  [, 伝統, 的, な, 女形, と, 云う, もの, の, 型, に, 嵌っ, て, 終始...\n",
       "1416  [, 「, あ, ツヒ, 人殺し, ツ, 」, 宵闇, を, 劈, 《, つん, ざ, 》,...\n",
       "1417  [, 本誌, （, 「, 劇作, 」, ）, 三月, 号, に, 発表, さ, れ, た, ...\n",
       "1418  [, 昔, 《, むかし, 》, は, いま, より, も, もっと, 松, 《, まつ, ...\n",
       "1419  [, 「, 親分, 」, 「, 何, ん, だ, 八, 。, 大層, な, 意, 氣, 込み...\n",
       "\n",
       "[1420 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(pd.concat([X.body,test.body]))\n",
    "x_train = tokenizer.texts_to_sequences(X.body)\n",
    "x_test = tokenizer.texts_to_sequences(test.body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>先ごろ の 本欄 に 僕 の 「 風 報 」 に かい た 「 天皇陛下 に 捧ぐる 言葉...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>旅 の 眼 に 映じ た 外国 の 正月 を といふ お 需 め で 昔 前 の 記憶 か...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>或 る 心持 の よい 夕方 日比谷公園 の 樹 の 繁み の 間 で 若葉 楓 の 梢 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>島々 《 しま ／ ＼ 》 と 云 ふ 町 の 宿屋 へ 着い た の は 午 過ぎ ――...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>或 る 田舎 に 母 と 子 と が 住ん で い た 。 そして 或 る 年 の 秋 次...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>八 五 郎 の 取柄 は 誰 と で も すぐ 友達 に なれる こと でし た 。 長 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>ある 婦人 が 私 に 言 つ た 。 私 が 情痴 作家 など ゝ 言 は れる こと ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>諸君 は 東京 市 某 町 某 番地 なる 風 博士 の 邸宅 を 御存じ で あろ う ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>「 御免 」 少し 職業 的 に 落着き 拂 つた 聲錢 形 平次 は それ を 聞く と...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>ちやう ど 今日 （ 十月 三 日 ） 文部省 で 著作 家 側 を 招い て 新 カナヅ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3312 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body\n",
       "0      先ごろ の 本欄 に 僕 の 「 風 報 」 に かい た 「 天皇陛下 に 捧ぐる 言葉...\n",
       "1      旅 の 眼 に 映じ た 外国 の 正月 を といふ お 需 め で 昔 前 の 記憶 か...\n",
       "2      或 る 心持 の よい 夕方 日比谷公園 の 樹 の 繁み の 間 で 若葉 楓 の 梢 ...\n",
       "3      島々 《 しま ／ ＼ 》 と 云 ふ 町 の 宿屋 へ 着い た の は 午 過ぎ ――...\n",
       "4      或 る 田舎 に 母 と 子 と が 住ん で い た 。 そして 或 る 年 の 秋 次...\n",
       "...                                                 ...\n",
       "3307   八 五 郎 の 取柄 は 誰 と で も すぐ 友達 に なれる こと でし た 。 長 ...\n",
       "3308   ある 婦人 が 私 に 言 つ た 。 私 が 情痴 作家 など ゝ 言 は れる こと ...\n",
       "3309   諸君 は 東京 市 某 町 某 番地 なる 風 博士 の 邸宅 を 御存じ で あろ う ...\n",
       "3310   「 御免 」 少し 職業 的 に 落着き 拂 つた 聲錢 形 平次 は それ を 聞く と...\n",
       "3311   ちやう ど 今日 （ 十月 三 日 ） 文部省 で 著作 家 側 を 招い て 新 カナヅ...\n",
       "\n",
       "[3312 rows x 1 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>先ごろ の 本欄 に 僕 の 「 風 報 」 に かい た 「 天皇陛下 に 捧ぐる 言葉...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>旅 の 眼 に 映じ た 外国 の 正月 を といふ お 需 め で 昔 前 の 記憶 か...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>或 る 心持 の よい 夕方 日比谷公園 の 樹 の 繁み の 間 で 若葉 楓 の 梢 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body\n",
       "0   先ごろ の 本欄 に 僕 の 「 風 報 」 に かい た 「 天皇陛下 に 捧ぐる 言葉...\n",
       "1   旅 の 眼 に 映じ た 外国 の 正月 を といふ お 需 め で 昔 前 の 記憶 か...\n",
       "2   或 る 心持 の よい 夕方 日比谷公園 の 樹 の 繁み の 間 で 若葉 楓 の 梢 ..."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 先ごろ の 本欄 に 僕 の 「 風 報 」 に かい た 「 天皇陛下 に 捧ぐる 言葉 」 を 評し て 俗 うけ を 狙っ た 媚態 露出 だ と の こと で ある が 白井 明 先生 の 鑑賞 眼 は 浅薄 低俗 と 申さ なけれ ば なら ない 。 あの 文章 に こもる 祖国 へ よせる 僕 の 愛情 や あれ を 書か ず に い られ なかっ た 情熱 を 読みとる こと が でき ない と は 白井 先生 が 頃日 書く 意味 も ない 駄文 ばかり 書い てる せい な の で ある 。 いったい に 文学 の 反語 性 に 味読 の 及ば ぬ 識見 低俗 な ヤカラ が 文学 を 批評 する という の が 間違っ て いる 。 僕 の 「 堕落 論 」 その他 の エッセイ に し て も 小説 に し て も その 反語 に こもる 正しい 意味 を 理解 し 得 ず に 軽率 な 判読 断定 を 下す から 読者 に 誤読 の お手本 を 与え て いる よう な もの で ある 。 本名 で は 愚かしい ソラゴト しか 書け ず 匿名 で しか 本音 の 吐け ぬ 文学 者 など という もの は ない 。 僕 に は 匿名 の 必要 は ない 。 いつ でも 本音 を 吐き ギリギリ の こと を 言っ てる から だ 。 だから また ぼく の 本音 は 文学 の 本質 的 な もの で あり 単なる 中傷 の ケチ くさい 汚らし さ は ない の で ある 。 白井 明 先生 も 本名 で 本音 を 吐く こと を 学び た ま え 。 本名 で 君 の ケチアサマシサ を さらけだす こと の 苦痛 に 堪え て その 争い の 嵐 の 中 で 自分 を 育て た ま え 。 さ すれ ば 文学 の 本質 に も やがて 近づき うる で あろ う 。 「 天皇陛下 に さ ゝ ぐる 言葉 」 に こもる 大いなる 愛情 も やみ がたい 情熱 も 君 の 目 に は 逆 の 意味 に うつる の も きわめて 当然 な こと で ある 。 しかし こういう 愚 に も つか ない 批評 で も それ が 君 の 本音 なら 仕方 が ない から せめて 本名 で 書か れん こと を 。 さ すれ ば 進歩 は あり うる で あろ う 。 \n",
      "[39421, 1, 71223, 3, 99, 1, 15, 210, 6869, 16, 3, 327, 5, 15, 33069, 3, 60179, 181, 16, 7, 12213, 6, 4656, 1241, 7, 8388, 5, 13447, 8646, 19, 10, 1, 23, 9, 22, 8, 27056, 1434, 258, 1, 5216, 135, 4, 9319, 11962, 10, 6102, 148, 49, 65, 18, 2, 104, 1003, 3, 17100, 6552, 26, 15300, 99, 1, 1625, 44, 254, 7, 918, 69, 3, 25, 87, 92, 5, 1732, 7, 29393, 23, 8, 270, 18, 10, 4, 27056, 258, 8, 40866, 760, 288, 13, 18, 45973, 122, 282, 164, 601, 17, 1, 9, 22, 2, 2819, 3, 171, 1, 30728, 331, 3, 71224, 1, 3000, 102, 18773, 11962, 17, 48078, 8, 171, 7, 874, 43, 48, 1, 8, 4985, 6, 30, 2, 99, 1, 15, 5289, 1310, 16, 1045, 1, 38138, 3, 14, 6, 13, 281, 3, 14, 6, 13, 27, 30728, 3, 17100, 2025, 288, 7, 711, 14, 318, 69, 3, 10280, 17, 26539, 6891, 7, 4743, 20, 1028, 3, 44048, 1, 15301, 7, 820, 6, 30, 38, 17, 31, 9, 22, 2, 9728, 9, 4, 25077, 80107, 379, 2332, 69, 25078, 9, 379, 13233, 1, 35848, 102, 171, 94, 103, 48, 31, 4, 18, 2, 99, 3, 4, 25078, 1, 363, 4, 18, 2, 531, 95, 13233, 7, 6135, 13758, 1, 23, 7, 308, 164, 20, 19, 2, 549, 112, 1117, 1, 13233, 4, 171, 1, 1079, 53, 17, 31, 9, 52, 3173, 28145, 1, 5384, 4737, 40867, 29, 4, 18, 1, 9, 22, 2, 27056, 1434, 258, 13, 9728, 9, 13233, 7, 7985, 23, 7, 9320, 5, 198, 160, 2, 9728, 9, 163, 1, 95670, 7, 42396, 23, 1, 1884, 3, 2498, 6, 27, 7149, 1, 4177, 1, 61, 9, 58, 7, 3154, 5, 198, 160, 2, 29, 377, 49, 171, 1, 1079, 3, 13, 470, 5893, 1288, 9, 231, 24, 2, 15, 33069, 3, 29, 109, 5813, 181, 16, 3, 17100, 8930, 1625, 13, 4623, 2754, 1732, 13, 163, 1, 167, 3, 4, 1605, 1, 288, 3, 8544, 1, 13, 3201, 917, 17, 23, 9, 22, 2, 173, 525, 4938, 3, 13, 519, 18, 874, 9, 13, 36, 8, 163, 1, 13233, 65, 910, 8, 18, 20, 1861, 9728, 9, 918, 4771, 23, 7, 2, 29, 377, 49, 1985, 4, 52, 1288, 9, 231, 24, 2]\n",
      " 旅 の 眼 に 映じ た 外国 の 正月 を といふ お 需 め で 昔 前 の 記憶 から 探し て み た が 其処 に は ほとんど 「 お正月 」 と いふ もの が ない 。 我々 の 頭 に 幼少 の 頃 から 浸 み 込ん で ゐる お正月 新年 といふ もの と は およそ かけ離れ た もの で あつ た 。 古い 年 が 逝き 新しい 年 が 来る と いふ 事 を 我々 の 祖先 が 何故 こんなに 重 大事 と し 華やか な 儀式 を 以 つて 迎 へる 様 に なつ た か その 穿鑿 は 別 として 欧米 人 は 実に あつ さ り と これ を 扱 つて ゐる 。 私 は 丁度 四 回 の 新年 を 巴里 で 迎 へ たわけ で ある が 仏蘭西 人 の 下宿 に 住み 故国 から の 留学生 とか 大使館 関係 の 人達 と の 交際 など も 少 なかつ た ので 猶 更 その 正月 は ひつ そり し た もの で あつ た 。 最初 の 年 は それでも 「 あ 今時分 は 弟妹 達 雑煮 で も 祝 つて ゐる か な 」 とか 母 の 得意 の 煎 田作 で 飯 を 食べ て みたい とか 思 つ たり し た もの で ある が 次 の 年 から は そんな 感傷 も 薄らぎ 結句 煩雑 な 儀礼 に 縛ら れ ない で 済む 身軽 さ の 気持 に のびのび と 己 れ を 浸し て ゐ た 。 それでも 大晦日 の 晩 は レヴエイヨン と い つ て みんな 大概 レストラン か 何 か に 出かけ 知人 等 と 食事 を 供 に し 踊 つ たり 唄 つ たり で 夜 を 更かす つまり それ が 外国 で は 新年 を 迎 へる 気持 の 唯 の 現 はれ と 云 へ よう 。 その 騒ぎ も 夜 が 明ける 頃 に は 何処 も す つかり 静ま つて 街 上 に も 屋内 に も 平常 と 何 の 変り も ない 日 が 来る 。 起き て 食堂 に でも 出 て 来る と 流石 下宿 の 女 主人 が 「 お早う 」 の 代り に 「 お 目出度う 」 と 云 つ て くれる 。 しかし それ も ほんの 軽い 挨拶 で 別に その 言葉 から 正月 を 感じ させ て くれる やう な もの で は ない 。 カトリック教 の 国 に 「 王様 の 日 」 と いふ の が ある 。 これ は 偶然 日本 の 「 松の内 」 に ある お祭り 日 で あ つて 向 ふ の 人達 に は 新年 と は 関 り の ない もの で ある が 日本人 で ある 私 など に は 時 が 時 な ので ちよ つと その 日 は お正月 らしい 気分 を 味 は へる もの だ つ た 。 それ は 聖書 に ある 通り 基督 が 生れ た 時 東方 の 国 の 博士 達 が 星 の 占 ひで ベツレヘム に 偉い 人 が 生れ た と 云 つたの により 東方 の 国 の 王 が その 誕生 を 祝ひ に 来 た といふ その 日 を 祝 ふ の で ある 。 この 日 各 家庭 で は 独身 だ つ たり 遠く から 学校 の 寄宿舎 に 来 て ゐる 人 など 家 を 持た ない 人達 を 招き 煖炉 を 前 に し て カルタ や 唄 や 隠し芸 の 披露 や 極 く 呑気 に 家庭 的 な 娯楽 に うち 興じる 。 そして この 日 に は 食後 に 必ず 特別 の 菓子 が 出る 。 丁度 誕生 日 や クリスマス の 時 の 様 な 大きい カステラ 風 の 菓子 だ が 大抵 は その 家 の 主婦 の 手製 といふ 事 に なつ て ゐる 。 これ を 主婦 が 人数 だけ に 分け て 各自 に 配る の で ある が この 中 に は たつ たつ 王様 の 人形 が 入れ て あり それ に ぶつ か つた 人 は 男 なら 王様 に なり 相手 の 女王 を 選ぶ し 女 なら 王様 を 選ぶ 権利 が ある 。 女王 なり 王様 なり が 決まる と みんな は 二 人 を ならべ て 口々 に 「 王様 お 目出度う 女 王様 お 目 出 た う 」 と 祝詞 を 述べ て 囃す の で ある 。 この 人形 を なるたけ その 日 の 人気 者 例へば その 中 に 恋人 同志 が ゐる と すれ ば その 方 に といふ 工合 に うまく 当る 様 に する の も 主婦 の 腕前 な の ださ う で ある 。 しかし 当 つた 当人 は てれくさい ので わざと みんな の 期待 通り に は 選ば なかつ たり する 。 かう し て 老若男女 無邪気 に 日 を 楽しむ この 日 だけ が ちよ つと 日本 の 正月 の カルタ 会 の 空気 など を 思は せ られる 唯 の もの だ つ た 。 \n",
      "[1360, 1, 135, 3, 12571, 5, 1112, 1, 2828, 7, 106, 39, 33070, 205, 9, 422, 117, 1, 1087, 20, 1817, 6, 125, 5, 8, 1333, 3, 4, 1251, 15, 7317, 16, 10, 165, 31, 8, 18, 2, 868, 1, 213, 3, 12775, 1, 259, 20, 9231, 125, 855, 9, 66, 7317, 8315, 106, 31, 10, 4, 5011, 20238, 5, 31, 9, 150, 5, 2, 1065, 113, 8, 60180, 511, 113, 8, 192, 10, 165, 89, 7, 868, 1, 4657, 8, 1145, 1191, 991, 892, 10, 14, 6948, 17, 9457, 7, 4782, 41, 2094, 632, 264, 3, 252, 5, 21, 27, 15891, 4, 576, 81, 8579, 37, 4, 517, 150, 29, 137, 10, 70, 7, 4805, 41, 66, 2, 34, 4, 836, 151, 1368, 1, 8315, 7, 5894, 9, 2094, 26, 17699, 9, 22, 8, 3471, 37, 1, 2454, 3, 4823, 10913, 20, 1, 12874, 257, 10519, 452, 1, 1672, 10, 1, 4627, 103, 13, 5634, 170, 5, 118, 3568, 2719, 27, 2828, 4, 1676, 2288, 14, 5, 31, 9, 150, 5, 2, 1077, 1, 113, 4, 623, 15, 141, 16886, 4, 13896, 284, 18774, 9, 13, 7014, 41, 66, 21, 17, 16, 257, 226, 1, 2687, 1, 19960, 71225, 9, 2605, 7, 1383, 6, 695, 257, 228, 40, 126, 14, 5, 31, 9, 22, 8, 374, 1, 113, 20, 4, 133, 3011, 13, 40868, 56385, 26540, 17, 13234, 3, 3191, 35, 18, 9, 6334, 13235, 29, 1, 310, 3, 11135, 10, 1768, 35, 7, 16887, 6, 79, 5, 2, 623, 11288, 1, 433, 4, 95671, 10, 25, 40, 6, 343, 3608, 14878, 21, 51, 21, 3, 899, 4658, 362, 10, 1535, 7, 4924, 3, 14, 4273, 40, 126, 3135, 40, 126, 9, 247, 7, 60181, 551, 36, 8, 1112, 9, 4, 8315, 7, 2094, 632, 310, 1, 939, 1, 1470, 579, 10, 128, 26, 38, 2, 27, 1992, 13, 247, 8, 9413, 259, 3, 4, 717, 13, 217, 1179, 60182, 41, 1054, 90, 3, 13, 17288, 3, 13, 4274, 10, 51, 1, 2062, 13, 18, 72, 8, 192, 2, 1044, 6, 1970, 3, 95, 107, 6, 192, 10, 5081, 2454, 1, 93, 316, 8, 15, 11136, 16, 1, 806, 3, 15, 39, 36927, 16, 10, 128, 40, 6, 654, 2, 173, 36, 13, 1826, 2879, 1224, 9, 1422, 27, 181, 20, 2828, 7, 174, 934, 6, 654, 78, 17, 31, 9, 4, 18, 2, 53212, 1, 472, 3, 15, 5512, 1, 72, 16, 10, 165, 1, 8, 22, 2, 70, 4, 1438, 140, 1, 15, 40869, 16, 3, 22, 38139, 72, 9, 141, 41, 494, 138, 1, 1672, 3, 4, 8315, 10, 4, 2112, 137, 1, 18, 31, 9, 22, 8, 714, 9, 22, 34, 103, 3, 4, 74, 8, 74, 17, 118, 1175, 505, 27, 72, 4, 7317, 186, 901, 7, 882, 4, 632, 31, 19, 40, 5, 2, 36, 4, 9566, 3, 22, 294, 14476, 8, 616, 5, 74, 17700, 1, 472, 1, 1165, 284, 8, 1547, 1, 8389, 1852, 65032, 3, 3922, 37, 8, 616, 5, 10, 128, 387, 3357, 17700, 1, 472, 1, 1626, 8, 27, 4263, 7, 18335, 3, 73, 5, 106, 27, 72, 7, 7014, 138, 1, 9, 22, 2, 45, 72, 1793, 835, 9, 4, 6451, 19, 40, 126, 970, 20, 485, 1, 8685, 3, 73, 6, 66, 37, 103, 84, 7, 1591, 18, 1672, 7, 11963, 10163, 7, 117, 3, 14, 6, 17701, 44, 3135, 44, 36928, 1, 7812, 44, 2810, 206, 5082, 3, 835, 53, 17, 4900, 3, 134, 42397, 2, 101, 45, 72, 3, 4, 12489, 3, 1129, 1041, 1, 3510, 8, 552, 2, 836, 4263, 72, 44, 11964, 1, 74, 1, 264, 17, 605, 23459, 210, 1, 3510, 19, 8, 2003, 4, 27, 84, 1, 3297, 1, 17289, 106, 89, 3, 252, 6, 66, 2, 70, 7, 3297, 8, 7520, 80, 3, 3761, 6, 5581, 3, 23839, 1, 9, 22, 8, 45, 61, 3, 4, 539, 539, 5512, 1, 1208, 8, 419, 6, 52, 36, 3, 2723, 21, 147, 37, 4, 120, 65, 5512, 3, 108, 424, 1, 7263, 7, 5635, 14, 93, 65, 5512, 7, 5635, 3342, 8, 22, 2, 7263, 108, 5512, 108, 8, 48079, 10, 343, 4, 50, 37, 7, 7521, 6, 8060, 3, 15, 5512, 39, 36927, 93, 5512, 39, 167, 107, 5, 24, 16, 10, 48080, 7, 1956, 6, 80108, 1, 9, 22, 2, 45, 1208, 7, 9370, 27, 72, 1, 3034, 94, 3155, 27, 61, 3, 2736, 2415, 8, 66, 10, 377, 49, 27, 82, 3, 106, 1248, 3, 1848, 2997, 264, 3, 43, 1, 13, 3297, 1, 9961, 17, 1, 2298, 24, 9, 22, 2, 173, 1639, 147, 3881, 4, 25533, 118, 2242, 343, 1, 1771, 294, 3, 4, 6153, 170, 126, 43, 2, 465, 14, 6, 18336, 3680, 3, 72, 7, 8965, 45, 72, 80, 8, 1175, 505, 140, 1, 2828, 1, 17701, 436, 1, 992, 103, 7, 555, 76, 268, 939, 1, 31, 19, 40, 5, 2]\n",
      " 或 る 心持 の よい 夕方 日比谷公園 の 樹 の 繁み の 間 で 若葉 楓 の 梢 を 眺め て い たら どこ から とも なく ラジオ の 声 が 流れ て 来 た 。 職業 紹介 で あっ た 。 ずっと 歩い て 行っ て 見 たら 空地 に 向っ た 高い ところ に 満州 国 から の 貴賓 を 迎える ため 赤 や 緑 で 装飾 さ れ た 拡声 機 が 据えつけ て あっ て そこ から 「 年齢 十 六 歳 前後 住む 込み で 月給 七 円 住み こみ で 月給 七 円 」 と 夕空 に 響い て いる の で あっ た 。 私 は パパ ママ は いけ ない という 松田 文相 の 小学 放送 の 試み や ラジオ に でる に は なかなか お金 が かかる ん で ねえ と 打ち かこっ た 或 る 長唄 の 師匠 の 言葉 など を 思い出し ながら その 声 を きい て いる の で あっ た 。 聴取 料 が 五 十 銭 に なっ た こと は ラジオ に対する 大衆 の 親しみ を 増し 何より の こと と 思う 。 ところで この間 馬場 先 を 通っ て い たら かね て 新聞 で 披露 さ れ て い た 犯人 逮捕 用 ラジオ 自動車 が 消防 自動車 の よう な 勢 で むこ う から 疾走 し て 来 た 。 通行人 も 珍し げに それ を よ け て 見送っ て い た 。 ふと 私 は 民間 自動車 の ラジオ は 許さ れ て い ず その 設備 の ある 新 車体 は セット を はずし て 車体 検査 を 受け ね ば なら ぬ という 事実 を 想い 起し 改めて 悠々 と 走り去る ラジオ 自動車 を 眺め た 。 \n",
      "[319, 130, 365, 1, 337, 1552, 23840, 1, 1995, 1, 17489, 1, 145, 9, 6839, 7954, 1, 3690, 7, 393, 6, 25, 114, 218, 20, 300, 68, 2600, 1, 179, 8, 789, 6, 73, 5, 2, 1450, 1606, 9, 57, 5, 2, 1355, 420, 6, 195, 6, 62, 114, 5499, 3, 812, 5, 787, 85, 3, 8061, 472, 20, 1, 60183, 7, 9729, 121, 756, 44, 2746, 9, 5999, 29, 35, 5, 22036, 2368, 8, 39422, 6, 57, 6, 168, 20, 15, 3644, 54, 177, 585, 1475, 3337, 2836, 9, 4916, 209, 448, 4823, 4480, 9, 4916, 209, 448, 16, 10, 22379, 3, 3029, 6, 30, 1, 9, 57, 5, 2, 34, 4, 7374, 7687, 4, 366, 18, 48, 22717, 21088, 1, 6494, 4118, 1, 1957, 44, 2600, 3, 1347, 3, 4, 614, 1188, 8, 1319, 28, 9, 261, 10, 1440, 31475, 5, 319, 130, 21089, 1, 2427, 1, 181, 103, 7, 1467, 91, 27, 179, 7, 581, 6, 30, 1, 9, 57, 5, 2, 16888, 3484, 8, 88, 54, 1082, 3, 63, 5, 23, 4, 2600, 524, 1196, 1, 4375, 7, 4071, 3063, 1, 23, 10, 207, 2, 938, 1596, 7264, 287, 7, 1727, 6, 25, 114, 956, 6, 514, 9, 7812, 29, 35, 6, 25, 5, 1809, 14620, 839, 2600, 1349, 8, 10843, 1349, 1, 38, 17, 1416, 9, 3798, 24, 20, 15302, 14, 6, 73, 5, 2, 9458, 13, 10520, 8098, 36, 7, 47, 301, 6, 6748, 6, 25, 5, 2, 541, 34, 4, 9513, 1349, 1, 2600, 4, 1887, 35, 6, 25, 69, 27, 6335, 1, 22, 443, 26541, 4, 20239, 7, 8615, 6, 26541, 8099, 7, 668, 55, 49, 65, 102, 48, 500, 7, 8192, 1304, 2659, 4036, 10, 30729, 2600, 1349, 7, 393, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "for text, vector in zip(X.body.head(3), x_train[0:3]):\n",
    "    print(text)\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=2000\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 32)          4719520   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 4,722,673\n",
      "Trainable params: 4,722,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "vocabulary_size = len(tokenizer.word_index) + 1  # 学習データの語彙数+1\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=32))\n",
    "model.add(LSTM(16, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import warnings\n",
    "# display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "warnings.filterwarnings('ignore')\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(x_train, y, test_size = 0.2, random_state = 0) # 80%のデータを学習データに、20%を検証データにする\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2649 samples, validate on 663 samples\n",
      "Epoch 1/10\n",
      " 800/2649 [========>.....................] - ETA: 1:34 - loss: 0.6150 - accuracy: 0.9100"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_X, train_y, batch_size=32, epochs=10,\n",
    "    validation_data=(valid_X, valid_y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "pred = model.predict_classes(valid_X)\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=test_y, y_pred=pred))\n",
    "print('accuracy = ', accuracy_score(y_true=test_y, y_pred=pred))\n",
    "print('precision = ', precision_score(y_true=test_y, y_pred=pred))\n",
    "print('recall = ', recall_score(y_true=test_y, y_pred=pred))\n",
    "print('f1 score = ', f1_score(y_true=test_y, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
